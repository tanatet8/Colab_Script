{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/ThaiNovel_OCR_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "                OCR PROCESSING WITH API - AUTOMATED VERSION\n",
        "                       สำหรับทำ Corpus นิยายไทย\n",
        "================================================================================\n",
        "\n",
        "Features:\n",
        "1. Automated OCR → LLM → Clean corpus\n",
        "2. ใช้ GPT-4o-mini (ถูกสุด) หรือ Claude Haiku\n",
        "3. Quality validation & tracking\n",
        "4. Save training pairs for fine-tuning\n",
        "\n",
        "Requirements:\n",
        "- pip install openai anthropic pandas tqdm\n",
        "- API keys (OpenAI หรือ Anthropic)\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# 📌 Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Drive (สำหรับ Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"✅ Libraries loaded\")"
      ],
      "metadata": {
        "id": "hT-ghRST21PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 2: Configuration & API Setup\n",
        "# ============================================\n",
        "class Config:\n",
        "    \"\"\"Configuration สำหรับ API Processing\"\"\"\n",
        "\n",
        "    # ⚠️ ใส่ API Keys ที่นี่ (หรือใช้ environment variables)\n",
        "    OPENAI_API_KEY = \"\"  # ใส่ OpenAI API key\n",
        "    ANTHROPIC_API_KEY = \"\"  # ใส่ Anthropic API key (ถ้าใช้ Claude)\n",
        "\n",
        "    # เลือก Model (uncomment อันที่จะใช้)\n",
        "    MODEL = \"gpt-4o-mini\"  # ถูกสุด แนะนำ!\n",
        "    # MODEL = \"gpt-3.5-turbo\"\n",
        "    # MODEL = \"claude-3-haiku\"\n",
        "\n",
        "    # Paths (Google Drive)\n",
        "    BASE = '/content/drive/MyDrive/OCR' if IN_COLAB else './OCR'\n",
        "\n",
        "    RAW_OCR_DIR = f'{BASE}/raw_ocr'\n",
        "    CLEANED_DIR = f'{BASE}/cleaned'\n",
        "    CORPUS_DIR = f'{BASE}/final_corpus'\n",
        "    TRAINING_PAIRS_DIR = f'{BASE}/training_pairs'\n",
        "    LOGS_DIR = f'{BASE}/logs'\n",
        "\n",
        "    # Processing settings\n",
        "    MAX_PAGES_PER_BATCH = 5  # จำนวนหน้าต่อ API call\n",
        "    MAX_RETRIES = 3  # retry ถ้า API error\n",
        "    TEMPERATURE = 0.1  # ต่ำ = consistent output\n",
        "    MAX_TOKENS = 4000  # max response length\n",
        "\n",
        "    # Cost tracking\n",
        "    PRICE_PER_1K_TOKENS = {\n",
        "        'gpt-4o-mini': 0.00015,  # $0.15 per 1M\n",
        "        'gpt-3.5-turbo': 0.0005,\n",
        "        'claude-3-haiku': 0.00025\n",
        "    }\n",
        "\n",
        "# สร้าง folders\n",
        "for folder in [Config.RAW_OCR_DIR, Config.CLEANED_DIR, Config.CORPUS_DIR,\n",
        "               Config.TRAINING_PAIRS_DIR, Config.LOGS_DIR]:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"✅ Config loaded\")"
      ],
      "metadata": {
        "id": "Y8rT6G4U26EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 3: API Clients\n",
        "# ============================================\n",
        "class LLMClient:\n",
        "    \"\"\"Universal LLM Client สำหรับ OpenAI และ Anthropic\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = Config.MODEL\n",
        "        self.client = None\n",
        "        self.total_tokens = 0\n",
        "        self.total_cost = 0\n",
        "\n",
        "        # Initialize ตาม model\n",
        "        if 'gpt' in self.model:\n",
        "            self._init_openai()\n",
        "        elif 'claude' in self.model:\n",
        "            self._init_anthropic()\n",
        "\n",
        "    def _init_openai(self):\n",
        "        \"\"\"Initialize OpenAI client\"\"\"\n",
        "        try:\n",
        "            import openai\n",
        "\n",
        "            # Set API key\n",
        "            if Config.OPENAI_API_KEY:\n",
        "                openai.api_key = Config.OPENAI_API_KEY\n",
        "            else:\n",
        "                # ลองหาจาก environment variable\n",
        "                openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "            if not openai.api_key:\n",
        "                raise ValueError(\"❌ ไม่พบ OpenAI API key! กรุณาใส่ใน Config\")\n",
        "\n",
        "            self.client = openai.OpenAI(api_key=openai.api_key)\n",
        "            print(f\"✅ OpenAI client ready (Model: {self.model})\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"❌ ต้องติดตั้ง: pip install openai\")\n",
        "            raise\n",
        "\n",
        "    def _init_anthropic(self):\n",
        "        \"\"\"Initialize Anthropic client\"\"\"\n",
        "        try:\n",
        "            import anthropic\n",
        "\n",
        "            if Config.ANTHROPIC_API_KEY:\n",
        "                api_key = Config.ANTHROPIC_API_KEY\n",
        "            else:\n",
        "                api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "            if not api_key:\n",
        "                raise ValueError(\"❌ ไม่พบ Anthropic API key!\")\n",
        "\n",
        "            self.client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(f\"✅ Anthropic client ready (Model: {self.model})\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"❌ ต้องติดตั้ง: pip install anthropic\")\n",
        "            raise\n",
        "\n",
        "    def clean_ocr_text(self, text: str, page_num: int = 1) -> Dict:\n",
        "        \"\"\"\n",
        "        ส่ง OCR text ให้ LLM แก้ไข\n",
        "\n",
        "        Returns:\n",
        "            {\n",
        "                'cleaned_text': str,\n",
        "                'tokens_used': int,\n",
        "                'cost': float,\n",
        "                'changes': list\n",
        "            }\n",
        "        \"\"\"\n",
        "        # สร้าง prompt\n",
        "        prompt = self._create_prompt(text)\n",
        "\n",
        "        # เรียก API ตาม provider\n",
        "        if 'gpt' in self.model:\n",
        "            result = self._call_openai(prompt)\n",
        "        else:\n",
        "            result = self._call_anthropic(prompt)\n",
        "\n",
        "        # Track usage\n",
        "        self.total_tokens += result['tokens_used']\n",
        "        self.total_cost += result['cost']\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _create_prompt(self, text: str) -> str:\n",
        "        \"\"\"สร้าง prompt สำหรับ OCR cleaning\"\"\"\n",
        "        return f\"\"\"แก้ไขข้อความ OCR จากนิยายภาษาไทยต่อไปนี้\n",
        "\n",
        "กฎการแก้ไข:\n",
        "1. แก้เฉพาะ typo และการสะกดผิด\n",
        "2. แก้คำที่ขาดหาย/แตกหัก (เช่น \"มา กำลัง\" → \"มากำลัง\")\n",
        "3. ลบตัวอักษรเดี่ยวที่ไม่มีความหมาย\n",
        "4. รักษารูปแบบบทสนทนา (คำพูดใน \"...\")\n",
        "5. ห้ามเพิ่มเนื้อหาใหม่\n",
        "6. ห้ามเปลี่ยนความหมาย\n",
        "\n",
        "ข้อความ OCR:\n",
        "{text}\n",
        "\n",
        "ข้อความที่แก้แล้ว:\"\"\"\n",
        "\n",
        "    def _call_openai(self, prompt: str) -> Dict:\n",
        "        \"\"\"Call OpenAI API\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"คุณคือผู้เชี่ยวชาญแก้ไข OCR ภาษาไทย\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                max_tokens=Config.MAX_TOKENS\n",
        "            )\n",
        "\n",
        "            # Extract result\n",
        "            cleaned_text = response.choices[0].message.content\n",
        "            tokens = response.usage.total_tokens\n",
        "\n",
        "            # Calculate cost\n",
        "            price_per_token = Config.PRICE_PER_1K_TOKENS.get(self.model, 0.0005) / 1000\n",
        "            cost = tokens * price_per_token\n",
        "\n",
        "            return {\n",
        "                'cleaned_text': cleaned_text,\n",
        "                'tokens_used': tokens,\n",
        "                'cost': cost,\n",
        "                'model': self.model\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ OpenAI API error: {e}\")\n",
        "            # Retry logic\n",
        "            for retry in range(Config.MAX_RETRIES):\n",
        "                time.sleep(2 ** retry)  # Exponential backoff\n",
        "                try:\n",
        "                    return self._call_openai(prompt)\n",
        "                except:\n",
        "                    continue\n",
        "            raise\n",
        "\n",
        "    def _call_anthropic(self, prompt: str) -> Dict:\n",
        "        \"\"\"Call Anthropic API\"\"\"\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=Config.MAX_TOKENS,\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            cleaned_text = response.content[0].text\n",
        "            tokens = response.usage.input_tokens + response.usage.output_tokens\n",
        "\n",
        "            price_per_token = Config.PRICE_PER_1K_TOKENS.get(self.model, 0.00025) / 1000\n",
        "            cost = tokens * price_per_token\n",
        "\n",
        "            return {\n",
        "                'cleaned_text': cleaned_text,\n",
        "                'tokens_used': tokens,\n",
        "                'cost': cost,\n",
        "                'model': self.model\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Anthropic API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_usage_summary(self) -> Dict:\n",
        "        \"\"\"สรุปการใช้งาน API\"\"\"\n",
        "        return {\n",
        "            'total_tokens': self.total_tokens,\n",
        "            'total_cost_usd': self.total_cost,\n",
        "            'total_cost_thb': self.total_cost * 35,  # ประมาณ\n",
        "            'pages_processed': self.total_tokens // 500  # ประมาณ 500 tokens/page\n",
        "        }\n",
        "\n",
        "print(\"✅ LLM Client ready\")"
      ],
      "metadata": {
        "id": "sOnSRufE2_v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# 📌 Block 4: OCR Processor\n",
        "# ============================================\n",
        "class OCRProcessor:\n",
        "    \"\"\"Main processor สำหรับ OCR → LLM → Clean corpus\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.llm = LLMClient()\n",
        "        self.stats = {\n",
        "            'processed': 0,\n",
        "            'failed': 0,\n",
        "            'total_cost': 0\n",
        "        }\n",
        "        self.training_pairs = []\n",
        "\n",
        "    def process_file(self, file_path: Path) -> Dict:\n",
        "        \"\"\"\n",
        "        Process 1 ไฟล์ OCR พร้อม validation\n",
        "\n",
        "        Returns:\n",
        "            {\n",
        "                'success': bool,\n",
        "                'cleaned_path': str,\n",
        "                'stats': dict,\n",
        "                'validation': dict\n",
        "            }\n",
        "        \"\"\"\n",
        "        print(f\"\\n📄 Processing: {file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            # อ่านไฟล์ OCR\n",
        "            raw_text = file_path.read_text(encoding='utf-8')\n",
        "\n",
        "            # ถ้าไฟล์ใหญ่ ต้องแบ่ง chunks\n",
        "            if len(raw_text) > 3000:\n",
        "                chunks = self._split_text(raw_text)\n",
        "                cleaned_chunks = []\n",
        "\n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    print(f\"   Chunk {i+1}/{len(chunks)}...\")\n",
        "                    result = self.llm.clean_ocr_text(chunk, i+1)\n",
        "                    cleaned_chunks.append(result['cleaned_text'])\n",
        "                    time.sleep(1)  # Rate limiting\n",
        "\n",
        "                cleaned_text = '\\n\\n'.join(cleaned_chunks)\n",
        "            else:\n",
        "                # ไฟล์เล็ก ส่งทั้งหมด\n",
        "                result = self.llm.clean_ocr_text(raw_text)\n",
        "                cleaned_text = result['cleaned_text']\n",
        "\n",
        "            # === VALIDATION ===\n",
        "            print(f\"   🔍 Validating...\")\n",
        "            validation = QualityValidator.enhanced_validate(\n",
        "                raw_text,\n",
        "                cleaned_text,\n",
        "                file_path.name\n",
        "            )\n",
        "\n",
        "            # แสดงผล validation\n",
        "            if validation['status'] == 'FAIL':\n",
        "                print(f\"   ❌ VALIDATION FAILED:\")\n",
        "                for issue in validation['issues']:\n",
        "                    print(f\"      {issue}\")\n",
        "            elif validation['status'] == 'WARNING':\n",
        "                print(f\"   ⚠️ VALIDATION WARNING:\")\n",
        "                if validation['warnings']:\n",
        "                    print(f\"      {validation['warnings'][0]}\")\n",
        "                if validation['suspicious']:\n",
        "                    print(f\"      {validation['suspicious'][0]}\")\n",
        "            else:\n",
        "                print(f\"   ✅ Validation passed (score: {validation['score']:.2f})\")\n",
        "\n",
        "            # บันทึก validation report ถ้ามี issues\n",
        "            if validation['status'] in ['FAIL', 'WARNING']:\n",
        "                val_report_path = QualityValidator.save_validation_report(validation)\n",
        "                print(f\"   📊 Validation report: {val_report_path.name}\")\n",
        "\n",
        "            # บันทึกผลลัพธ์ (แม้ validation จะ fail ก็ save ไว้ review)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "            # ถ้า validation fail ใส่ prefix WARNING_\n",
        "            if validation['status'] == 'FAIL':\n",
        "                clean_filename = f\"WARNING_{file_path.stem}_clean_{timestamp}.txt\"\n",
        "            else:\n",
        "                clean_filename = f\"{file_path.stem}_clean_{timestamp}.txt\"\n",
        "\n",
        "            clean_path = Path(Config.CLEANED_DIR) / clean_filename\n",
        "            clean_path.write_text(cleaned_text, encoding='utf-8')\n",
        "\n",
        "            # เก็บ training pair (เฉพาะที่ผ่าน validation)\n",
        "            if validation['status'] != 'FAIL':\n",
        "                self.training_pairs.append({\n",
        "                    'input': raw_text[:1000],\n",
        "                    'output': cleaned_text[:1000],\n",
        "                    'source': file_path.name,\n",
        "                    'timestamp': timestamp,\n",
        "                    'validation_score': validation['score']\n",
        "                })\n",
        "\n",
        "            # Update stats\n",
        "            self.stats['processed'] += 1\n",
        "            if validation['status'] == 'FAIL':\n",
        "                self.stats['validation_failed'] = self.stats.get('validation_failed', 0) + 1\n",
        "            elif validation['status'] == 'WARNING':\n",
        "                self.stats['validation_warning'] = self.stats.get('validation_warning', 0) + 1\n",
        "\n",
        "            print(f\"   ✅ Saved: {clean_filename}\")\n",
        "            print(f\"   💰 Cost: ${result.get('cost', 0):.4f}\")\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'cleaned_path': str(clean_path),\n",
        "                'tokens': result.get('tokens_used', 0),\n",
        "                'cost': result.get('cost', 0),\n",
        "                'validation': validation\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error: {e}\")\n",
        "            self.stats['failed'] += 1\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def _split_text(self, text: str, max_chars: int = 2500) -> List[str]:\n",
        "        \"\"\"แบ่ง text ยาวเป็น chunks\"\"\"\n",
        "        # แบ่งตาม paragraph ถ้าเป็นไปได้\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para_length = len(para)\n",
        "\n",
        "            if current_length + para_length > max_chars and current_chunk:\n",
        "                # Chunk เต็ม - save และเริ่มใหม่\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = [para]\n",
        "                current_length = para_length\n",
        "            else:\n",
        "                current_chunk.append(para)\n",
        "                current_length += para_length\n",
        "\n",
        "        # Chunk สุดท้าย\n",
        "        if current_chunk:\n",
        "            chunks.append('\\n\\n'.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_batch(self, file_pattern: str = \"*.txt\", limit: int = None):\n",
        "        \"\"\"\n",
        "        Process หลายไฟล์\n",
        "\n",
        "        Args:\n",
        "            file_pattern: pattern ของไฟล์ที่จะ process\n",
        "            limit: จำนวนไฟล์สูงสุด (None = ทั้งหมด)\n",
        "        \"\"\"\n",
        "        # หาไฟล์ทั้งหมด\n",
        "        raw_dir = Path(Config.RAW_OCR_DIR)\n",
        "        files = list(raw_dir.glob(file_pattern))\n",
        "\n",
        "        if limit:\n",
        "            files = files[:limit]\n",
        "\n",
        "        print(f\"\\n🚀 Processing {len(files)} files...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Process แต่ละไฟล์\n",
        "        results = []\n",
        "        for file_path in tqdm(files, desc=\"Processing\"):\n",
        "            result = self.process_file(file_path)\n",
        "            results.append(result)\n",
        "\n",
        "            # Rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # สรุปผล\n",
        "        self._print_summary(results)\n",
        "\n",
        "        # บันทึก training pairs\n",
        "        self._save_training_pairs()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _print_summary(self, results: List[Dict]):\n",
        "        \"\"\"แสดงสรุปผลการ process พร้อม validation summary\"\"\"\n",
        "        successful = [r for r in results if r.get('success')]\n",
        "        total_tokens = sum(r.get('tokens', 0) for r in successful)\n",
        "        total_cost = sum(r.get('cost', 0) for r in successful)\n",
        "\n",
        "        # นับ validation status\n",
        "        validation_stats = {\n",
        "            'PASS': 0,\n",
        "            'WARNING': 0,\n",
        "            'FAIL': 0\n",
        "        }\n",
        "\n",
        "        for r in successful:\n",
        "            if 'validation' in r:\n",
        "                status = r['validation'].get('status', 'UNKNOWN')\n",
        "                validation_stats[status] = validation_stats.get(status, 0) + 1\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"📊 PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"✅ Success: {len(successful)}/{len(results)}\")\n",
        "        print(f\"❌ Failed: {len(results) - len(successful)}\")\n",
        "        print(f\"🔤 Total tokens: {total_tokens:,}\")\n",
        "        print(f\"💰 Total cost: ${total_cost:.4f} (~{total_cost*35:.2f} บาท)\")\n",
        "\n",
        "        # Validation summary\n",
        "        print(f\"\\n📋 Validation Summary:\")\n",
        "        print(f\"   ✅ Passed: {validation_stats.get('PASS', 0)}\")\n",
        "        print(f\"   ⚠️ Warnings: {validation_stats.get('WARNING', 0)}\")\n",
        "        print(f\"   ❌ Failed: {validation_stats.get('FAIL', 0)}\")\n",
        "\n",
        "        # แจ้งเตือนถ้ามี validation issues\n",
        "        if validation_stats.get('WARNING', 0) > 0:\n",
        "            print(f\"\\n💡 มี {validation_stats['WARNING']} ไฟล์ที่ควรตรวจสอบ\")\n",
        "            print(f\"   ดู validation reports ใน: {Config.LOGS_DIR}\")\n",
        "\n",
        "        if validation_stats.get('FAIL', 0) > 0:\n",
        "            print(f\"\\n⚠️ มี {validation_stats['FAIL']} ไฟล์ที่ validation ไม่ผ่าน\")\n",
        "            print(f\"   ไฟล์เหล่านี้มี prefix 'WARNING_' ใน cleaned folder\")\n",
        "\n",
        "        print(f\"\\n📁 Cleaned files saved to: {Config.CLEANED_DIR}\")\n",
        "\n",
        "        # API usage summary\n",
        "        usage = self.llm.get_usage_summary()\n",
        "        print(f\"\\n📈 API Usage:\")\n",
        "        print(f\"   Model: {Config.MODEL}\")\n",
        "        print(f\"   Tokens: {usage['total_tokens']:,}\")\n",
        "        print(f\"   Cost: ${usage['total_cost_usd']:.4f} (~{usage['total_cost_thb']:.2f} บาท)\")\n",
        "\n",
        "    def _save_training_pairs(self):\n",
        "        \"\"\"บันทึก training pairs สำหรับ fine-tuning\"\"\"\n",
        "        if not self.training_pairs:\n",
        "            return\n",
        "\n",
        "        # Save as JSONL\n",
        "        pairs_file = Path(Config.TRAINING_PAIRS_DIR) / f\"pairs_{datetime.now().strftime('%Y%m%d')}.jsonl\"\n",
        "\n",
        "        with open(pairs_file, 'w', encoding='utf-8') as f:\n",
        "            for pair in self.training_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"💾 Training pairs saved: {pairs_file}\")\n",
        "\n",
        "print(\"✅ OCR Processor ready\")"
      ],
      "metadata": {
        "id": "giqxK3Sy3FnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 5: Enhanced Quality Validator\n",
        "# ============================================\n",
        "class QualityValidator:\n",
        "    \"\"\"ตรวจสอบคุณภาพของ cleaned text แบบละเอียด\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(raw_text: str, cleaned_text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        ตรวจสอบคุณภาพการ clean แบบพื้นฐาน\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "\n",
        "        # 1. ตรวจสอบความยาว\n",
        "        len_ratio = len(cleaned_text) / len(raw_text) if raw_text else 0\n",
        "        len_change = (len(cleaned_text) - len(raw_text)) / len(raw_text) * 100 if raw_text else 0\n",
        "\n",
        "        if len_ratio < 0.8:\n",
        "            issues.append(f\"⚠️ ข้อความสั้นลง {abs(len_change):.1f}% (อาจมีการลบเนื้อหา)\")\n",
        "        elif len_ratio > 1.2:\n",
        "            issues.append(f\"⚠️ ข้อความยาวขึ้น {len_change:.1f}% (อาจมีการเพิ่มเนื้อหา)\")\n",
        "        elif len_ratio < 0.9:\n",
        "            warnings.append(f\"📝 ข้อความสั้นลง {abs(len_change):.1f}%\")\n",
        "        elif len_ratio > 1.1:\n",
        "            warnings.append(f\"📝 ข้อความยาวขึ้น {len_change:.1f}%\")\n",
        "\n",
        "        # 2. ตรวจสอบ quotes\n",
        "        raw_quotes = len(re.findall(r'\"[^\"]*\"', raw_text))\n",
        "        clean_quotes = len(re.findall(r'\"[^\"]*\"', cleaned_text))\n",
        "\n",
        "        if abs(raw_quotes - clean_quotes) > 3:\n",
        "            issues.append(f\"⚠️ จำนวน quotes ต่างกันมาก ({raw_quotes} → {clean_quotes})\")\n",
        "        elif abs(raw_quotes - clean_quotes) > 1:\n",
        "            warnings.append(f\"📝 จำนวน quotes ต่างกัน ({raw_quotes} → {clean_quotes})\")\n",
        "\n",
        "        # 3. คำนวณ score\n",
        "        score = 1.0\n",
        "        score -= len(issues) * 0.2\n",
        "        score -= len(warnings) * 0.05\n",
        "        score = max(0, min(1, score))\n",
        "\n",
        "        return {\n",
        "            'valid': len(issues) == 0,\n",
        "            'score': score,\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'length_ratio': len_ratio,\n",
        "            'length_change_percent': len_change\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def enhanced_validate(raw_text: str, cleaned_text: str, filename: str = \"\") -> Dict:\n",
        "        \"\"\"\n",
        "        ตรวจสอบแบบละเอียด พร้อม diff และ suspicious changes\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "        suspicious_changes = []\n",
        "\n",
        "        # 1. Basic validation\n",
        "        basic_result = QualityValidator.validate(raw_text, cleaned_text)\n",
        "        issues.extend(basic_result['issues'])\n",
        "        warnings.extend(basic_result['warnings'])\n",
        "\n",
        "        # 2. ตรวจคำที่เปลี่ยน\n",
        "        raw_words = set(raw_text.split())\n",
        "        clean_words = set(cleaned_text.split())\n",
        "\n",
        "        added_words = clean_words - raw_words\n",
        "        removed_words = raw_words - clean_words\n",
        "\n",
        "        # 3. หาคำที่น่าสงสัย\n",
        "        for word in added_words:\n",
        "            # คำยาวเกิน 15 ตัว = น่าสงสัย\n",
        "            if len(word) > 15:\n",
        "                suspicious_changes.append(f\"➕ เพิ่มคำยาว: '{word}'\")\n",
        "            # คำภาษาอังกฤษที่ไม่น่าอยู่ในนิยายไทย\n",
        "            elif word.isascii() and len(word) > 3 and word.lower() not in ['okay', 'yes', 'no']:\n",
        "                suspicious_changes.append(f\"➕ เพิ่มภาษาอังกฤษ: '{word}'\")\n",
        "\n",
        "        # ตรวจคำที่หายไปเยอะ\n",
        "        if len(removed_words) > 20:\n",
        "            suspicious_changes.append(f\"➖ คำหายไป {len(removed_words)} คำ\")\n",
        "\n",
        "        # 4. ตรวจ pattern ที่เปลี่ยนบ่อย\n",
        "        pattern_changes = QualityValidator._check_common_patterns(raw_text, cleaned_text)\n",
        "        if pattern_changes:\n",
        "            warnings.extend(pattern_changes)\n",
        "\n",
        "        # 5. สร้าง diff sample\n",
        "        diff_sample = QualityValidator._get_diff_sample(raw_text, cleaned_text)\n",
        "\n",
        "        # 6. สร้าง report\n",
        "        status = 'PASS'\n",
        "        if issues:\n",
        "            status = 'FAIL'\n",
        "        elif warnings or suspicious_changes:\n",
        "            status = 'WARNING'\n",
        "\n",
        "        report = {\n",
        "            'filename': filename,\n",
        "            'status': status,\n",
        "            'score': basic_result['score'],\n",
        "            'stats': {\n",
        "                'length_change': f\"{basic_result['length_change_percent']:+.1f}%\",\n",
        "                'words_added': len(added_words),\n",
        "                'words_removed': len(removed_words),\n",
        "                'quotes_change': f\"{len(re.findall(r'\\\"', raw_text))} → {len(re.findall(r'\\\"', cleaned_text))}\"\n",
        "            },\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'suspicious': suspicious_changes[:5],  # แสดงแค่ 5 อันแรก\n",
        "            'diff_sample': diff_sample,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_common_patterns(raw_text: str, cleaned_text: str) -> List[str]:\n",
        "        \"\"\"ตรวจ pattern ที่มักเปลี่ยน\"\"\"\n",
        "        warnings = []\n",
        "\n",
        "        # ตรวจ ๆ (ไม้ยมก)\n",
        "        raw_yamok_space = raw_text.count(' ๆ ')\n",
        "        clean_yamok_space = cleaned_text.count(' ๆ ')\n",
        "        raw_yamok_no_space = raw_text.count('ๆ') - raw_yamok_space\n",
        "        clean_yamok_no_space = cleaned_text.count('ๆ') - clean_yamok_space\n",
        "\n",
        "        if raw_yamok_space != clean_yamok_space or raw_yamok_no_space != clean_yamok_no_space:\n",
        "            warnings.append(f\"📝 รูปแบบ 'ๆ' เปลี่ยน (space: {raw_yamok_space}→{clean_yamok_space}, no-space: {raw_yamok_no_space}→{clean_yamok_no_space})\")\n",
        "\n",
        "        # ตรวจตัวเลข\n",
        "        raw_numbers = len(re.findall(r'\\d+', raw_text))\n",
        "        clean_numbers = len(re.findall(r'\\d+', cleaned_text))\n",
        "        if abs(raw_numbers - clean_numbers) > 2:\n",
        "            warnings.append(f\"📝 จำนวนตัวเลขเปลี่ยน ({raw_numbers} → {clean_numbers})\")\n",
        "\n",
        "        return warnings\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_diff_sample(raw_text: str, cleaned_text: str, max_lines: int = 3) -> List[str]:\n",
        "        \"\"\"แสดงตัวอย่างที่เปลี่ยน\"\"\"\n",
        "        import difflib\n",
        "\n",
        "        # แบ่งเป็นบรรทัด\n",
        "        raw_lines = raw_text[:500].split('\\n')\n",
        "        clean_lines = cleaned_text[:500].split('\\n')\n",
        "\n",
        "        # หา diff\n",
        "        diff = difflib.unified_diff(\n",
        "            raw_lines,\n",
        "            clean_lines,\n",
        "            lineterm='',\n",
        "            n=0\n",
        "        )\n",
        "\n",
        "        changes = []\n",
        "        for line in diff:\n",
        "            if line.startswith('+') and not line.startswith('+++'):\n",
        "                changes.append(f\"✅ {line[1:][:100]}\")  # จำกัด 100 chars\n",
        "            elif line.startswith('-') and not line.startswith('---'):\n",
        "                changes.append(f\"❌ {line[1:][:100]}\")\n",
        "\n",
        "        return changes[:max_lines]\n",
        "\n",
        "    @staticmethod\n",
        "    def save_validation_report(report: Dict, output_dir: str = None):\n",
        "        \"\"\"บันทึก validation report\"\"\"\n",
        "        if output_dir is None:\n",
        "            output_dir = Config.LOGS_DIR\n",
        "\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # สร้างชื่อไฟล์\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = report.get('filename', 'unknown').replace('.txt', '')\n",
        "        report_file = Path(output_dir) / f\"validation_{filename}_{timestamp}.json\"\n",
        "\n",
        "        # บันทึก\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return report_file\n",
        "\n",
        "print(\"✅ Enhanced Quality Validator ready\")"
      ],
      "metadata": {
        "id": "x87inmdr3Kj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 6: Main Menu\n",
        "# ============================================\n",
        "def main_menu():\n",
        "    \"\"\"Interactive menu สำหรับใช้งาน\"\"\"\n",
        "\n",
        "    processor = OCRProcessor()\n",
        "\n",
        "    while True:\n",
        "        print(\"\"\"\n",
        "╔════════════════════════════════════════════╗\n",
        "║     OCR PROCESSING WITH API v1.0            ║\n",
        "║          Automated Thai Novel OCR           ║\n",
        "╚════════════════════════════════════════════╝\n",
        "\n",
        "[1] 🚀 Process ไฟล์เดียว\n",
        "[2] 📦 Process หลายไฟล์ (Batch)\n",
        "[3] 💰 Check API usage & cost\n",
        "[4] 🔧 Test with sample text\n",
        "[5] 📊 View statistics\n",
        "[6] ⚙️ Settings\n",
        "[7] ❌ Exit\n",
        "\n",
        "        \"\"\")\n",
        "\n",
        "        choice = input(\"Select (1-7): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            # Process single file\n",
        "            print(\"\\n📄 Single File Processing\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # List available files\n",
        "            raw_dir = Path(Config.RAW_OCR_DIR)\n",
        "            files = list(raw_dir.glob(\"*.txt\"))\n",
        "\n",
        "            if not files:\n",
        "                print(\"❌ ไม่พบไฟล์ใน raw_ocr/\")\n",
        "                input(\"\\nPress Enter to continue...\")\n",
        "                continue\n",
        "\n",
        "            print(\"พบไฟล์:\")\n",
        "            for i, f in enumerate(files[:10], 1):\n",
        "                print(f\"  [{i}] {f.name}\")\n",
        "\n",
        "            file_idx = input(\"\\nเลือกไฟล์ (number): \").strip()\n",
        "\n",
        "            try:\n",
        "                selected_file = files[int(file_idx) - 1]\n",
        "                processor.process_file(selected_file)\n",
        "            except:\n",
        "                print(\"❌ Invalid selection\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            # Batch processing\n",
        "            print(\"\\n📦 Batch Processing\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            limit = input(\"จำนวนไฟล์ที่จะ process (Enter = ทั้งหมด): \").strip()\n",
        "            limit = int(limit) if limit else None\n",
        "\n",
        "            confirm = input(f\"\\n⚠️ จะ process {limit or 'ทั้งหมด'} ไฟล์ ต้องการดำเนินการ? (y/n): \")\n",
        "\n",
        "            if confirm.lower() == 'y':\n",
        "                processor.process_batch(limit=limit)\n",
        "            else:\n",
        "                print(\"❌ Cancelled\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            # Check usage\n",
        "            print(\"\\n💰 API Usage & Cost\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            usage = processor.llm.get_usage_summary()\n",
        "            print(f\"Model: {Config.MODEL}\")\n",
        "            print(f\"Total tokens: {usage['total_tokens']:,}\")\n",
        "            print(f\"Total cost: ${usage['total_cost_usd']:.4f}\")\n",
        "            print(f\"Total cost (THB): ~{usage['total_cost_thb']:.2f} บาท\")\n",
        "            print(f\"Est. pages: ~{usage['pages_processed']}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # Test sample\n",
        "            print(\"\\n🔧 Test with Sample\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            sample = \"\"\"หอประชุมเป็นอาคารที่มีหน้าตาคล้ายบ้านชั้นเดียวทั่วไป\n",
        "แต่มีขนาดใหญ่กว่าเล็กน้อย ชายหญิงในชุดไว้ทุกข์หลายคน\n",
        "เดินขวักไขว่ไปมาท่ามกลางความวุ่นวาย\"\"\"\n",
        "\n",
        "            print(\"Sample text:\")\n",
        "            print(sample)\n",
        "            print(\"\\nProcessing...\")\n",
        "\n",
        "            result = processor.llm.clean_ocr_text(sample)\n",
        "\n",
        "            print(\"\\nCleaned text:\")\n",
        "            print(result['cleaned_text'])\n",
        "            print(f\"\\nTokens: {result['tokens_used']}\")\n",
        "            print(f\"Cost: ${result['cost']:.4f}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            # Statistics\n",
        "            print(\"\\n📊 Statistics\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # Count files\n",
        "            raw_files = len(list(Path(Config.RAW_OCR_DIR).glob(\"*.txt\")))\n",
        "            clean_files = len(list(Path(Config.CLEANED_DIR).glob(\"*.txt\")))\n",
        "\n",
        "            print(f\"Raw OCR files: {raw_files}\")\n",
        "            print(f\"Cleaned files: {clean_files}\")\n",
        "            print(f\"Success rate: {processor.stats['processed']}/{processor.stats['processed'] + processor.stats['failed']}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '6':\n",
        "            # Settings\n",
        "            print(\"\\n⚙️ Settings\")\n",
        "            print(\"-\" * 40)\n",
        "            print(f\"Current model: {Config.MODEL}\")\n",
        "            print(f\"Temperature: {Config.TEMPERATURE}\")\n",
        "            print(f\"Max tokens: {Config.MAX_TOKENS}\")\n",
        "\n",
        "            change = input(\"\\nChange model? (y/n): \")\n",
        "            if change.lower() == 'y':\n",
        "                print(\"\\nAvailable models:\")\n",
        "                print(\"[1] gpt-4o-mini (cheapest)\")\n",
        "                print(\"[2] gpt-3.5-turbo\")\n",
        "                print(\"[3] claude-3-haiku\")\n",
        "\n",
        "                model_choice = input(\"Select: \").strip()\n",
        "                if model_choice == '1':\n",
        "                    Config.MODEL = 'gpt-4o-mini'\n",
        "                elif model_choice == '2':\n",
        "                    Config.MODEL = 'gpt-3.5-turbo'\n",
        "                elif model_choice == '3':\n",
        "                    Config.MODEL = 'claude-3-haiku'\n",
        "\n",
        "                processor.llm = LLMClient()  # Reinitialize\n",
        "                print(f\"✅ Model changed to: {Config.MODEL}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '7':\n",
        "            print(\"\\n👋 Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Invalid choice\")\n",
        "\n",
        "print(\"✅ Main menu ready\")"
      ],
      "metadata": {
        "id": "Z3MsFtPV3O2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 7: Quick Start Functions\n",
        "# ============================================\n",
        "\n",
        "def quick_setup():\n",
        "    \"\"\"Setup API key และ test connection\"\"\"\n",
        "    print(\"\\n🔧 Quick Setup\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check API key\n",
        "    if not Config.OPENAI_API_KEY and not Config.ANTHROPIC_API_KEY:\n",
        "        print(\"\\n⚠️ ไม่พบ API key!\")\n",
        "        print(\"\\nวิธีใส่ API key:\")\n",
        "        print(\"1. แก้ใน Config class ด้านบน\")\n",
        "        print(\"2. หรือ set environment variable:\")\n",
        "\n",
        "        provider = input(\"\\nใช้ [1] OpenAI หรือ [2] Anthropic? : \").strip()\n",
        "\n",
        "        if provider == '1':\n",
        "            key = input(\"Enter OpenAI API key: \").strip()\n",
        "            Config.OPENAI_API_KEY = key\n",
        "            Config.MODEL = 'gpt-4o-mini'\n",
        "        else:\n",
        "            key = input(\"Enter Anthropic API key: \").strip()\n",
        "            Config.ANTHROPIC_API_KEY = key\n",
        "            Config.MODEL = 'claude-3-haiku'\n",
        "\n",
        "    # Test connection\n",
        "    print(\"\\n🔍 Testing API connection...\")\n",
        "    try:\n",
        "        client = LLMClient()\n",
        "        result = client.clean_ocr_text(\"ทดสอบ API\")\n",
        "        print(\"✅ API connection successful!\")\n",
        "        print(f\"   Model: {Config.MODEL}\")\n",
        "        print(f\"   Test cost: ${result['cost']:.4f}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ API test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def process_single_file_quick(filename: str):\n",
        "    \"\"\"Process 1 ไฟล์แบบเร็ว\"\"\"\n",
        "    processor = OCRProcessor()\n",
        "    file_path = Path(Config.RAW_OCR_DIR) / filename\n",
        "\n",
        "    if not file_path.exists():\n",
        "        print(f\"❌ File not found: {filename}\")\n",
        "        return\n",
        "\n",
        "    result = processor.process_file(file_path)\n",
        "\n",
        "    if result['success']:\n",
        "        print(f\"\\n✅ Success!\")\n",
        "        print(f\"   Output: {result['cleaned_path']}\")\n",
        "        print(f\"   Cost: ${result['cost']:.4f} (~{result['cost']*35:.2f} บาท)\")\n",
        "    else:\n",
        "        print(f\"\\n❌ Failed: {result.get('error')}\")"
      ],
      "metadata": {
        "id": "YqofTKi-3SyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 8: Main Execution\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\"\"\n",
        "    ================================================================================\n",
        "                      OCR PROCESSING WITH API v1.0\n",
        "                         Automated Thai Novel OCR\n",
        "    ================================================================================\n",
        "\n",
        "    🎯 Features:\n",
        "       - Automated OCR cleaning with GPT/Claude\n",
        "       - Cost tracking & optimization\n",
        "       - Quality validation\n",
        "       - Training pairs collection\n",
        "\n",
        "    💰 Estimated cost:\n",
        "       - GPT-4o-mini: ~0.01 บาท/หน้า\n",
        "       - 100 หน้า = ~1 บาท\n",
        "       - 1,000 หน้า = ~10 บาท\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    # Quick setup ถ้ายังไม่มี API key\n",
        "    if not Config.OPENAI_API_KEY and not Config.ANTHROPIC_API_KEY:\n",
        "        print(\"📝 ต้อง setup API key ก่อน\")\n",
        "        if quick_setup():\n",
        "            print(\"\\n✅ Setup complete! Ready to use\")\n",
        "        else:\n",
        "            print(\"\\n❌ Setup failed. Please check API key\")\n",
        "            exit(1)\n",
        "\n",
        "    # Run main menu\n",
        "    print(\"\\n🚀 Starting main menu...\")\n",
        "    main_menu()\n",
        "\n",
        "    print(\"\\n🎉 Thank you for using OCR Processor!\")"
      ],
      "metadata": {
        "id": "1kpRPoH53Vzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ยินดีต้อนรับสู่ Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}