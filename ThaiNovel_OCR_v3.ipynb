{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/ThaiNovel_OCR_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "                OCR PROCESSING WITH API - AUTOMATED VERSION\n",
        "                       ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ Corpus ‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢\n",
        "================================================================================\n",
        "\n",
        "Features:\n",
        "1. Automated OCR ‚Üí LLM ‚Üí Clean corpus\n",
        "2. ‡πÉ‡∏ä‡πâ GPT-4o-mini (‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡∏î) ‡∏´‡∏£‡∏∑‡∏≠ Claude Haiku\n",
        "3. Quality validation & tracking\n",
        "4. Save training pairs for fine-tuning\n",
        "\n",
        "Requirements:\n",
        "- pip install openai anthropic pandas tqdm\n",
        "- API keys (OpenAI ‡∏´‡∏£‡∏∑‡∏≠ Anthropic)\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# üìå Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Drive (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")"
      ],
      "metadata": {
        "id": "hT-ghRST21PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 2: Configuration & API Setup\n",
        "# ============================================\n",
        "class Config:\n",
        "    \"\"\"Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API Processing\"\"\"\n",
        "\n",
        "    # ‚ö†Ô∏è ‡πÉ‡∏™‡πà API Keys ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ environment variables)\n",
        "    OPENAI_API_KEY = \"\"  # ‡πÉ‡∏™‡πà OpenAI API key\n",
        "    ANTHROPIC_API_KEY = \"\"  # ‡πÉ‡∏™‡πà Anthropic API key (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Claude)\n",
        "\n",
        "    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model (uncomment ‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ)\n",
        "    MODEL = \"gpt-4o-mini\"  # ‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡∏î ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!\n",
        "    # MODEL = \"gpt-3.5-turbo\"\n",
        "    # MODEL = \"claude-3-haiku\"\n",
        "\n",
        "    # Paths (Google Drive)\n",
        "    BASE = '/content/drive/MyDrive/OCR' if IN_COLAB else './OCR'\n",
        "\n",
        "    RAW_OCR_DIR = f'{BASE}/raw_ocr'\n",
        "    CLEANED_DIR = f'{BASE}/cleaned'\n",
        "    CORPUS_DIR = f'{BASE}/final_corpus'\n",
        "    TRAINING_PAIRS_DIR = f'{BASE}/training_pairs'\n",
        "    LOGS_DIR = f'{BASE}/logs'\n",
        "\n",
        "    # Processing settings\n",
        "    MAX_PAGES_PER_BATCH = 5  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠ API call\n",
        "    MAX_RETRIES = 3  # retry ‡∏ñ‡πâ‡∏≤ API error\n",
        "    TEMPERATURE = 0.1  # ‡∏ï‡πà‡∏≥ = consistent output\n",
        "    MAX_TOKENS = 4000  # max response length\n",
        "\n",
        "    # Cost tracking\n",
        "    PRICE_PER_1K_TOKENS = {\n",
        "        'gpt-4o-mini': 0.00015,  # $0.15 per 1M\n",
        "        'gpt-3.5-turbo': 0.0005,\n",
        "        'claude-3-haiku': 0.00025\n",
        "    }\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á folders\n",
        "for folder in [Config.RAW_OCR_DIR, Config.CLEANED_DIR, Config.CORPUS_DIR,\n",
        "               Config.TRAINING_PAIRS_DIR, Config.LOGS_DIR]:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Config loaded\")"
      ],
      "metadata": {
        "id": "Y8rT6G4U26EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 3: API Clients\n",
        "# ============================================\n",
        "class LLMClient:\n",
        "    \"\"\"Universal LLM Client ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI ‡πÅ‡∏•‡∏∞ Anthropic\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = Config.MODEL\n",
        "        self.client = None\n",
        "        self.total_tokens = 0\n",
        "        self.total_cost = 0\n",
        "\n",
        "        # Initialize ‡∏ï‡∏≤‡∏° model\n",
        "        if 'gpt' in self.model:\n",
        "            self._init_openai()\n",
        "        elif 'claude' in self.model:\n",
        "            self._init_anthropic()\n",
        "\n",
        "    def _init_openai(self):\n",
        "        \"\"\"Initialize OpenAI client\"\"\"\n",
        "        try:\n",
        "            import openai\n",
        "\n",
        "            # Set API key\n",
        "            if Config.OPENAI_API_KEY:\n",
        "                openai.api_key = Config.OPENAI_API_KEY\n",
        "            else:\n",
        "                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å environment variable\n",
        "                openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "            if not openai.api_key:\n",
        "                raise ValueError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö OpenAI API key! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡πÉ‡∏ô Config\")\n",
        "\n",
        "            self.client = openai.OpenAI(api_key=openai.api_key)\n",
        "            print(f\"‚úÖ OpenAI client ready (Model: {self.model})\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install openai\")\n",
        "            raise\n",
        "\n",
        "    def _init_anthropic(self):\n",
        "        \"\"\"Initialize Anthropic client\"\"\"\n",
        "        try:\n",
        "            import anthropic\n",
        "\n",
        "            if Config.ANTHROPIC_API_KEY:\n",
        "                api_key = Config.ANTHROPIC_API_KEY\n",
        "            else:\n",
        "                api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "            if not api_key:\n",
        "                raise ValueError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Anthropic API key!\")\n",
        "\n",
        "            self.client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(f\"‚úÖ Anthropic client ready (Model: {self.model})\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install anthropic\")\n",
        "            raise\n",
        "\n",
        "    def clean_ocr_text(self, text: str, page_num: int = 1) -> Dict:\n",
        "        \"\"\"\n",
        "        ‡∏™‡πà‡∏á OCR text ‡πÉ‡∏´‡πâ LLM ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç\n",
        "\n",
        "        Returns:\n",
        "            {\n",
        "                'cleaned_text': str,\n",
        "                'tokens_used': int,\n",
        "                'cost': float,\n",
        "                'changes': list\n",
        "            }\n",
        "        \"\"\"\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt\n",
        "        prompt = self._create_prompt(text)\n",
        "\n",
        "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏ï‡∏≤‡∏° provider\n",
        "        if 'gpt' in self.model:\n",
        "            result = self._call_openai(prompt)\n",
        "        else:\n",
        "            result = self._call_anthropic(prompt)\n",
        "\n",
        "        # Track usage\n",
        "        self.total_tokens += result['tokens_used']\n",
        "        self.total_cost += result['cost']\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _create_prompt(self, text: str) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR cleaning\"\"\"\n",
        "        return f\"\"\"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR ‡∏à‡∏≤‡∏Å‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n",
        "\n",
        "‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:\n",
        "1. ‡πÅ‡∏Å‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ typo ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î\n",
        "2. ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢/‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô \"‡∏°‡∏≤ ‡∏Å‡∏≥‡∏•‡∏±‡∏á\" ‚Üí \"‡∏°‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á\")\n",
        "3. ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢\n",
        "4. ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÉ‡∏ô \"...\")\n",
        "5. ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡∏°‡πà\n",
        "6. ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR:\n",
        "{text}\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß:\"\"\"\n",
        "\n",
        "    def _call_openai(self, prompt: str) -> Dict:\n",
        "        \"\"\"Call OpenAI API\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç OCR ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                max_tokens=Config.MAX_TOKENS\n",
        "            )\n",
        "\n",
        "            # Extract result\n",
        "            cleaned_text = response.choices[0].message.content\n",
        "            tokens = response.usage.total_tokens\n",
        "\n",
        "            # Calculate cost\n",
        "            price_per_token = Config.PRICE_PER_1K_TOKENS.get(self.model, 0.0005) / 1000\n",
        "            cost = tokens * price_per_token\n",
        "\n",
        "            return {\n",
        "                'cleaned_text': cleaned_text,\n",
        "                'tokens_used': tokens,\n",
        "                'cost': cost,\n",
        "                'model': self.model\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OpenAI API error: {e}\")\n",
        "            # Retry logic\n",
        "            for retry in range(Config.MAX_RETRIES):\n",
        "                time.sleep(2 ** retry)  # Exponential backoff\n",
        "                try:\n",
        "                    return self._call_openai(prompt)\n",
        "                except:\n",
        "                    continue\n",
        "            raise\n",
        "\n",
        "    def _call_anthropic(self, prompt: str) -> Dict:\n",
        "        \"\"\"Call Anthropic API\"\"\"\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=Config.MAX_TOKENS,\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            cleaned_text = response.content[0].text\n",
        "            tokens = response.usage.input_tokens + response.usage.output_tokens\n",
        "\n",
        "            price_per_token = Config.PRICE_PER_1K_TOKENS.get(self.model, 0.00025) / 1000\n",
        "            cost = tokens * price_per_token\n",
        "\n",
        "            return {\n",
        "                'cleaned_text': cleaned_text,\n",
        "                'tokens_used': tokens,\n",
        "                'cost': cost,\n",
        "                'model': self.model\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Anthropic API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_usage_summary(self) -> Dict:\n",
        "        \"\"\"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô API\"\"\"\n",
        "        return {\n",
        "            'total_tokens': self.total_tokens,\n",
        "            'total_cost_usd': self.total_cost,\n",
        "            'total_cost_thb': self.total_cost * 35,  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì\n",
        "            'pages_processed': self.total_tokens // 500  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 500 tokens/page\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ LLM Client ready\")"
      ],
      "metadata": {
        "id": "sOnSRufE2_v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# üìå Block 4: OCR Processor\n",
        "# ============================================\n",
        "class OCRProcessor:\n",
        "    \"\"\"Main processor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR ‚Üí LLM ‚Üí Clean corpus\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.llm = LLMClient()\n",
        "        self.stats = {\n",
        "            'processed': 0,\n",
        "            'failed': 0,\n",
        "            'total_cost': 0\n",
        "        }\n",
        "        self.training_pairs = []\n",
        "\n",
        "    def process_file(self, file_path: Path) -> Dict:\n",
        "        \"\"\"\n",
        "        Process 1 ‡πÑ‡∏ü‡∏•‡πå OCR ‡∏û‡∏£‡πâ‡∏≠‡∏° validation\n",
        "\n",
        "        Returns:\n",
        "            {\n",
        "                'success': bool,\n",
        "                'cleaned_path': str,\n",
        "                'stats': dict,\n",
        "                'validation': dict\n",
        "            }\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìÑ Processing: {file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå OCR\n",
        "            raw_text = file_path.read_text(encoding='utf-8')\n",
        "\n",
        "            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á chunks\n",
        "            if len(raw_text) > 3000:\n",
        "                chunks = self._split_text(raw_text)\n",
        "                cleaned_chunks = []\n",
        "\n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    print(f\"   Chunk {i+1}/{len(chunks)}...\")\n",
        "                    result = self.llm.clean_ocr_text(chunk, i+1)\n",
        "                    cleaned_chunks.append(result['cleaned_text'])\n",
        "                    time.sleep(1)  # Rate limiting\n",
        "\n",
        "                cleaned_text = '\\n\\n'.join(cleaned_chunks)\n",
        "            else:\n",
        "                # ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å ‡∏™‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "                result = self.llm.clean_ocr_text(raw_text)\n",
        "                cleaned_text = result['cleaned_text']\n",
        "\n",
        "            # === VALIDATION ===\n",
        "            print(f\"   üîç Validating...\")\n",
        "            validation = QualityValidator.enhanced_validate(\n",
        "                raw_text,\n",
        "                cleaned_text,\n",
        "                file_path.name\n",
        "            )\n",
        "\n",
        "            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• validation\n",
        "            if validation['status'] == 'FAIL':\n",
        "                print(f\"   ‚ùå VALIDATION FAILED:\")\n",
        "                for issue in validation['issues']:\n",
        "                    print(f\"      {issue}\")\n",
        "            elif validation['status'] == 'WARNING':\n",
        "                print(f\"   ‚ö†Ô∏è VALIDATION WARNING:\")\n",
        "                if validation['warnings']:\n",
        "                    print(f\"      {validation['warnings'][0]}\")\n",
        "                if validation['suspicious']:\n",
        "                    print(f\"      {validation['suspicious'][0]}\")\n",
        "            else:\n",
        "                print(f\"   ‚úÖ Validation passed (score: {validation['score']:.2f})\")\n",
        "\n",
        "            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å validation report ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ issues\n",
        "            if validation['status'] in ['FAIL', 'WARNING']:\n",
        "                val_report_path = QualityValidator.save_validation_report(validation)\n",
        "                print(f\"   üìä Validation report: {val_report_path.name}\")\n",
        "\n",
        "            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÅ‡∏°‡πâ validation ‡∏à‡∏∞ fail ‡∏Å‡πá save ‡πÑ‡∏ß‡πâ review)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "            # ‡∏ñ‡πâ‡∏≤ validation fail ‡πÉ‡∏™‡πà prefix WARNING_\n",
        "            if validation['status'] == 'FAIL':\n",
        "                clean_filename = f\"WARNING_{file_path.stem}_clean_{timestamp}.txt\"\n",
        "            else:\n",
        "                clean_filename = f\"{file_path.stem}_clean_{timestamp}.txt\"\n",
        "\n",
        "            clean_path = Path(Config.CLEANED_DIR) / clean_filename\n",
        "            clean_path.write_text(cleaned_text, encoding='utf-8')\n",
        "\n",
        "            # ‡πÄ‡∏Å‡πá‡∏ö training pair (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô validation)\n",
        "            if validation['status'] != 'FAIL':\n",
        "                self.training_pairs.append({\n",
        "                    'input': raw_text[:1000],\n",
        "                    'output': cleaned_text[:1000],\n",
        "                    'source': file_path.name,\n",
        "                    'timestamp': timestamp,\n",
        "                    'validation_score': validation['score']\n",
        "                })\n",
        "\n",
        "            # Update stats\n",
        "            self.stats['processed'] += 1\n",
        "            if validation['status'] == 'FAIL':\n",
        "                self.stats['validation_failed'] = self.stats.get('validation_failed', 0) + 1\n",
        "            elif validation['status'] == 'WARNING':\n",
        "                self.stats['validation_warning'] = self.stats.get('validation_warning', 0) + 1\n",
        "\n",
        "            print(f\"   ‚úÖ Saved: {clean_filename}\")\n",
        "            print(f\"   üí∞ Cost: ${result.get('cost', 0):.4f}\")\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'cleaned_path': str(clean_path),\n",
        "                'tokens': result.get('tokens_used', 0),\n",
        "                'cost': result.get('cost', 0),\n",
        "                'validation': validation\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "            self.stats['failed'] += 1\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def _split_text(self, text: str, max_chars: int = 2500) -> List[str]:\n",
        "        \"\"\"‡πÅ‡∏ö‡πà‡∏á text ‡∏¢‡∏≤‡∏ß‡πÄ‡∏õ‡πá‡∏ô chunks\"\"\"\n",
        "        # ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° paragraph ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para_length = len(para)\n",
        "\n",
        "            if current_length + para_length > max_chars and current_chunk:\n",
        "                # Chunk ‡πÄ‡∏ï‡πá‡∏° - save ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = [para]\n",
        "                current_length = para_length\n",
        "            else:\n",
        "                current_chunk.append(para)\n",
        "                current_length += para_length\n",
        "\n",
        "        # Chunk ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
        "        if current_chunk:\n",
        "            chunks.append('\\n\\n'.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_batch(self, file_pattern: str = \"*.txt\", limit: int = None):\n",
        "        \"\"\"\n",
        "        Process ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå\n",
        "\n",
        "        Args:\n",
        "            file_pattern: pattern ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞ process\n",
        "            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (None = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
        "        \"\"\"\n",
        "        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "        raw_dir = Path(Config.RAW_OCR_DIR)\n",
        "        files = list(raw_dir.glob(file_pattern))\n",
        "\n",
        "        if limit:\n",
        "            files = files[:limit]\n",
        "\n",
        "        print(f\"\\nüöÄ Processing {len(files)} files...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Process ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå\n",
        "        results = []\n",
        "        for file_path in tqdm(files, desc=\"Processing\"):\n",
        "            result = self.process_file(file_path)\n",
        "            results.append(result)\n",
        "\n",
        "            # Rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\n",
        "        self._print_summary(results)\n",
        "\n",
        "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å training pairs\n",
        "        self._save_training_pairs()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _print_summary(self, results: List[Dict]):\n",
        "        \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ process ‡∏û‡∏£‡πâ‡∏≠‡∏° validation summary\"\"\"\n",
        "        successful = [r for r in results if r.get('success')]\n",
        "        total_tokens = sum(r.get('tokens', 0) for r in successful)\n",
        "        total_cost = sum(r.get('cost', 0) for r in successful)\n",
        "\n",
        "        # ‡∏ô‡∏±‡∏ö validation status\n",
        "        validation_stats = {\n",
        "            'PASS': 0,\n",
        "            'WARNING': 0,\n",
        "            'FAIL': 0\n",
        "        }\n",
        "\n",
        "        for r in successful:\n",
        "            if 'validation' in r:\n",
        "                status = r['validation'].get('status', 'UNKNOWN')\n",
        "                validation_stats[status] = validation_stats.get(status, 0) + 1\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"üìä PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"‚úÖ Success: {len(successful)}/{len(results)}\")\n",
        "        print(f\"‚ùå Failed: {len(results) - len(successful)}\")\n",
        "        print(f\"üî§ Total tokens: {total_tokens:,}\")\n",
        "        print(f\"üí∞ Total cost: ${total_cost:.4f} (~{total_cost*35:.2f} ‡∏ö‡∏≤‡∏ó)\")\n",
        "\n",
        "        # Validation summary\n",
        "        print(f\"\\nüìã Validation Summary:\")\n",
        "        print(f\"   ‚úÖ Passed: {validation_stats.get('PASS', 0)}\")\n",
        "        print(f\"   ‚ö†Ô∏è Warnings: {validation_stats.get('WARNING', 0)}\")\n",
        "        print(f\"   ‚ùå Failed: {validation_stats.get('FAIL', 0)}\")\n",
        "\n",
        "        # ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ validation issues\n",
        "        if validation_stats.get('WARNING', 0) > 0:\n",
        "            print(f\"\\nüí° ‡∏°‡∏µ {validation_stats['WARNING']} ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö\")\n",
        "            print(f\"   ‡∏î‡∏π validation reports ‡πÉ‡∏ô: {Config.LOGS_DIR}\")\n",
        "\n",
        "        if validation_stats.get('FAIL', 0) > 0:\n",
        "            print(f\"\\n‚ö†Ô∏è ‡∏°‡∏µ {validation_stats['FAIL']} ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà validation ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô\")\n",
        "            print(f\"   ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏µ prefix 'WARNING_' ‡πÉ‡∏ô cleaned folder\")\n",
        "\n",
        "        print(f\"\\nüìÅ Cleaned files saved to: {Config.CLEANED_DIR}\")\n",
        "\n",
        "        # API usage summary\n",
        "        usage = self.llm.get_usage_summary()\n",
        "        print(f\"\\nüìà API Usage:\")\n",
        "        print(f\"   Model: {Config.MODEL}\")\n",
        "        print(f\"   Tokens: {usage['total_tokens']:,}\")\n",
        "        print(f\"   Cost: ${usage['total_cost_usd']:.4f} (~{usage['total_cost_thb']:.2f} ‡∏ö‡∏≤‡∏ó)\")\n",
        "\n",
        "    def _save_training_pairs(self):\n",
        "        \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å training pairs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fine-tuning\"\"\"\n",
        "        if not self.training_pairs:\n",
        "            return\n",
        "\n",
        "        # Save as JSONL\n",
        "        pairs_file = Path(Config.TRAINING_PAIRS_DIR) / f\"pairs_{datetime.now().strftime('%Y%m%d')}.jsonl\"\n",
        "\n",
        "        with open(pairs_file, 'w', encoding='utf-8') as f:\n",
        "            for pair in self.training_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"üíæ Training pairs saved: {pairs_file}\")\n",
        "\n",
        "print(\"‚úÖ OCR Processor ready\")"
      ],
      "metadata": {
        "id": "giqxK3Sy3FnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 5: Enhanced Quality Validator\n",
        "# ============================================\n",
        "class QualityValidator:\n",
        "    \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á cleaned text ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(raw_text: str, cleaned_text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£ clean ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "\n",
        "        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß\n",
        "        len_ratio = len(cleaned_text) / len(raw_text) if raw_text else 0\n",
        "        len_change = (len(cleaned_text) - len(raw_text)) / len(raw_text) * 100 if raw_text else 0\n",
        "\n",
        "        if len_ratio < 0.8:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á {abs(len_change):.1f}% (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)\")\n",
        "        elif len_ratio > 1.2:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô {len_change:.1f}% (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)\")\n",
        "        elif len_ratio < 0.9:\n",
        "            warnings.append(f\"üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á {abs(len_change):.1f}%\")\n",
        "        elif len_ratio > 1.1:\n",
        "            warnings.append(f\"üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô {len_change:.1f}%\")\n",
        "\n",
        "        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö quotes\n",
        "        raw_quotes = len(re.findall(r'\"[^\"]*\"', raw_text))\n",
        "        clean_quotes = len(re.findall(r'\"[^\"]*\"', cleaned_text))\n",
        "\n",
        "        if abs(raw_quotes - clean_quotes) > 3:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô quotes ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å ({raw_quotes} ‚Üí {clean_quotes})\")\n",
        "        elif abs(raw_quotes - clean_quotes) > 1:\n",
        "            warnings.append(f\"üìù ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô quotes ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ({raw_quotes} ‚Üí {clean_quotes})\")\n",
        "\n",
        "        # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score\n",
        "        score = 1.0\n",
        "        score -= len(issues) * 0.2\n",
        "        score -= len(warnings) * 0.05\n",
        "        score = max(0, min(1, score))\n",
        "\n",
        "        return {\n",
        "            'valid': len(issues) == 0,\n",
        "            'score': score,\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'length_ratio': len_ratio,\n",
        "            'length_change_percent': len_change\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def enhanced_validate(raw_text: str, cleaned_text: str, filename: str = \"\") -> Dict:\n",
        "        \"\"\"\n",
        "        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏° diff ‡πÅ‡∏•‡∏∞ suspicious changes\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "        suspicious_changes = []\n",
        "\n",
        "        # 1. Basic validation\n",
        "        basic_result = QualityValidator.validate(raw_text, cleaned_text)\n",
        "        issues.extend(basic_result['issues'])\n",
        "        warnings.extend(basic_result['warnings'])\n",
        "\n",
        "        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
        "        raw_words = set(raw_text.split())\n",
        "        clean_words = set(cleaned_text.split())\n",
        "\n",
        "        added_words = clean_words - raw_words\n",
        "        removed_words = raw_words - clean_words\n",
        "\n",
        "        # 3. ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢\n",
        "        for word in added_words:\n",
        "            # ‡∏Ñ‡∏≥‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 15 ‡∏ï‡∏±‡∏ß = ‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢\n",
        "            if len(word) > 15:\n",
        "                suspicious_changes.append(f\"‚ûï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏¢‡∏≤‡∏ß: '{word}'\")\n",
        "            # ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢\n",
        "            elif word.isascii() and len(word) > 3 and word.lower() not in ['okay', 'yes', 'no']:\n",
        "                suspicious_changes.append(f\"‚ûï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©: '{word}'\")\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÄ‡∏¢‡∏≠‡∏∞\n",
        "        if len(removed_words) > 20:\n",
        "            suspicious_changes.append(f\"‚ûñ ‡∏Ñ‡∏≥‡∏´‡∏≤‡∏¢‡πÑ‡∏õ {len(removed_words)} ‡∏Ñ‡∏≥\")\n",
        "\n",
        "        # 4. ‡∏ï‡∏£‡∏ß‡∏à pattern ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πà‡∏≠‡∏¢\n",
        "        pattern_changes = QualityValidator._check_common_patterns(raw_text, cleaned_text)\n",
        "        if pattern_changes:\n",
        "            warnings.extend(pattern_changes)\n",
        "\n",
        "        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á diff sample\n",
        "        diff_sample = QualityValidator._get_diff_sample(raw_text, cleaned_text)\n",
        "\n",
        "        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á report\n",
        "        status = 'PASS'\n",
        "        if issues:\n",
        "            status = 'FAIL'\n",
        "        elif warnings or suspicious_changes:\n",
        "            status = 'WARNING'\n",
        "\n",
        "        report = {\n",
        "            'filename': filename,\n",
        "            'status': status,\n",
        "            'score': basic_result['score'],\n",
        "            'stats': {\n",
        "                'length_change': f\"{basic_result['length_change_percent']:+.1f}%\",\n",
        "                'words_added': len(added_words),\n",
        "                'words_removed': len(removed_words),\n",
        "                'quotes_change': f\"{len(re.findall(r'\\\"', raw_text))} ‚Üí {len(re.findall(r'\\\"', cleaned_text))}\"\n",
        "            },\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'suspicious': suspicious_changes[:5],  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 5 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å\n",
        "            'diff_sample': diff_sample,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_common_patterns(raw_text: str, cleaned_text: str) -> List[str]:\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à pattern ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\"\"\"\n",
        "        warnings = []\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à ‡πÜ (‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å)\n",
        "        raw_yamok_space = raw_text.count(' ‡πÜ ')\n",
        "        clean_yamok_space = cleaned_text.count(' ‡πÜ ')\n",
        "        raw_yamok_no_space = raw_text.count('‡πÜ') - raw_yamok_space\n",
        "        clean_yamok_no_space = cleaned_text.count('‡πÜ') - clean_yamok_space\n",
        "\n",
        "        if raw_yamok_space != clean_yamok_space or raw_yamok_no_space != clean_yamok_no_space:\n",
        "            warnings.append(f\"üìù ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö '‡πÜ' ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (space: {raw_yamok_space}‚Üí{clean_yamok_space}, no-space: {raw_yamok_no_space}‚Üí{clean_yamok_no_space})\")\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "        raw_numbers = len(re.findall(r'\\d+', raw_text))\n",
        "        clean_numbers = len(re.findall(r'\\d+', cleaned_text))\n",
        "        if abs(raw_numbers - clean_numbers) > 2:\n",
        "            warnings.append(f\"üìù ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ({raw_numbers} ‚Üí {clean_numbers})\")\n",
        "\n",
        "        return warnings\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_diff_sample(raw_text: str, cleaned_text: str, max_lines: int = 3) -> List[str]:\n",
        "        \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\"\"\"\n",
        "        import difflib\n",
        "\n",
        "        # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\n",
        "        raw_lines = raw_text[:500].split('\\n')\n",
        "        clean_lines = cleaned_text[:500].split('\\n')\n",
        "\n",
        "        # ‡∏´‡∏≤ diff\n",
        "        diff = difflib.unified_diff(\n",
        "            raw_lines,\n",
        "            clean_lines,\n",
        "            lineterm='',\n",
        "            n=0\n",
        "        )\n",
        "\n",
        "        changes = []\n",
        "        for line in diff:\n",
        "            if line.startswith('+') and not line.startswith('+++'):\n",
        "                changes.append(f\"‚úÖ {line[1:][:100]}\")  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 100 chars\n",
        "            elif line.startswith('-') and not line.startswith('---'):\n",
        "                changes.append(f\"‚ùå {line[1:][:100]}\")\n",
        "\n",
        "        return changes[:max_lines]\n",
        "\n",
        "    @staticmethod\n",
        "    def save_validation_report(report: Dict, output_dir: str = None):\n",
        "        \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å validation report\"\"\"\n",
        "        if output_dir is None:\n",
        "            output_dir = Config.LOGS_DIR\n",
        "\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = report.get('filename', 'unknown').replace('.txt', '')\n",
        "        report_file = Path(output_dir) / f\"validation_{filename}_{timestamp}.json\"\n",
        "\n",
        "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return report_file\n",
        "\n",
        "print(\"‚úÖ Enhanced Quality Validator ready\")"
      ],
      "metadata": {
        "id": "x87inmdr3Kj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 6: Main Menu\n",
        "# ============================================\n",
        "def main_menu():\n",
        "    \"\"\"Interactive menu ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\"\"\n",
        "\n",
        "    processor = OCRProcessor()\n",
        "\n",
        "    while True:\n",
        "        print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë     OCR PROCESSING WITH API v1.0            ‚ïë\n",
        "‚ïë          Automated Thai Novel OCR           ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "[1] üöÄ Process ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "[2] üì¶ Process ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå (Batch)\n",
        "[3] üí∞ Check API usage & cost\n",
        "[4] üîß Test with sample text\n",
        "[5] üìä View statistics\n",
        "[6] ‚öôÔ∏è Settings\n",
        "[7] ‚ùå Exit\n",
        "\n",
        "        \"\"\")\n",
        "\n",
        "        choice = input(\"Select (1-7): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            # Process single file\n",
        "            print(\"\\nüìÑ Single File Processing\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # List available files\n",
        "            raw_dir = Path(Config.RAW_OCR_DIR)\n",
        "            files = list(raw_dir.glob(\"*.txt\"))\n",
        "\n",
        "            if not files:\n",
        "                print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô raw_ocr/\")\n",
        "                input(\"\\nPress Enter to continue...\")\n",
        "                continue\n",
        "\n",
        "            print(\"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå:\")\n",
        "            for i, f in enumerate(files[:10], 1):\n",
        "                print(f\"  [{i}] {f.name}\")\n",
        "\n",
        "            file_idx = input(\"\\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå (number): \").strip()\n",
        "\n",
        "            try:\n",
        "                selected_file = files[int(file_idx) - 1]\n",
        "                processor.process_file(selected_file)\n",
        "            except:\n",
        "                print(\"‚ùå Invalid selection\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            # Batch processing\n",
        "            print(\"\\nüì¶ Batch Processing\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            limit = input(\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞ process (Enter = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î): \").strip()\n",
        "            limit = int(limit) if limit else None\n",
        "\n",
        "            confirm = input(f\"\\n‚ö†Ô∏è ‡∏à‡∏∞ process {limit or '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'} ‡πÑ‡∏ü‡∏•‡πå ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£? (y/n): \")\n",
        "\n",
        "            if confirm.lower() == 'y':\n",
        "                processor.process_batch(limit=limit)\n",
        "            else:\n",
        "                print(\"‚ùå Cancelled\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            # Check usage\n",
        "            print(\"\\nüí∞ API Usage & Cost\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            usage = processor.llm.get_usage_summary()\n",
        "            print(f\"Model: {Config.MODEL}\")\n",
        "            print(f\"Total tokens: {usage['total_tokens']:,}\")\n",
        "            print(f\"Total cost: ${usage['total_cost_usd']:.4f}\")\n",
        "            print(f\"Total cost (THB): ~{usage['total_cost_thb']:.2f} ‡∏ö‡∏≤‡∏ó\")\n",
        "            print(f\"Est. pages: ~{usage['pages_processed']}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # Test sample\n",
        "            print(\"\\nüîß Test with Sample\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            sample = \"\"\"‡∏´‡∏≠‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ö‡πâ‡∏≤‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n",
        "‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡∏ä‡∏≤‡∏¢‡∏´‡∏ç‡∏¥‡∏á‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡πÑ‡∏ß‡πâ‡∏ó‡∏∏‡∏Å‡∏Ç‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô\n",
        "‡πÄ‡∏î‡∏¥‡∏ô‡∏Ç‡∏ß‡∏±‡∏Å‡πÑ‡∏Ç‡∏ß‡πà‡πÑ‡∏õ‡∏°‡∏≤‡∏ó‡πà‡∏≤‡∏°‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏∏‡πà‡∏ô‡∏ß‡∏≤‡∏¢\"\"\"\n",
        "\n",
        "            print(\"Sample text:\")\n",
        "            print(sample)\n",
        "            print(\"\\nProcessing...\")\n",
        "\n",
        "            result = processor.llm.clean_ocr_text(sample)\n",
        "\n",
        "            print(\"\\nCleaned text:\")\n",
        "            print(result['cleaned_text'])\n",
        "            print(f\"\\nTokens: {result['tokens_used']}\")\n",
        "            print(f\"Cost: ${result['cost']:.4f}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            # Statistics\n",
        "            print(\"\\nüìä Statistics\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # Count files\n",
        "            raw_files = len(list(Path(Config.RAW_OCR_DIR).glob(\"*.txt\")))\n",
        "            clean_files = len(list(Path(Config.CLEANED_DIR).glob(\"*.txt\")))\n",
        "\n",
        "            print(f\"Raw OCR files: {raw_files}\")\n",
        "            print(f\"Cleaned files: {clean_files}\")\n",
        "            print(f\"Success rate: {processor.stats['processed']}/{processor.stats['processed'] + processor.stats['failed']}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '6':\n",
        "            # Settings\n",
        "            print(\"\\n‚öôÔ∏è Settings\")\n",
        "            print(\"-\" * 40)\n",
        "            print(f\"Current model: {Config.MODEL}\")\n",
        "            print(f\"Temperature: {Config.TEMPERATURE}\")\n",
        "            print(f\"Max tokens: {Config.MAX_TOKENS}\")\n",
        "\n",
        "            change = input(\"\\nChange model? (y/n): \")\n",
        "            if change.lower() == 'y':\n",
        "                print(\"\\nAvailable models:\")\n",
        "                print(\"[1] gpt-4o-mini (cheapest)\")\n",
        "                print(\"[2] gpt-3.5-turbo\")\n",
        "                print(\"[3] claude-3-haiku\")\n",
        "\n",
        "                model_choice = input(\"Select: \").strip()\n",
        "                if model_choice == '1':\n",
        "                    Config.MODEL = 'gpt-4o-mini'\n",
        "                elif model_choice == '2':\n",
        "                    Config.MODEL = 'gpt-3.5-turbo'\n",
        "                elif model_choice == '3':\n",
        "                    Config.MODEL = 'claude-3-haiku'\n",
        "\n",
        "                processor.llm = LLMClient()  # Reinitialize\n",
        "                print(f\"‚úÖ Model changed to: {Config.MODEL}\")\n",
        "\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '7':\n",
        "            print(\"\\nüëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice\")\n",
        "\n",
        "print(\"‚úÖ Main menu ready\")"
      ],
      "metadata": {
        "id": "Z3MsFtPV3O2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 7: Quick Start Functions\n",
        "# ============================================\n",
        "\n",
        "def quick_setup():\n",
        "    \"\"\"Setup API key ‡πÅ‡∏•‡∏∞ test connection\"\"\"\n",
        "    print(\"\\nüîß Quick Setup\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check API key\n",
        "    if not Config.OPENAI_API_KEY and not Config.ANTHROPIC_API_KEY:\n",
        "        print(\"\\n‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö API key!\")\n",
        "        print(\"\\n‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏™‡πà API key:\")\n",
        "        print(\"1. ‡πÅ‡∏Å‡πâ‡πÉ‡∏ô Config class ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô\")\n",
        "        print(\"2. ‡∏´‡∏£‡∏∑‡∏≠ set environment variable:\")\n",
        "\n",
        "        provider = input(\"\\n‡πÉ‡∏ä‡πâ [1] OpenAI ‡∏´‡∏£‡∏∑‡∏≠ [2] Anthropic? : \").strip()\n",
        "\n",
        "        if provider == '1':\n",
        "            key = input(\"Enter OpenAI API key: \").strip()\n",
        "            Config.OPENAI_API_KEY = key\n",
        "            Config.MODEL = 'gpt-4o-mini'\n",
        "        else:\n",
        "            key = input(\"Enter Anthropic API key: \").strip()\n",
        "            Config.ANTHROPIC_API_KEY = key\n",
        "            Config.MODEL = 'claude-3-haiku'\n",
        "\n",
        "    # Test connection\n",
        "    print(\"\\nüîç Testing API connection...\")\n",
        "    try:\n",
        "        client = LLMClient()\n",
        "        result = client.clean_ocr_text(\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö API\")\n",
        "        print(\"‚úÖ API connection successful!\")\n",
        "        print(f\"   Model: {Config.MODEL}\")\n",
        "        print(f\"   Test cost: ${result['cost']:.4f}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå API test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def process_single_file_quick(filename: str):\n",
        "    \"\"\"Process 1 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß\"\"\"\n",
        "    processor = OCRProcessor()\n",
        "    file_path = Path(Config.RAW_OCR_DIR) / filename\n",
        "\n",
        "    if not file_path.exists():\n",
        "        print(f\"‚ùå File not found: {filename}\")\n",
        "        return\n",
        "\n",
        "    result = processor.process_file(file_path)\n",
        "\n",
        "    if result['success']:\n",
        "        print(f\"\\n‚úÖ Success!\")\n",
        "        print(f\"   Output: {result['cleaned_path']}\")\n",
        "        print(f\"   Cost: ${result['cost']:.4f} (~{result['cost']*35:.2f} ‡∏ö‡∏≤‡∏ó)\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Failed: {result.get('error')}\")"
      ],
      "metadata": {
        "id": "YqofTKi-3SyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 8: Main Execution\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\"\"\n",
        "    ================================================================================\n",
        "                      OCR PROCESSING WITH API v1.0\n",
        "                         Automated Thai Novel OCR\n",
        "    ================================================================================\n",
        "\n",
        "    üéØ Features:\n",
        "       - Automated OCR cleaning with GPT/Claude\n",
        "       - Cost tracking & optimization\n",
        "       - Quality validation\n",
        "       - Training pairs collection\n",
        "\n",
        "    üí∞ Estimated cost:\n",
        "       - GPT-4o-mini: ~0.01 ‡∏ö‡∏≤‡∏ó/‡∏´‡∏ô‡πâ‡∏≤\n",
        "       - 100 ‡∏´‡∏ô‡πâ‡∏≤ = ~1 ‡∏ö‡∏≤‡∏ó\n",
        "       - 1,000 ‡∏´‡∏ô‡πâ‡∏≤ = ~10 ‡∏ö‡∏≤‡∏ó\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    # Quick setup ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ API key\n",
        "    if not Config.OPENAI_API_KEY and not Config.ANTHROPIC_API_KEY:\n",
        "        print(\"üìù ‡∏ï‡πâ‡∏≠‡∏á setup API key ‡∏Å‡πà‡∏≠‡∏ô\")\n",
        "        if quick_setup():\n",
        "            print(\"\\n‚úÖ Setup complete! Ready to use\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Setup failed. Please check API key\")\n",
        "            exit(1)\n",
        "\n",
        "    # Run main menu\n",
        "    print(\"\\nüöÄ Starting main menu...\")\n",
        "    main_menu()\n",
        "\n",
        "    print(\"\\nüéâ Thank you for using OCR Processor!\")"
      ],
      "metadata": {
        "id": "1kpRPoH53Vzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}