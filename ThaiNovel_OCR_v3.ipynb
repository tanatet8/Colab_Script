{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/ThaiNovel_OCR_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "                OCR PROCESSING WITH API v3.0 - FIXED VERSION\n",
        "                       Enhanced Thai Novel OCR Processor\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# üìå Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Drive (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RWrHi-Zr9pV",
        "outputId": "4b66002a-9288-4182-e037-5f9af6c29320"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Libraries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 2: API Key Loading & Configuration\n",
        "# ============================================\n",
        "\n",
        "def load_api_key():\n",
        "    \"\"\"‡πÇ‡∏´‡∏•‡∏î API key ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á .env ‡πÅ‡∏•‡∏∞ .env.txt)\"\"\"\n",
        "    base_path = \"/content/drive/MyDrive/OCR\" if IN_COLAB else \"./OCR\"\n",
        "\n",
        "    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö\n",
        "    possible_files = [\n",
        "        f\"{base_path}/openai.env\",\n",
        "        f\"{base_path}/openai.env.txt\"\n",
        "    ]\n",
        "\n",
        "    for env_path in possible_files:\n",
        "        try:\n",
        "            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ key\n",
        "            with open(env_path, \"r\") as f:\n",
        "                key = f.read().strip()\n",
        "\n",
        "            if key and key.startswith('sk-'):\n",
        "                # ‡∏ï‡∏±‡πâ‡∏á environment variable\n",
        "                os.environ[\"OPENAI_API_KEY\"] = key\n",
        "                print(f\"‚úÖ OpenAI API key loaded from: {Path(env_path).name}\")\n",
        "                return key\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Invalid API key format in {Path(env_path).name}\")\n",
        "                continue\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading {env_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏î ‡πÜ\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå API key ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\")\n",
        "    print(\"   ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏ô‡∏∂‡πà‡∏á:\")\n",
        "    for path in possible_files:\n",
        "        print(f\"   - {path}\")\n",
        "    print(\"   ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: sk-xxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "    return None\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API Processing\"\"\"\n",
        "\n",
        "    # ‡πÇ‡∏´‡∏•‡∏î API key ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
        "    OPENAI_API_KEY = load_api_key()\n",
        "    ANTHROPIC_API_KEY = \"\"  # ‡πÉ‡∏™‡πà Anthropic API key ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Claude\n",
        "\n",
        "    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model\n",
        "    MODEL = \"gpt-4o-mini\"  # ‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡∏î ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!\n",
        "    # MODEL = \"gpt-3.5-turbo\"\n",
        "    # MODEL = \"claude-3-haiku\"\n",
        "\n",
        "    # Paths (Google Drive)\n",
        "    BASE = '/content/drive/MyDrive/OCR' if IN_COLAB else './OCR'\n",
        "\n",
        "    RAW_OCR_DIR = f'{BASE}/raw_ocr'\n",
        "    CLEANED_DIR = f'{BASE}/cleaned'\n",
        "    CORPUS_DIR = f'{BASE}/final_corpus'\n",
        "    TRAINING_PAIRS_DIR = f'{BASE}/training_pairs'\n",
        "    LOGS_DIR = f'{BASE}/logs'\n",
        "\n",
        "    # Processing settings\n",
        "    MAX_PAGES_PER_BATCH = 8  # ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£\n",
        "    MAX_RETRIES = 3\n",
        "    TEMPERATURE = 0.05  # ‡∏•‡∏î‡∏•‡∏á‡πÉ‡∏´‡πâ consistent ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "    MAX_TOKENS = 8000\n",
        "    CONTEXT_OVERLAP = 100  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà overlap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á chunk\n",
        "\n",
        "    # Cost tracking (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô - ‡πÅ‡∏¢‡∏Å input/output)\n",
        "    PRICE_PER_1K_TOKENS = {\n",
        "        'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006},  # $0.15/$0.60 per 1M\n",
        "        'gpt-3.5-turbo': {'input': 0.0005, 'output': 0.0015},\n",
        "        'claude-3-haiku': {'input': 0.00025, 'output': 0.00125}\n",
        "    }\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á folders\n",
        "for folder in [Config.RAW_OCR_DIR, Config.CLEANED_DIR, Config.CORPUS_DIR,\n",
        "               Config.TRAINING_PAIRS_DIR, Config.LOGS_DIR]:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Config loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qwt1E6bsAvI",
        "outputId": "fb9ab72c-524e-4b25-d1e7-64aa168682ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded from: openai.env\n",
            "‚úÖ Config loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 3: Usage Logger\n",
        "# ============================================\n",
        "class UsageLogger:\n",
        "    \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô API ‡πÄ‡∏õ‡πá‡∏ô CSV ‡∏û‡∏£‡πâ‡∏≠‡∏° detailed tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.log_file = Path(Config.LOGS_DIR) / \"usage.csv\"\n",
        "        self.filename_map_file = Path(Config.LOGS_DIR) / \"filename_mapping.json\"\n",
        "        self._init_csv()\n",
        "\n",
        "    def _init_csv(self):\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ\"\"\"\n",
        "        if not self.log_file.exists():\n",
        "            columns = [\n",
        "                'timestamp', 'original_filename', 'clean_filename', 'pages_count',\n",
        "                'model', 'input_tokens', 'output_tokens', 'total_tokens',\n",
        "                'cost_usd', 'cost_thb', 'processing_time_sec', 'retry_count',\n",
        "                'validation_status'\n",
        "            ]\n",
        "            df = pd.DataFrame(columns=columns)\n",
        "            df.to_csv(self.log_file, index=False, encoding='utf-8')\n",
        "\n",
        "    def log_usage(self, original_filename: str, clean_filename: str, pages_count: int,\n",
        "                  model: str, input_tokens: int, output_tokens: int, cost_usd: float,\n",
        "                  processing_time: float = 0, retry_count: int = 0,\n",
        "                  validation_status: str = 'PASS'):\n",
        "        \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\"\"\n",
        "\n",
        "        new_row = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'original_filename': original_filename,\n",
        "            'clean_filename': clean_filename,\n",
        "            'pages_count': pages_count,\n",
        "            'model': model,\n",
        "            'input_tokens': input_tokens,\n",
        "            'output_tokens': output_tokens,\n",
        "            'total_tokens': input_tokens + output_tokens,\n",
        "            'cost_usd': cost_usd,\n",
        "            'cost_thb': cost_usd * 35,  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì\n",
        "            'processing_time_sec': processing_time,\n",
        "            'retry_count': retry_count,\n",
        "            'validation_status': validation_status\n",
        "        }\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á CSV\n",
        "        df = pd.DataFrame([new_row])\n",
        "        df.to_csv(self.log_file, mode='a', header=False, index=False, encoding='utf-8')\n",
        "\n",
        "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å filename mapping\n",
        "        self._save_filename_mapping(original_filename, clean_filename)\n",
        "\n",
        "    def _save_filename_mapping(self, original: str, cleaned: str):\n",
        "        \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà\"\"\"\n",
        "        mapping = {}\n",
        "        if self.filename_map_file.exists():\n",
        "            try:\n",
        "                with open(self.filename_map_file, 'r', encoding='utf-8') as f:\n",
        "                    mapping = json.load(f)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        mapping[cleaned] = {\n",
        "            'original': original,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        with open(self.filename_map_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def get_summary(self) -> Dict:\n",
        "        \"\"\"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(self.log_file, encoding='utf-8')\n",
        "\n",
        "            if df.empty:\n",
        "                return {'total_files': 0, 'total_cost_usd': 0, 'total_tokens': 0}\n",
        "\n",
        "            return {\n",
        "                'total_files': len(df),\n",
        "                'total_pages': df['pages_count'].sum(),\n",
        "                'total_tokens': df['total_tokens'].sum(),\n",
        "                'input_tokens': df['input_tokens'].sum(),\n",
        "                'output_tokens': df['output_tokens'].sum(),\n",
        "                'total_cost_usd': df['cost_usd'].sum(),\n",
        "                'total_cost_thb': df['cost_thb'].sum(),\n",
        "                'avg_cost_per_page': df['cost_usd'].sum() / df['pages_count'].sum() if df['pages_count'].sum() > 0 else 0,\n",
        "                'avg_processing_time': df['processing_time_sec'].mean(),\n",
        "                'most_used_model': df['model'].mode()[0] if len(df) > 0 else 'N/A',\n",
        "                'validation_stats': df['validation_status'].value_counts().to_dict() if 'validation_status' in df.columns else {}\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error reading usage log: {e}\")\n",
        "            return {'total_files': 0, 'total_cost_usd': 0, 'total_tokens': 0}\n",
        "\n",
        "print(\"‚úÖ Enhanced Usage Logger ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZdskcZ9sDdS",
        "outputId": "0424bfe3-567b-4cc1-d509-c8df97025905"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced Usage Logger ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 4: Thai Text Utilities\n",
        "# ============================================\n",
        "class ThaiTextUtils:\n",
        "    \"\"\"Utilities ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_unicode(text: str) -> str:\n",
        "        \"\"\"Normalize Unicode characters\"\"\"\n",
        "        # ‡∏Å‡∏≥‡∏à‡∏±‡∏î zero-width characters\n",
        "        text = re.sub(r'[\\u200b\\ufeff\\u00a0]', '', text)\n",
        "        # Normalize Unicode form\n",
        "        text = unicodedata.normalize('NFC', text)\n",
        "        # ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÑ‡∏ó‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "        text = re.sub(r'([‡∏Å-‡∏Æ])([‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π])([‡πà‡πâ‡πä‡πã])', r'\\1\\3\\2', text)\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def count_thai_quotes(text: str) -> Dict[str, int]:\n",
        "        \"\"\"‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô '‡∏Ñ‡∏π‡πà' ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î (pairs)\"\"\"\n",
        "        eng_double = text.count('\"') // 2\n",
        "        eng_single = text.count(\"'\") // 2\n",
        "        thai_double = (text.count('\\u201c') + text.count('\\u201d')) // 2  # ‚Äú ‚Äù\n",
        "        thai_single = (text.count('\\u2018') + text.count('\\u2019')) // 2  # ‚Äò ‚Äô\n",
        "        return {\n",
        "            'english_double': eng_double,\n",
        "            'english_single': eng_single,\n",
        "            'thai_double': thai_double,\n",
        "            'thai_single': thai_single,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_yamok_spacing(text: str) -> str:\n",
        "        \"\"\"‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ‡πÜ (‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å)\"\"\"\n",
        "        text = re.sub(r'\\s+‡πÜ\\s+', '‡πÜ ', text)\n",
        "        text = re.sub(r'\\s+‡πÜ(?=\\s|$)', '‡πÜ', text)\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_word_breaks(text: str) -> List[str]:\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô '‡∏°‡∏≤ ‡∏Å‡∏≥‡∏•‡∏±‡∏á')\"\"\"\n",
        "        broken_patterns = [\n",
        "            r'([‡∏Å-‡∏Æ])\\s+([‡∏Å-‡∏Æ‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πç‡πå‡πà‡πâ‡πä‡πã])',      # ‡∏û‡∏¢‡∏±‡∏ç‡∏ä‡∏ô‡∏∞ + ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ + ‡∏™‡∏£‡∏∞/‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå\n",
        "            r'([‡∏Å-‡∏Æ‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π])\\s+([‡∏Å-‡∏Æ]{1}(?![‡∏Å-‡∏Æ]))', # ‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å\n",
        "        ]\n",
        "        issues = []\n",
        "        for pattern in broken_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            issues.extend([f\"{m[0]} {m[1]}\" for m in matches])\n",
        "        return list(set(issues))\n",
        "\n",
        "print(\"‚úÖ Thai Text Utilities ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQVDdGTsRzC",
        "outputId": "bd3f980c-8af7-4b9e-a2ad-41ab684f1f80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Thai Text Utilities ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 5: Multi-page Parser (XML Enhanced)\n",
        "# ============================================\n",
        "class MultiPageParser:\n",
        "    \"\"\"‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏° XML processing\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_multipage_file(content: str) -> List[Dict]:\n",
        "        \"\"\"‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\"\"\"\n",
        "        pages = []\n",
        "        page_sections = re.split(r'--- PAGE:\\s*(\\d+)\\s*---', content)\n",
        "        if len(page_sections) > 1:\n",
        "            for i in range(1, len(page_sections), 2):\n",
        "                if i + 1 < len(page_sections):\n",
        "                    page_num = int(page_sections[i])\n",
        "                    page_content = page_sections[i + 1]\n",
        "                    raw_text = \"\"\n",
        "                    cleaned_text = \"\"\n",
        "                    raw_match = re.search(\n",
        "                        r'--- RAW ---(.*?)(?=--- CLEANED ---|--- PAGE:|$)',\n",
        "                        page_content, re.DOTALL\n",
        "                    )\n",
        "                    if raw_match:\n",
        "                        raw_text = raw_match.group(1)  # keep as-is\n",
        "                    else:\n",
        "                        raw_text = page_content\n",
        "                    cleaned_match = re.search(\n",
        "                        r'--- CLEANED ---(.*?)(?=--- PAGE:|$)',\n",
        "                        page_content, re.DOTALL\n",
        "                    )\n",
        "                    if cleaned_match:\n",
        "                        cleaned_text = cleaned_match.group(1).strip()\n",
        "                        cleaned_text = ThaiTextUtils.normalize_unicode(cleaned_text)\n",
        "                    pages.append({\n",
        "                        'page_num': page_num,\n",
        "                        'raw_text': raw_text,\n",
        "                        'cleaned_text': cleaned_text\n",
        "                    })\n",
        "        return pages\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_xml_result(xml_content: str) -> List[Dict]:\n",
        "        \"\"\"‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ XML tags <page id=\"X\">\"\"\"\n",
        "        pages = []\n",
        "        page_matches = re.findall(\n",
        "            r'<page\\s+id=[\\\"\\'](\\d+)[\\\"\\']>(.*?)</page>',\n",
        "            xml_content, re.DOTALL | re.IGNORECASE\n",
        "        )\n",
        "        for page_num_str, content in page_matches:\n",
        "            content = ThaiTextUtils.normalize_unicode(content.strip())\n",
        "            pages.append({\n",
        "                'page_num': int(page_num_str),\n",
        "                'cleaned_text': content\n",
        "            })\n",
        "        return pages\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_metadata(content: str) -> Dict:\n",
        "        \"\"\"‡πÅ‡∏¢‡∏Å metadata ‡∏à‡∏≤‡∏Å header ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå\"\"\"\n",
        "        metadata = {}\n",
        "        patterns = {\n",
        "            'book_title': r'### üìò ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠.*?:\\s*(.*)',\n",
        "            'chapter': r'### üßæ Chapter:\\s*(.*)',\n",
        "            'sub_chapter': r'### üîñ Sub-Chapter:\\s*(.*)',\n",
        "            'format': r'### üìÇ Format:\\s*(.*)',\n",
        "            'purpose': r'### üß† Purpose:\\s*(.*)'\n",
        "        }\n",
        "        for key, pattern in patterns.items():\n",
        "            match = re.search(pattern, content)\n",
        "            if match:\n",
        "                metadata[key] = match.group(1).strip()\n",
        "        return metadata\n",
        "\n",
        "print(\"‚úÖ Multi-page Parser ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HTXIqNCsUvm",
        "outputId": "4b94c5db-2a9b-4155-ed79-d55005681d4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Multi-page Parser ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 6: Enhanced LLM Client with Metadata Analysis\n",
        "# ============================================\n",
        "class LLMClient:\n",
        "    \"\"\"Universal LLM Client with enhanced error handling ‡πÅ‡∏•‡∏∞ metadata analysis\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = Config.MODEL\n",
        "        self.client = None\n",
        "        self.total_tokens = 0\n",
        "        self.total_cost = 0\n",
        "        self.usage_logger = UsageLogger()\n",
        "        if 'gpt' in self.model:\n",
        "            self._init_openai()\n",
        "        elif 'claude' in self.model:\n",
        "            self._init_anthropic()\n",
        "\n",
        "    def _init_openai(self):\n",
        "        \"\"\"Initialize OpenAI client (SDK v1.0+)\"\"\"\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            if not Config.OPENAI_API_KEY:\n",
        "                raise ValueError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö OpenAI API key! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå key\")\n",
        "            self.client = OpenAI(api_key=Config.OPENAI_API_KEY)\n",
        "            print(f\"‚úÖ OpenAI client ready (Model: {self.model})\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install --upgrade openai>=1.0.0\")\n",
        "            raise\n",
        "\n",
        "    def _init_anthropic(self):\n",
        "        \"\"\"Initialize Anthropic client\"\"\"\n",
        "        try:\n",
        "            import anthropic\n",
        "            api_key = Config.ANTHROPIC_API_KEY or os.getenv('ANTHROPIC_API_KEY')\n",
        "            if not api_key:\n",
        "                raise ValueError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Anthropic API key!\")\n",
        "            self.client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(f\"‚úÖ Anthropic client ready (Model: {self.model})\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install anthropic\")\n",
        "            raise\n",
        "\n",
        "    def _create_multipage_xml_prompt(self, pages: List[Dict]) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç OCR ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤\"\"\"\n",
        "        page_texts = [f\"--- ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {p['page_num']} ---\\n{p['raw_text']}\" for p in pages]\n",
        "        combined_text = \"\\n\\n\".join(page_texts)\n",
        "        return f\"\"\"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR ‡∏à‡∏≤‡∏Å‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n",
        "\n",
        "üö® ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î):\n",
        "1. ‡πÅ‡∏Å‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ typo ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î\n",
        "2. ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢/‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô \"‡∏°‡∏≤ ‡∏Å‡∏≥‡∏•‡∏±‡∏á\" ‚Üí \"‡∏°‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á\")\n",
        "3. ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏±‡∏ß ‡∏Å ‡∏≠ ‡∏¢ ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß ‡πÜ)\n",
        "4. ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÉ‡∏ô \"...\")\n",
        "5. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡∏°‡πà\n",
        "6. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ\n",
        "7. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢\n",
        "8. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ/‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà\n",
        "9. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£/‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà\n",
        "10. ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "\n",
        "üìã ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):\n",
        "‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:\n",
        "\n",
        "<page num=\"1\">\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏ô‡πâ‡∏≤ 1\n",
        "</page>\n",
        "<page num=\"2\">\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏ô‡πâ‡∏≤ 2\n",
        "</page>\n",
        "\n",
        "‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ ‡∏Ñ‡∏≥‡∏ô‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î\n",
        "\n",
        "üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR:\n",
        "{combined_text}\n",
        "\n",
        "üîÑ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\"\"\"\n",
        "\n",
        "    def _create_single_page_prompt(self, text: str) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç OCR ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\"\"\"\n",
        "        return f\"\"\"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR ‡∏à‡∏≤‡∏Å‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n",
        "\n",
        "üö® ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î):\n",
        "1. ‡πÅ‡∏Å‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ typo ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î\n",
        "2. ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢/‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô \"‡∏°‡∏≤ ‡∏Å‡∏≥‡∏•‡∏±‡∏á\" ‚Üí \"‡∏°‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á\")\n",
        "3. ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢\n",
        "4. ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÉ‡∏ô \"...\")\n",
        "5. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡∏°‡πà\n",
        "6. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ\n",
        "7. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢\n",
        "8. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£/‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà\n",
        "\n",
        "‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR:\n",
        "{text}\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß:\"\"\"\n",
        "\n",
        "    def _create_metadata_analysis_prompt(self, page_text: str, page_num: int, book_info: Dict) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå metadata ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤\"\"\"\n",
        "        return f\"\"\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î metadata ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "\n",
        "üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠:\n",
        "- ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á: {book_info.get('book_title', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}\n",
        "- ‡∏ö‡∏ó: {book_info.get('chapter', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}\n",
        "- ‡∏´‡∏ô‡πâ‡∏≤: {page_num}\n",
        "\n",
        "üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤:\n",
        "{page_text[:2500]}  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 2500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î token\n",
        "\n",
        "üéØ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ (‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà comment ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢):\n",
        "{{\n",
        "  \"tone\": [\"‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡πÄ‡∏ä‡πà‡∏ô dramatic, tense, romantic\"],\n",
        "  \"tags\": [\"‡πÅ‡∏ó‡πá‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ general\"],\n",
        "  \"characters\": [\"‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\"],\n",
        "  \"places\": [\"‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ\"],\n",
        "  \"objects\": [\"‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ\"],\n",
        "  \"dialogue_pairs\": ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏π‡πà‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤,\n",
        "  \"char_count\": ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì,\n",
        "  \"word_count\": ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì,\n",
        "  \"paragraph_count\": ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤,\n",
        "  \"style_notes\": \"‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ (1-2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)\",\n",
        "  \"summary\": \"‡∏™‡∏£‡∏∏‡∏õ 1 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 25 ‡∏Ñ‡∏≥\",\n",
        "  \"anomalies\": \"‡∏™‡∏¥‡πà‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏´‡∏£‡∏∑‡∏≠ null\",\n",
        "  \"confidence\": 0.85\n",
        "}}\n",
        "\n",
        "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\"\"\"\n",
        "\n",
        "    def _call_api_with_retry(self, prompt: str, filename: str, retry_count: int) -> Dict:\n",
        "        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏û‡∏£‡πâ‡∏≠‡∏° retry mechanism\"\"\"\n",
        "        for attempt in range(Config.MAX_RETRIES + 1):\n",
        "            try:\n",
        "                if 'gpt' in self.model:\n",
        "                    return self._call_openai(prompt)\n",
        "                else:\n",
        "                    return self._call_anthropic(prompt)\n",
        "            except Exception as e:\n",
        "                error_type = type(e).__name__\n",
        "                print(f\"   ‚ùå API error (attempt {attempt + 1}): {error_type}\")\n",
        "                if attempt < Config.MAX_RETRIES:\n",
        "                    base_delay = 2 ** attempt\n",
        "                    jitter = random.uniform(0.5, 1.5)\n",
        "                    delay = base_delay * jitter\n",
        "                    print(f\"   ‚è± Retrying in {delay:.1f}s...\")\n",
        "                    time.sleep(delay)\n",
        "                else:\n",
        "                    raise Exception(f\"API failed after {Config.MAX_RETRIES + 1} attempts: {e}\")\n",
        "\n",
        "    def _call_openai(self, prompt: str) -> Dict:\n",
        "        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å OpenAI API\"\"\"\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=Config.TEMPERATURE,\n",
        "            max_tokens=Config.MAX_TOKENS\n",
        "        )\n",
        "        cleaned_text = response.choices[0].message.content\n",
        "        total_tokens = response.usage.total_tokens\n",
        "        input_tokens = getattr(response.usage, 'prompt_tokens', total_tokens // 2)\n",
        "        output_tokens = getattr(response.usage, 'completion_tokens', total_tokens // 2)\n",
        "        prices = Config.PRICE_PER_1K_TOKENS.get(self.model, {'input': 0.0005, 'output': 0.0015})\n",
        "        input_cost = (input_tokens / 1000) * prices['input']\n",
        "        output_cost = (output_tokens / 1000) * prices['output']\n",
        "        total_cost = input_cost + output_cost\n",
        "        return {\n",
        "            'cleaned_text': cleaned_text,\n",
        "            'tokens_used': total_tokens,\n",
        "            'input_tokens': input_tokens,\n",
        "            'output_tokens': output_tokens,\n",
        "            'cost': total_cost,\n",
        "            'model': self.model\n",
        "        }\n",
        "\n",
        "    def _call_anthropic(self, prompt: str) -> Dict:\n",
        "        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Anthropic API\"\"\"\n",
        "        response = self.client.messages.create(\n",
        "            model=self.model,\n",
        "            max_tokens=Config.MAX_TOKENS,\n",
        "            temperature=Config.TEMPERATURE,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        cleaned_text = response.content[0].text\n",
        "        input_tokens = getattr(response.usage, 'input_tokens', 0)\n",
        "        output_tokens = getattr(response.usage, 'output_tokens', 0)\n",
        "        total_tokens = input_tokens + output_tokens\n",
        "        prices = Config.PRICE_PER_1K_TOKENS.get(self.model, {'input': 0.00025, 'output': 0.00125})\n",
        "        input_cost = (input_tokens / 1000) * prices['input']\n",
        "        output_cost = (output_tokens / 1000) * prices['output']\n",
        "        total_cost = input_cost + output_cost\n",
        "        return {\n",
        "            'cleaned_text': cleaned_text,\n",
        "            'tokens_used': total_tokens,\n",
        "            'input_tokens': input_tokens,\n",
        "            'output_tokens': output_tokens,\n",
        "            'cost': total_cost,\n",
        "            'model': self.model\n",
        "        }\n",
        "\n",
        "    def analyze_page_metadata(self, page_text: str, page_num: int, book_info: Dict, filename: str = \"\") -> Dict:\n",
        "        \"\"\"\n",
        "        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå metadata ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "\n",
        "        Args:\n",
        "            page_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
        "            page_num: ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤\n",
        "            book_info: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ (title, chapter, etc.)\n",
        "            filename: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging\n",
        "\n",
        "        Returns:\n",
        "            Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ metadata ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt\n",
        "        prompt = self._create_metadata_analysis_prompt(page_text, page_num, book_info)\n",
        "\n",
        "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API\n",
        "        result = self._call_api_with_retry(prompt, f\"{filename}_meta_p{page_num}\", 0)\n",
        "\n",
        "        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° parse JSON ‡∏à‡∏≤‡∏Å response\n",
        "        try:\n",
        "            import json\n",
        "            # ‡∏•‡∏ö markdown code block ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
        "            response_text = result['cleaned_text']\n",
        "            response_text = response_text.replace('```json\\n', '').replace('\\n```', '')\n",
        "            response_text = response_text.replace('```', '')\n",
        "\n",
        "            metadata = json.loads(response_text)\n",
        "\n",
        "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö required fields\n",
        "            required_fields = ['tone', 'tags', 'characters', 'places', 'objects',\n",
        "                             'dialogue_pairs', 'style_notes', 'summary', 'confidence']\n",
        "            for field in required_fields:\n",
        "                if field not in metadata:\n",
        "                    metadata[field] = [] if field in ['tone', 'tags', 'characters', 'places', 'objects'] else \"\"\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"   ‚ö†Ô∏è Failed to parse metadata for page {page_num}: {e}\")\n",
        "            # Return default metadata\n",
        "            metadata = {\n",
        "                'tone': ['unknown'],\n",
        "                'tags': ['general'],\n",
        "                'characters': [],\n",
        "                'places': [],\n",
        "                'objects': [],\n",
        "                'dialogue_pairs': 0,\n",
        "                'char_count': len(page_text),\n",
        "                'word_count': len(page_text.split()),\n",
        "                'paragraph_count': page_text.count('\\n\\n') + 1,\n",
        "                'style_notes': '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ',\n",
        "                'summary': '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ',\n",
        "                'anomalies': 'JSON parsing failed',\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\n",
        "        processing_time = time.time() - start_time\n",
        "        metadata['processing_time'] = processing_time\n",
        "        metadata['tokens_used'] = result.get('tokens_used', 0)\n",
        "        metadata['cost'] = result.get('cost', 0)\n",
        "\n",
        "        # Log usage\n",
        "        self.usage_logger.log_usage(\n",
        "            original_filename=filename,\n",
        "            clean_filename=f\"{filename}_metadata_p{page_num}\",\n",
        "            pages_count=1,\n",
        "            model=self.model,\n",
        "            input_tokens=result.get('input_tokens', 0),\n",
        "            output_tokens=result.get('output_tokens', 0),\n",
        "            cost_usd=result.get('cost', 0),\n",
        "            processing_time=processing_time,\n",
        "            retry_count=0,\n",
        "            validation_status='META_ANALYSIS'\n",
        "        )\n",
        "\n",
        "        self.total_tokens += result.get('tokens_used', 0)\n",
        "        self.total_cost += result.get('cost', 0)\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def clean_multipage_ocr(self, pages: List[Dict], filename: str = \"\") -> Dict:\n",
        "        \"\"\"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç OCR ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô\"\"\"\n",
        "        start_time = time.time()\n",
        "        retry_count = 0\n",
        "        prompt = self._create_multipage_xml_prompt(pages)\n",
        "        result = self._call_api_with_retry(prompt, filename, retry_count)\n",
        "        cleaned_pages = self._parse_xml_result(result['cleaned_text'], pages)\n",
        "        processing_time = time.time() - start_time\n",
        "        self.total_tokens += result['tokens_used']\n",
        "        self.total_cost += result['cost']\n",
        "        self.usage_logger.log_usage(\n",
        "            original_filename=filename,\n",
        "            clean_filename=f\"{filename}_processed\",\n",
        "            pages_count=len(pages),\n",
        "            model=self.model,\n",
        "            input_tokens=result.get('input_tokens', 0),\n",
        "            output_tokens=result.get('output_tokens', 0),\n",
        "            cost_usd=result['cost'],\n",
        "            processing_time=processing_time,\n",
        "            retry_count=retry_count\n",
        "        )\n",
        "        return {\n",
        "            'cleaned_pages': cleaned_pages,\n",
        "            'tokens_used': result['tokens_used'],\n",
        "            'cost': result['cost'],\n",
        "            'processing_time': processing_time,\n",
        "            'input_tokens': result.get('input_tokens', 0),\n",
        "            'output_tokens': result.get('output_tokens', 0)\n",
        "        }\n",
        "\n",
        "    def clean_ocr_text(self, text: str, filename: str = \"\", page_num: int = 1) -> Dict:\n",
        "        \"\"\"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç OCR ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\"\"\"\n",
        "        start_time = time.time()\n",
        "        retry_count = 0\n",
        "        prompt = self._create_single_page_prompt(text)\n",
        "        result = self._call_api_with_retry(prompt, filename, retry_count)\n",
        "        processing_time = time.time() - start_time\n",
        "        self.total_tokens += result['tokens_used']\n",
        "        self.total_cost += result['cost']\n",
        "        self.usage_logger.log_usage(\n",
        "            original_filename=filename,\n",
        "            clean_filename=f\"{filename}_processed_p{page_num}\",\n",
        "            pages_count=1,\n",
        "            model=self.model,\n",
        "            input_tokens=result.get('input_tokens', 0),\n",
        "            output_tokens=result.get('output_tokens', 0),\n",
        "            cost_usd=result['cost'],\n",
        "            processing_time=processing_time,\n",
        "            retry_count=retry_count\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    def _parse_xml_result(self, cleaned_text: str, original_pages: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å XML tags\"\"\"\n",
        "        cleaned_pages = []\n",
        "        xml_pages = MultiPageParser.parse_xml_result(cleaned_text)\n",
        "        xml_dict = {p['page_num']: p['cleaned_text'] for p in xml_pages}\n",
        "        for original_page in original_pages:\n",
        "            page_num = original_page['page_num']\n",
        "            cleaned_content = xml_dict.get(page_num, \"\")\n",
        "            if not cleaned_content:\n",
        "                print(f\"   ‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ {page_num}\")\n",
        "                cleaned_content = original_page['raw_text']\n",
        "            cleaned_pages.append({\n",
        "                'page_num': page_num,\n",
        "                'raw_text': original_page['raw_text'],\n",
        "                'cleaned_text': cleaned_content\n",
        "            })\n",
        "        return cleaned_pages\n",
        "\n",
        "    def generate_text(self, prompt: str, filename: str = \"\", system: str = \"You are a helpful assistant.\") -> Dict:\n",
        "        \"\"\"Generate text ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\"\"\"\n",
        "        start_time = time.time()\n",
        "        retry_count = 0\n",
        "        for attempt in range(Config.MAX_RETRIES + 1):\n",
        "            try:\n",
        "                if 'gpt' in self.model:\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=self.model,\n",
        "                        messages=[{\"role\": \"system\", \"content\": system},\n",
        "                                  {\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=Config.TEMPERATURE,\n",
        "                        max_tokens=Config.MAX_TOKENS\n",
        "                    )\n",
        "                    result_text = response.choices[0].message.content\n",
        "                    total_tokens = response.usage.total_tokens\n",
        "                    input_tokens = getattr(response.usage, 'prompt_tokens', total_tokens // 2)\n",
        "                    output_tokens = getattr(response.usage, 'completion_tokens', total_tokens // 2)\n",
        "                else:\n",
        "                    response = self.client.messages.create(\n",
        "                        model=self.model,\n",
        "                        max_tokens=Config.MAX_TOKENS,\n",
        "                        temperature=Config.TEMPERATURE,\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                    )\n",
        "                    result_text = response.content[0].text\n",
        "                    input_tokens = getattr(response.usage, 'input_tokens', 0)\n",
        "                    output_tokens = getattr(response.usage, 'output_tokens', 0)\n",
        "                    total_tokens = input_tokens + output_tokens\n",
        "\n",
        "                prices = Config.PRICE_PER_1K_TOKENS.get(self.model, {'input': 0.0005, 'output': 0.0015})\n",
        "                input_cost = (input_tokens / 1000) * prices['input']\n",
        "                output_cost = (output_tokens / 1000) * prices['output']\n",
        "                total_cost = input_cost + output_cost\n",
        "\n",
        "                processing_time = time.time() - start_time\n",
        "                self.usage_logger.log_usage(\n",
        "                    original_filename=filename,\n",
        "                    clean_filename=f\"{filename}_generated\",\n",
        "                    pages_count=1,\n",
        "                    model=self.model,\n",
        "                    input_tokens=input_tokens,\n",
        "                    output_tokens=output_tokens,\n",
        "                    cost_usd=total_cost,\n",
        "                    processing_time=processing_time,\n",
        "                    retry_count=retry_count\n",
        "                )\n",
        "                self.total_tokens += total_tokens\n",
        "                self.total_cost += total_cost\n",
        "                return {\n",
        "                    'text': result_text,\n",
        "                    'tokens_used': total_tokens,\n",
        "                    'input_tokens': input_tokens,\n",
        "                    'output_tokens': output_tokens,\n",
        "                    'cost': total_cost,\n",
        "                    'processing_time': processing_time\n",
        "                }\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if attempt < Config.MAX_RETRIES:\n",
        "                    base_delay = 2 ** attempt\n",
        "                    jitter = random.uniform(0.5, 1.5)\n",
        "                    delay = base_delay * jitter\n",
        "                    print(f\"   Retry {attempt + 1}/{Config.MAX_RETRIES} in {delay:.1f}s...\")\n",
        "                    time.sleep(delay)\n",
        "                else:\n",
        "                    raise Exception(f\"Generate text failed: {e}\")\n",
        "\n",
        "    def get_usage_summary(self) -> Dict:\n",
        "        \"\"\"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô API\"\"\"\n",
        "        file_summary = self.usage_logger.get_summary()\n",
        "        return {\n",
        "            'total_tokens': self.total_tokens,\n",
        "            'total_cost_usd': self.total_cost,\n",
        "            'total_cost_thb': self.total_cost * 35,\n",
        "            'pages_processed': self.total_tokens // 500,\n",
        "            'session_files': file_summary.get('total_files', 0),\n",
        "            'session_pages': file_summary.get('total_pages', 0),\n",
        "            'avg_cost_per_page': file_summary.get('avg_cost_per_page', 0),\n",
        "            'detailed_stats': file_summary\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Enhanced LLM Client with Metadata Analysis ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQocnXzKWfVC",
        "outputId": "ec240266-9c10-42c3-f7d2-725d36287eb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced LLM Client with Metadata Analysis ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 7: Enhanced Text Chunker\n",
        "# ============================================\n",
        "class EnhancedChunker:\n",
        "    \"\"\"‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ context overlap ‡πÅ‡∏•‡∏∞ smart boundary detection\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def smart_chunk_text(text: str, max_chars: int = 2500) -> List[str]:\n",
        "        \"\"\"‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î‡∏î‡πâ‡∏ß‡∏¢ context overlap\"\"\"\n",
        "        sentence_endings = ['.', '!', '?', '‚Ä¶', '\"', '\"']\n",
        "        paragraph_break = '\\n\\n'\n",
        "\n",
        "        chunks = []\n",
        "        current_pos = 0\n",
        "        half = max_chars // 2\n",
        "\n",
        "        while current_pos < len(text):\n",
        "            chunk_end = min(current_pos + max_chars, len(text))\n",
        "            if chunk_end < len(text):\n",
        "                para_pos = text.rfind(paragraph_break, current_pos, chunk_end)\n",
        "                if para_pos > current_pos + half:\n",
        "                    chunk_end = para_pos + len(paragraph_break)\n",
        "                else:\n",
        "                    best_pos = -1\n",
        "                    for ending in sentence_endings:\n",
        "                        pos = text.rfind(ending, current_pos + half, chunk_end)\n",
        "                        if pos > best_pos:\n",
        "                            best_pos = pos + len(ending)\n",
        "                    if best_pos > current_pos:\n",
        "                        chunk_end = best_pos\n",
        "\n",
        "            chunk = text[current_pos:chunk_end]\n",
        "\n",
        "            if chunks and Config.CONTEXT_OVERLAP > 0:\n",
        "                overlap_start = max(0, current_pos - Config.CONTEXT_OVERLAP)\n",
        "                overlap_text = text[overlap_start:current_pos]\n",
        "                if overlap_text:\n",
        "                    chunk = f\"[‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {overlap_text[-50:]}...]\\n\\n{chunk}\"\n",
        "\n",
        "            chunks.append(chunk)\n",
        "            prev_pos = current_pos\n",
        "            current_pos = chunk_end\n",
        "            if current_pos == prev_pos and current_pos < len(text):\n",
        "                current_pos += 1\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_chunks_with_dedup(chunks: List[str]) -> str:\n",
        "        \"\"\"‡∏£‡∏ß‡∏° chunks ‡πÇ‡∏î‡∏¢‡∏•‡∏ö overlap ‡∏ã‡πâ‡∏≥\"\"\"\n",
        "        if not chunks:\n",
        "            return \"\"\n",
        "        if len(chunks) == 1:\n",
        "            return chunks[0]\n",
        "\n",
        "        result = chunks[0]\n",
        "        for i in range(1, len(chunks)):\n",
        "            chunk = chunks[i]\n",
        "            if chunk.startswith('[‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:'):\n",
        "                cut = chunk.find(']\\n\\n')\n",
        "                if cut != -1:\n",
        "                    chunk = chunk[cut + 4:]\n",
        "            result += chunk\n",
        "        return result\n",
        "\n",
        "print(\"‚úÖ Enhanced Chunker ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w7mgSmjtWbs",
        "outputId": "c7631191-8c1c-4236-fb08-e288e5271dca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced Chunker ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 8: Thai-specific Validator\n",
        "# ============================================\n",
        "class ThaiValidator:\n",
        "    \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_thai_text(raw_text: str, cleaned_text: str, filename: str = \"\") -> Dict:\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\"\"\n",
        "        issues, warnings = [], []\n",
        "\n",
        "        # 1) ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß\n",
        "        len_ratio = len(cleaned_text) / len(raw_text) if raw_text else 0\n",
        "        len_change = (len(cleaned_text) - len(raw_text)) / len(raw_text) * 100 if raw_text else 0\n",
        "        if len_ratio < 0.7:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏°‡∏≤‡∏Å {abs(len_change):.1f}% (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)\")\n",
        "        elif len_ratio > 1.3:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å {len_change:.1f}% (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)\")\n",
        "        elif len_ratio < 0.85:\n",
        "            warnings.append(f\"üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á {abs(len_change):.1f}%\")\n",
        "        elif len_ratio > 1.15:\n",
        "            warnings.append(f\"üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô {len_change:.1f}%\")\n",
        "\n",
        "        # 2) ‡∏≠‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®\n",
        "        raw_quotes = ThaiTextUtils.count_thai_quotes(raw_text)\n",
        "        clean_quotes = ThaiTextUtils.count_thai_quotes(cleaned_text)\n",
        "        total_raw = sum(raw_quotes.values())\n",
        "        total_clean = sum(clean_quotes.values())\n",
        "        if abs(total_raw - total_clean) > 3:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏≠‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡∏Å ({total_raw} ‚Üí {total_clean})\")\n",
        "        elif abs(total_raw - total_clean) > 1:\n",
        "            warnings.append(f\"üìù ‡∏≠‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ({total_raw} ‚Üí {total_clean})\")\n",
        "\n",
        "        # 3) ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å\n",
        "        raw_yamok = raw_text.count('‡πÜ')\n",
        "        clean_yamok = cleaned_text.count('‡πÜ')\n",
        "        if abs(raw_yamok - clean_yamok) > 2:\n",
        "            warnings.append(f\"üìù ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å (‡πÜ) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ({raw_yamok} ‚Üí {clean_yamok})\")\n",
        "\n",
        "        # 4) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "        raw_numbers = len(re.findall(r'\\d+', raw_text))\n",
        "        clean_numbers = len(re.findall(r'\\d+', cleaned_text))\n",
        "        if abs(raw_numbers - clean_numbers) > 3:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡∏Å ({raw_numbers} ‚Üí {clean_numbers})\")\n",
        "        elif abs(raw_numbers - clean_numbers) > 1:\n",
        "            warnings.append(f\"üìù ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ({raw_numbers} ‚Üí {clean_numbers})\")\n",
        "\n",
        "        # 5) ‡∏Ñ‡∏≥‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å\n",
        "        broken_words_raw = ThaiTextUtils.detect_word_breaks(raw_text)\n",
        "        broken_words_clean = ThaiTextUtils.detect_word_breaks(cleaned_text)\n",
        "        if len(broken_words_clean) > len(broken_words_raw) * 0.5:\n",
        "            warnings.append(f\"üìù ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å: {broken_words_clean[:3]}\")\n",
        "\n",
        "        # Score & Status\n",
        "        score = 1.0 - 0.3*len(issues) - 0.1*len(warnings)\n",
        "        score = max(0, min(1, score))\n",
        "        status = 'PASS'\n",
        "        if issues:\n",
        "            status = 'FAIL'\n",
        "        elif warnings:\n",
        "            status = 'WARNING'\n",
        "\n",
        "        return {\n",
        "            'filename': filename,\n",
        "            'status': status,\n",
        "            'score': score,\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'stats': {\n",
        "                'length_change': f\"{len_change:+.1f}%\",\n",
        "                'quotes_change': f\"{total_raw} ‚Üí {total_clean}\",\n",
        "                'yamok_change': f\"{raw_yamok} ‚Üí {clean_yamok}\",\n",
        "                'numbers_change': f\"{raw_numbers} ‚Üí {clean_numbers}\",\n",
        "                'broken_words_remaining': len(broken_words_clean)\n",
        "            },\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Thai Validator ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM8TqqHtgKA",
        "outputId": "950ccf72-4a88-4524-f961-c4c095e8a710"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Thai Validator ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 9: Enhanced OCR Processor with Metadata\n",
        "# ============================================\n",
        "class OCRProcessor:\n",
        "    \"\"\"Main processor with metadata analysis ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤\"\"\"\n",
        "\n",
        "    def __init__(self, analyze_metadata: bool = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            analyze_metadata: ‡∏ñ‡πâ‡∏≤ True ‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå metadata ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤ (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°)\n",
        "        \"\"\"\n",
        "        self.llm = LLMClient()\n",
        "        self.chunker = EnhancedChunker()\n",
        "        self.validator = ThaiValidator()\n",
        "        self.analyze_metadata = analyze_metadata\n",
        "        self.stats = {\n",
        "            'processed': 0,\n",
        "            'failed': 0,\n",
        "            'validation_pass': 0,\n",
        "            'validation_warning': 0,\n",
        "            'validation_fail': 0,\n",
        "            'multipage_files': 0,\n",
        "            'single_page_files': 0,\n",
        "            'metadata_analyzed': 0\n",
        "        }\n",
        "        self.training_pairs = []\n",
        "        self.metadata_collection = []  # ‡πÄ‡∏Å‡πá‡∏ö metadata ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö analysis\n",
        "\n",
        "    # ---------- Top-level ----------\n",
        "    def process_file(self, file_path: Path) -> Dict:\n",
        "        \"\"\"Process ‡πÑ‡∏ü‡∏•‡πå OCR ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata analysis\"\"\"\n",
        "        print(f\"\\nüìÑ Processing: {file_path.name}\")\n",
        "        if self.analyze_metadata:\n",
        "            print(\"   üîç Metadata analysis: ENABLED\")\n",
        "        try:\n",
        "            raw_content = file_path.read_text(encoding='utf-8')\n",
        "            if \"--- PAGE:\" in raw_content:\n",
        "                return self._process_multipage_file(file_path, raw_content)\n",
        "            else:\n",
        "                normalized_content = ThaiTextUtils.normalize_unicode(raw_content)\n",
        "                return self._process_single_page_file(file_path, normalized_content)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "            self.stats['failed'] += 1\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    # ---------- Multi-page with Metadata ----------\n",
        "    def _process_multipage_file(self, file_path: Path, content: str) -> Dict:\n",
        "        \"\"\"Process multi-page file ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata analysis\"\"\"\n",
        "        print(\"   üìñ Multi-page file detected\")\n",
        "        metadata = MultiPageParser.extract_metadata(content)\n",
        "        pages = MultiPageParser.parse_multipage_file(content)\n",
        "        print(f\"   üìä Found {len(pages)} pages\")\n",
        "        if metadata:\n",
        "            print(f\"   üìò Book: {metadata.get('book_title', 'Unknown')}\")\n",
        "            print(f\"   üßæ Chapter: {metadata.get('chapter', 'Unknown')}\")\n",
        "\n",
        "        total_chars = sum(len(p['raw_text']) for p in pages)\n",
        "        estimated_tokens = total_chars * 0.75\n",
        "        print(f\"   üìè Total content: {total_chars:,} chars (~{estimated_tokens:,.0f} tokens)\")\n",
        "\n",
        "        # Process cleaning\n",
        "        if len(pages) <= Config.MAX_PAGES_PER_BATCH:\n",
        "            print(f\"   üöÄ Processing all {len(pages)} pages in one batch...\")\n",
        "            result = self.llm.clean_multipage_ocr(pages, file_path.name)\n",
        "            cleaned_pages = result['cleaned_pages']\n",
        "            total_cost = result['cost']\n",
        "            total_tokens = result['tokens_used']\n",
        "            processing_time = result.get('processing_time', 0)\n",
        "        else:\n",
        "            print(f\"   üì¶ Processing in batches (max {Config.MAX_PAGES_PER_BATCH} pages/batch)...\")\n",
        "            cleaned_pages, total_cost, total_tokens, processing_time = [], 0, 0, 0\n",
        "            for i in range(0, len(pages), Config.MAX_PAGES_PER_BATCH):\n",
        "                batch = pages[i:i + Config.MAX_PAGES_PER_BATCH]\n",
        "                batch_num = i // Config.MAX_PAGES_PER_BATCH + 1\n",
        "                print(f\"      Batch {batch_num}: Pages {batch[0]['page_num']}-{batch[-1]['page_num']}\")\n",
        "                result = self.llm.clean_multipage_ocr(batch, f\"{file_path.name}_batch_{batch_num}\")\n",
        "                cleaned_pages.extend(result['cleaned_pages'])\n",
        "                total_cost += result['cost']\n",
        "                total_tokens += result['tokens_used']\n",
        "                processing_time += result.get('processing_time', 0)\n",
        "                if i + Config.MAX_PAGES_PER_BATCH < len(pages):\n",
        "                    time.sleep(2)\n",
        "\n",
        "        # Analyze metadata for each page (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)\n",
        "        page_metadata_dict = {}\n",
        "        metadata_cost = 0\n",
        "        metadata_tokens = 0\n",
        "\n",
        "        if self.analyze_metadata:\n",
        "            print(\"   üîç Analyzing metadata for each page...\")\n",
        "            for page in cleaned_pages:\n",
        "                page_num = page['page_num']\n",
        "                print(f\"      Analyzing page {page_num}...\")\n",
        "\n",
        "                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå metadata ‡∏à‡∏≤‡∏Å cleaned text\n",
        "                page_meta = self.llm.analyze_page_metadata(\n",
        "                    page_text=page['cleaned_text'],\n",
        "                    page_num=page_num,\n",
        "                    book_info=metadata,\n",
        "                    filename=file_path.name\n",
        "                )\n",
        "\n",
        "                page_metadata_dict[page_num] = page_meta\n",
        "                metadata_cost += page_meta.get('cost', 0)\n",
        "                metadata_tokens += page_meta.get('tokens_used', 0)\n",
        "                self.stats['metadata_analyzed'] += 1\n",
        "\n",
        "                # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö analysis ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á\n",
        "                self.metadata_collection.append({\n",
        "                    'file': file_path.name,\n",
        "                    'page': page_num,\n",
        "                    'metadata': page_meta\n",
        "                })\n",
        "\n",
        "                # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ API rate limit\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            print(f\"   ‚úÖ Metadata analysis complete: {metadata_tokens:,} tokens, ${metadata_cost:.4f}\")\n",
        "            total_cost += metadata_cost\n",
        "            total_tokens += metadata_tokens\n",
        "\n",
        "        # Validation\n",
        "        validation_results = []\n",
        "        for page in cleaned_pages:\n",
        "            if page.get('raw_text') and page.get('cleaned_text'):\n",
        "                val = self.validator.validate_thai_text(\n",
        "                    page['raw_text'], page['cleaned_text'], f\"{file_path.name}_p{page['page_num']}\"\n",
        "                )\n",
        "                validation_results.append(val)\n",
        "\n",
        "        validation_summary = self._summarize_validation(validation_results)\n",
        "\n",
        "        # ---------- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå output ----------\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        clean_filename = f\"{file_path.stem}_clean_{timestamp}.txt\"\n",
        "        clean_path = Path(Config.CLEANED_DIR) / clean_filename\n",
        "\n",
        "        output_content = []\n",
        "        if metadata:\n",
        "            if 'book_title' in metadata:\n",
        "                output_content.append(f\"### üìò ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ (Book Title): {metadata['book_title']}\")\n",
        "            if 'chapter' in metadata:\n",
        "                output_content.append(f\"### üßæ Chapter: {metadata['chapter']}\")\n",
        "            if 'sub_chapter' in metadata:\n",
        "                output_content.append(f\"### üîñ Sub-Chapter: {metadata['sub_chapter']}\")\n",
        "            output_content.append(f\"### üìÇ Format: CLEANED (v3.0)\")\n",
        "            if 'purpose' in metadata:\n",
        "                output_content.append(f\"### üß† Purpose: {metadata['purpose']}\")\n",
        "        output_content.append(f\"### ‚öôÔ∏è Processing: {datetime.now().isoformat()}\")\n",
        "        output_content.append(f\"### üìä Stats: {len(cleaned_pages)} pages, {total_tokens:,} tokens, ${total_cost:.4f}\")\n",
        "        output_content.append(f\"### ‚úÖ Validation: {validation_summary}\")\n",
        "        output_content.append(\"\")\n",
        "\n",
        "        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏° metadata\n",
        "        for page in cleaned_pages:\n",
        "            page_num = page['page_num']\n",
        "            output_content.append(f\"--- PAGE: {page_num} ---\")\n",
        "\n",
        "            # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
        "            if page_num in page_metadata_dict:\n",
        "                meta = page_metadata_dict[page_num]\n",
        "                output_content.append(f\"### üìò Book: {metadata.get('book_title', '')}\")\n",
        "                output_content.append(f\"### üßæ Chapter: {metadata.get('chapter', '')}\")\n",
        "                output_content.append(f\"### üìÑ Page: {page_num}\")\n",
        "                output_content.append(f\"üó£Ô∏è Tone: {', '.join(meta.get('tone', ['unknown']))}\")\n",
        "                output_content.append(f\"üè∑Ô∏è Tags: {', '.join(meta.get('tags', ['general']))}\")\n",
        "                output_content.append(f\"üë• Characters: {', '.join(meta.get('characters', []))}\")\n",
        "                output_content.append(f\"üìç Places: {', '.join(meta.get('places', []))}\")\n",
        "                output_content.append(f\"üî∏ Objects: {', '.join(meta.get('objects', []))}\")\n",
        "                output_content.append(f\"üí¨ Dialogue Pairs: {meta.get('dialogue_pairs', 0)}\")\n",
        "                output_content.append(\n",
        "                    f\"üìä Stats: Chars‚âà{meta.get('char_count', 0)} | Words‚âà{meta.get('word_count', 0)} | Paragraphs‚âà{meta.get('paragraph_count', 0)}\"\n",
        "                )\n",
        "                output_content.append(f\"‚úèÔ∏è Style Notes: {meta.get('style_notes', '')}\")\n",
        "                output_content.append(f\"üìù One-line Summary: {meta.get('summary', '')}\")\n",
        "                if meta.get('anomalies'):\n",
        "                    output_content.append(f\"‚ö†Ô∏è Anomalies: {meta.get('anomalies')}\")\n",
        "                output_content.append(f\"Confidence: {meta.get('confidence', 0.0)}\")\n",
        "\n",
        "            output_content.append(\"--- RAW ---\")\n",
        "            output_content.append(page['raw_text'])\n",
        "            output_content.append(\"--- CLEANED ---\")\n",
        "            output_content.append(page['cleaned_text'])\n",
        "            output_content.append(\"\")\n",
        "\n",
        "        clean_path.write_text('\\n'.join(output_content), encoding='utf-8')\n",
        "\n",
        "        # ‡πÄ‡∏Å‡πá‡∏ö training pairs (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà validation ‡πÑ‡∏°‡πà FAIL)\n",
        "        for page, val in zip(cleaned_pages, validation_results):\n",
        "            if page.get('raw_text') and page.get('cleaned_text') and val['status'] != 'FAIL':\n",
        "                self.training_pairs.append({\n",
        "                    'input': page['raw_text'][:1000],\n",
        "                    'output': page['cleaned_text'][:1000],\n",
        "                    'source': f\"{file_path.name}_page_{page['page_num']}\",\n",
        "                    'timestamp': timestamp,\n",
        "                    'validation_score': val['score'],\n",
        "                    'metadata': metadata,\n",
        "                    'page_metadata': page_metadata_dict.get(page['page_num'], {})\n",
        "                })\n",
        "\n",
        "        # Update stats\n",
        "        self.stats['processed'] += 1\n",
        "        self.stats['multipage_files'] += 1\n",
        "        for val in validation_results:\n",
        "            self.stats[f\"validation_{val['status'].lower()}\"] += 1\n",
        "\n",
        "        print(f\"   ‚úÖ Saved: {clean_filename}\")\n",
        "        print(f\"   üìÑ Pages: {len(cleaned_pages)}\")\n",
        "        print(f\"   üî§ Tokens: {total_tokens:,}\")\n",
        "        print(f\"   ‚è± Time: {processing_time:.1f}s\")\n",
        "        print(f\"   üí∞ Cost: ${total_cost:.4f} (~{total_cost*35:.2f} ‡∏ö‡∏≤‡∏ó)\")\n",
        "        print(f\"   üìã Validation: {validation_summary}\")\n",
        "        if self.analyze_metadata:\n",
        "            print(f\"   üîç Metadata analyzed: {len(page_metadata_dict)} pages\")\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'cleaned_path': str(clean_path),\n",
        "            'pages_count': len(cleaned_pages),\n",
        "            'tokens': total_tokens,\n",
        "            'cost': total_cost,\n",
        "            'processing_time': processing_time,\n",
        "            'validation_summary': validation_summary,\n",
        "            'metadata': metadata,\n",
        "            'page_metadata': page_metadata_dict if self.analyze_metadata else {}\n",
        "        }\n",
        "\n",
        "    # ---------- Single-page with Metadata ----------\n",
        "    def _process_single_page_file(self, file_path: Path, content: str) -> Dict:\n",
        "        \"\"\"Process single page file ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata analysis\"\"\"\n",
        "        print(\"   üìÑ Single page file\")\n",
        "\n",
        "        # Clean OCR\n",
        "        if len(content) > 3000:\n",
        "            chunks = self.chunker.smart_chunk_text(content, 2500)\n",
        "            cleaned_chunks, total_cost, total_tokens, processing_time = [], 0, 0, 0\n",
        "            print(f\"   üì¶ Split into {len(chunks)} smart chunks\")\n",
        "            for i, chunk in enumerate(chunks, 1):\n",
        "                print(f\"      Chunk {i}/{len(chunks)}...\")\n",
        "                result = self.llm.clean_ocr_text(chunk, filename=f\"{file_path.name}_chunk_{i}\")\n",
        "                cleaned_chunks.append(result['cleaned_text'])\n",
        "                total_cost += result.get('cost', 0)\n",
        "                total_tokens += result.get('tokens_used', 0)\n",
        "                processing_time += result.get('processing_time', 0)\n",
        "                time.sleep(1)\n",
        "            cleaned_text = self.chunker.merge_chunks_with_dedup(cleaned_chunks)\n",
        "        else:\n",
        "            result = self.llm.clean_ocr_text(content, filename=file_path.name)\n",
        "            cleaned_text = result['cleaned_text']\n",
        "            total_cost = result['cost']\n",
        "            total_tokens = result['tokens_used']\n",
        "            processing_time = result.get('processing_time', 0)\n",
        "\n",
        "        # Analyze metadata ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "        page_metadata = {}\n",
        "        if self.analyze_metadata:\n",
        "            print(\"   üîç Analyzing metadata...\")\n",
        "            page_metadata = self.llm.analyze_page_metadata(\n",
        "                page_text=cleaned_text,\n",
        "                page_num=1,\n",
        "                book_info={'book_title': file_path.stem},\n",
        "                filename=file_path.name\n",
        "            )\n",
        "            total_cost += page_metadata.get('cost', 0)\n",
        "            total_tokens += page_metadata.get('tokens_used', 0)\n",
        "            self.stats['metadata_analyzed'] += 1\n",
        "\n",
        "        # Validation\n",
        "        validation_result = self.validator.validate_thai_text(content, cleaned_text, file_path.name)\n",
        "\n",
        "        # Create output file\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        clean_filename = (f\"WARNING_{file_path.stem}_clean_{timestamp}.txt\"\n",
        "                          if validation_result['status'] == 'FAIL'\n",
        "                          else f\"{file_path.stem}_clean_{timestamp}.txt\")\n",
        "        clean_path = Path(Config.CLEANED_DIR) / clean_filename\n",
        "\n",
        "        # Build output content\n",
        "        output_lines = []\n",
        "        # Header\n",
        "        output_lines.append(f\"### üìò File: {file_path.name}\")\n",
        "        output_lines.append(f\"### üìÇ Format: CLEANED (v3.0)\")\n",
        "        output_lines.append(f\"### ‚öôÔ∏è Processing: {datetime.now().isoformat()}\")\n",
        "        output_lines.append(f\"### üìä Stats: {total_tokens:,} tokens, ${total_cost:.4f}\")\n",
        "        output_lines.append(f\"### ‚úÖ Validation: {validation_result['status']} (score: {validation_result['score']:.2f})\")\n",
        "        output_lines.append(\"\")\n",
        "\n",
        "        # Metadata ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
        "        if page_metadata:\n",
        "            output_lines.append(\"--- METADATA ---\")\n",
        "            output_lines.append(f\"üó£Ô∏è Tone: {', '.join(page_metadata.get('tone', ['unknown']))}\")\n",
        "            output_lines.append(f\"üè∑Ô∏è Tags: {', '.join(page_metadata.get('tags', ['general']))}\")\n",
        "            output_lines.append(f\"üë• Characters: {', '.join(page_metadata.get('characters', []))}\")\n",
        "            output_lines.append(f\"üìç Places: {', '.join(page_metadata.get('places', []))}\")\n",
        "            output_lines.append(f\"üî∏ Objects: {', '.join(page_metadata.get('objects', []))}\")\n",
        "            output_lines.append(f\"üí¨ Dialogue Pairs: {page_metadata.get('dialogue_pairs', 0)}\")\n",
        "            output_lines.append(\n",
        "                f\"üìä Stats: Chars‚âà{page_metadata.get('char_count', 0)} | Words‚âà{page_metadata.get('word_count', 0)} | Paragraphs‚âà{page_metadata.get('paragraph_count', 0)}\"\n",
        "            )\n",
        "            output_lines.append(f\"‚úèÔ∏è Style Notes: {page_metadata.get('style_notes', '')}\")\n",
        "            output_lines.append(f\"üìù One-line Summary: {page_metadata.get('summary', '')}\")\n",
        "            if page_metadata.get('anomalies'):\n",
        "                output_lines.append(f\"‚ö†Ô∏è Anomalies: {page_metadata.get('anomalies')}\")\n",
        "            output_lines.append(f\"Confidence: {page_metadata.get('confidence', 0.0)}\")\n",
        "            output_lines.append(\"\")\n",
        "\n",
        "        # Content\n",
        "        output_lines.append(\"--- CLEANED ---\")\n",
        "        output_lines.append(cleaned_text)\n",
        "\n",
        "        clean_path.write_text('\\n'.join(output_lines), encoding='utf-8')\n",
        "\n",
        "        # Save training pair\n",
        "        if validation_result['status'] != 'FAIL':\n",
        "            self.training_pairs.append({\n",
        "                'input': content[:1000],\n",
        "                'output': cleaned_text[:1000],\n",
        "                'source': file_path.name,\n",
        "                'timestamp': timestamp,\n",
        "                'validation_score': validation_result['score'],\n",
        "                'metadata': page_metadata if self.analyze_metadata else {}\n",
        "            })\n",
        "\n",
        "        # Update stats\n",
        "        self.stats['processed'] += 1\n",
        "        self.stats['single_page_files'] += 1\n",
        "        self.stats[f\"validation_{validation_result['status'].lower()}\"] += 1\n",
        "\n",
        "        print(f\"   ‚úÖ Saved: {clean_filename}\")\n",
        "        print(f\"   üìã Validation: {validation_result['status']} (score: {validation_result['score']:.2f})\")\n",
        "        print(f\"   üí∞ Cost: ${total_cost:.4f}\")\n",
        "        if self.analyze_metadata:\n",
        "            print(f\"   üîç Metadata analyzed: 1 page\")\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'cleaned_path': str(clean_path),\n",
        "            'tokens': total_tokens,\n",
        "            'cost': total_cost,\n",
        "            'processing_time': processing_time,\n",
        "            'validation_result': validation_result,\n",
        "            'metadata': page_metadata if self.analyze_metadata else {}\n",
        "        }\n",
        "\n",
        "    # ---------- Helper Methods ----------\n",
        "    def _summarize_validation(self, validation_results: List[Dict]) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ validation\"\"\"\n",
        "        if not validation_results:\n",
        "            return \"No validation data\"\n",
        "        pass_count = sum(1 for v in validation_results if v['status'] == 'PASS')\n",
        "        warning_count = sum(1 for v in validation_results if v['status'] == 'WARNING')\n",
        "        fail_count = sum(1 for v in validation_results if v['status'] == 'FAIL')\n",
        "        avg_score = sum(v['score'] for v in validation_results) / len(validation_results)\n",
        "        return f\"‚úÖ{pass_count} ‚ö†Ô∏è{warning_count} ‚ùå{fail_count} (avg: {avg_score:.2f})\"\n",
        "\n",
        "    # ---------- Batch Processing ----------\n",
        "    def process_batch(self, file_pattern: str = \"*.txt\", limit: int = None, analyze_metadata: bool = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å RAW_OCR_DIR\n",
        "\n",
        "        Args:\n",
        "            file_pattern: ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\n",
        "            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "            analyze_metadata: override ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ metadata analysis\n",
        "        \"\"\"\n",
        "        if analyze_metadata is not None:\n",
        "            self.analyze_metadata = analyze_metadata\n",
        "\n",
        "        raw_dir = Path(Config.RAW_OCR_DIR)\n",
        "        files = sorted(list(raw_dir.glob(file_pattern)))\n",
        "        if limit:\n",
        "            files = files[:limit]\n",
        "\n",
        "        print(f\"\\nüß∫ Batch: {len(files)} files found (pattern='{file_pattern}')\")\n",
        "        if self.analyze_metadata:\n",
        "            print(\"   üîç Metadata analysis: ENABLED\")\n",
        "        else:\n",
        "            print(\"   ‚ö° Metadata analysis: DISABLED (faster, cheaper)\")\n",
        "\n",
        "        results = []\n",
        "        for i, f in enumerate(files, 1):\n",
        "            print(f\"\\n[{i}/{len(files)}] {f.name}\")\n",
        "            res = self.process_file(f)\n",
        "            results.append({'file': f.name, **res})\n",
        "\n",
        "        print(\"\\nüèÅ Batch done.\")\n",
        "        return {\n",
        "            'count': len(results),\n",
        "            'results': results,\n",
        "            'stats': self.stats\n",
        "        }\n",
        "\n",
        "    # ---------- Export Functions ----------\n",
        "    def export_training_pairs(self, out_name: str = None) -> str:\n",
        "        \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å training pairs ‡πÄ‡∏õ‡πá‡∏ô JSONL\"\"\"\n",
        "        if not self.training_pairs:\n",
        "            print(\"‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ training_pairs ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\")\n",
        "            return \"\"\n",
        "        out_dir = Path(Config.TRAINING_PAIRS_DIR)\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        out_name = out_name or f\"training_pairs_{stamp}.jsonl\"\n",
        "        out_path = out_dir / out_name\n",
        "        with open(out_path, 'w', encoding='utf-8') as f:\n",
        "            for row in self.training_pairs:\n",
        "                f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "        print(f\"‚úÖ Exported training pairs: {out_path.name} ({len(self.training_pairs)} rows)\")\n",
        "        return str(out_path)\n",
        "\n",
        "    def export_metadata_analysis(self, out_name: str = None) -> str:\n",
        "        \"\"\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metadata analysis ‡πÄ‡∏õ‡πá‡∏ô JSON\"\"\"\n",
        "        if not self.metadata_collection:\n",
        "            print(\"‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ metadata ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\")\n",
        "            return \"\"\n",
        "        out_dir = Path(Config.CORPUS_DIR)\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        out_name = out_name or f\"metadata_analysis_{stamp}.json\"\n",
        "        out_path = out_dir / out_name\n",
        "        with open(out_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.metadata_collection, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"‚úÖ Exported metadata analysis: {out_path.name} ({len(self.metadata_collection)} pages)\")\n",
        "        return str(out_path)\n",
        "\n",
        "    def get_usage_summary(self) -> Dict:\n",
        "        \"\"\"‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\"\"\n",
        "        return self.llm.get_usage_summary()\n",
        "\n",
        "print(\"‚úÖ Enhanced OCR Processor with Metadata ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inUR98QoXBv_",
        "outputId": "1ccdf317-efd5-4f41-91d8-baa798b203d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced OCR Processor with Metadata ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 10: Corpus Builder\n",
        "# ============================================\n",
        "class CorpusBuilder:\n",
        "    \"\"\"\n",
        "    ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô CLEANED_DIR ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "    - ‡∏î‡∏∂‡∏á‡πÄ‡∏°‡∏ó‡∏≤‡∏î‡∏≤‡∏ï‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡πÑ‡∏ü‡∏•‡πå (### ... )\n",
        "    - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ CLEANED ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤\n",
        "    - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSONL/CSV ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cleaned_dir = Path(Config.CLEANED_DIR)\n",
        "        self.corpus_dir = Path(Config.CORPUS_DIR)\n",
        "        self.corpus_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _parse_cleaned_file(self, path: Path) -> Dict:\n",
        "        \"\"\"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CLEANED ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô metadata + pages\"\"\"\n",
        "        text = path.read_text(encoding='utf-8', errors='ignore')\n",
        "        meta = MultiPageParser.extract_metadata(text)\n",
        "\n",
        "        # ‡πÅ‡∏¢‡∏Å‡∏´‡∏ô‡πâ‡∏≤ (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡∏°‡∏µ --- PAGE: ... ---)\n",
        "        pages = MultiPageParser.parse_multipage_file(text)\n",
        "        if not pages:\n",
        "            # ‡πÄ‡∏õ‡πá‡∏ô single text: ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "            pages = [{\n",
        "                'page_num': 1,\n",
        "                'raw_text': \"\",\n",
        "                'cleaned_text': ThaiTextUtils.normalize_unicode(text)\n",
        "            }]\n",
        "\n",
        "        # ‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞ CLEANED\n",
        "        all_cleaned = []\n",
        "        for p in pages:\n",
        "            ct = p.get('cleaned_text', '')\n",
        "            if ct:\n",
        "                all_cleaned.append(ct)\n",
        "\n",
        "        return {\n",
        "            'file': path.name,\n",
        "            'meta': meta,\n",
        "            'pages': pages,\n",
        "            'cleaned_joined': \"\\n\".join(all_cleaned).strip()\n",
        "        }\n",
        "\n",
        "    def build(self, pattern: str = \"*.txt\", out_stem: str = None) -> Dict:\n",
        "        \"\"\"‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô CLEANED_DIR ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á pattern ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏™\"\"\"\n",
        "        files = sorted(self.cleaned_dir.glob(pattern))\n",
        "        if not files:\n",
        "            print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô CLEANED_DIR\")\n",
        "            return {'count': 0, 'jsonl': '', 'csv': ''}\n",
        "\n",
        "        data_rows = []\n",
        "        jsonl_rows = []\n",
        "        total_chars = 0\n",
        "        total_pages = 0\n",
        "\n",
        "        for f in files:\n",
        "            parsed = self._parse_cleaned_file(f)\n",
        "            book = parsed['meta'].get('book_title', '')\n",
        "            chapter = parsed['meta'].get('chapter', '')\n",
        "            sub = parsed['meta'].get('sub_chapter', '')\n",
        "            text_joined = parsed['cleaned_joined']\n",
        "            n_pages = len(parsed['pages'])\n",
        "\n",
        "            total_pages += n_pages\n",
        "            total_chars += len(text_joined)\n",
        "\n",
        "            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö JSONL (‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÅ‡∏ñ‡∏ß)\n",
        "            jsonl_rows.append({\n",
        "                'source_file': parsed['file'],\n",
        "                'book_title': book,\n",
        "                'chapter': chapter,\n",
        "                'sub_chapter': sub,\n",
        "                'pages': n_pages,\n",
        "                'text': text_joined\n",
        "            })\n",
        "\n",
        "            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV (‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡πÜ)\n",
        "            data_rows.append({\n",
        "                'source_file': parsed['file'],\n",
        "                'book_title': book,\n",
        "                'chapter': chapter,\n",
        "                'sub_chapter': sub,\n",
        "                'pages': n_pages,\n",
        "                'chars': len(text_joined)\n",
        "            })\n",
        "\n",
        "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        out_stem = out_stem or f\"corpus_{stamp}\"\n",
        "        jsonl_path = self.corpus_dir / f\"{out_stem}.jsonl\"\n",
        "        csv_path = self.corpus_dir / f\"{out_stem}.csv\"\n",
        "        stats_path = self.corpus_dir / f\"{out_stem}_stats.json\"\n",
        "\n",
        "        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô JSONL\n",
        "        with open(jsonl_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "            for row in jsonl_rows:\n",
        "                jf.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô CSV\n",
        "        pd.DataFrame(data_rows).to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥\n",
        "        stats = {\n",
        "            'files': len(files),\n",
        "            'pages': total_pages,\n",
        "            'chars': total_chars,\n",
        "            'avg_chars_per_file': (total_chars / len(files)) if files else 0,\n",
        "            'avg_pages_per_file': (total_pages / len(files)) if files else 0,\n",
        "            'created_at': datetime.now().isoformat()\n",
        "        }\n",
        "        with open(stats_path, \"w\", encoding=\"utf-8\") as sf:\n",
        "            json.dump(stats, sf, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"‚úÖ Corpus JSONL: {jsonl_path.name}\")\n",
        "        print(f\"‚úÖ Corpus CSV:   {csv_path.name}\")\n",
        "        print(f\"üìà Stats:        {stats_path.name}\")\n",
        "        print(f\"üì¶ Files: {stats['files']}, Pages: {stats['pages']}, Chars: {stats['chars']:,}\")\n",
        "\n",
        "        return {\n",
        "            'count': len(files),\n",
        "            'jsonl': str(jsonl_path),\n",
        "            'csv': str(csv_path),\n",
        "            'stats': stats\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Corpus Builder ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqbfvrs5uYJa",
        "outputId": "0b202cef-ab19-4bf2-82aa-2b31405e286b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Corpus Builder ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 11: Quick Start Functions\n",
        "# ============================================\n",
        "\n",
        "def quick_setup():\n",
        "    \"\"\"Enhanced setup with validation\"\"\"\n",
        "    print(\"\\nüîß Enhanced Quick Setup v3.0\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1) ‡∏ï‡∏£‡∏ß‡∏à API Keys\n",
        "    if not Config.OPENAI_API_KEY:\n",
        "        print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö OpenAI API key\")\n",
        "        print(f\"   ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå: {Config.BASE}/openai.env ‡∏´‡∏£‡∏∑‡∏≠ openai.env.txt\")\n",
        "        print(\"   ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: sk-xxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "    else:\n",
        "        print(\"‚úÖ OpenAI API key: OK\")\n",
        "\n",
        "    # 2) ‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\n",
        "    print(\"\\nüìÅ Directories\")\n",
        "    for p in [Config.RAW_OCR_DIR, Config.CLEANED_DIR, Config.CORPUS_DIR, Config.TRAINING_PAIRS_DIR, Config.LOGS_DIR]:\n",
        "        Path(p).mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"   - {p} ‚úîÔ∏è\")\n",
        "\n",
        "    # 3) ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≠‡∏ô‡∏ü‡∏¥‡∏Å\n",
        "    print(\"\\n‚öôÔ∏è Config\")\n",
        "    print(f\"   Model: {Config.MODEL}\")\n",
        "    print(f\"   Max pages per batch: {Config.MAX_PAGES_PER_BATCH}\")\n",
        "    print(f\"   Temperature: {Config.TEMPERATURE}\")\n",
        "    print(f\"   Context overlap: {Config.CONTEXT_OVERLAP}\")\n",
        "    print(\"\\n‚úÖ Setup complete.\")\n",
        "\n",
        "\n",
        "def quick_process_sample(filename: str):\n",
        "    \"\"\"\n",
        "    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏≤‡∏Å RAW_OCR_DIR ‡πÇ‡∏î‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
        "    \"\"\"\n",
        "    path = Path(Config.RAW_OCR_DIR) / filename\n",
        "    if not path.exists():\n",
        "        print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {path}\")\n",
        "        return\n",
        "    proc = OCRProcessor()\n",
        "    result = proc.process_file(path)\n",
        "    print(\"\\nüì¶ Result (single):\")\n",
        "    print(json.dumps({k: v for k, v in result.items() if k != 'metadata'}, ensure_ascii=False, indent=2))\n",
        "    return result\n",
        "\n",
        "\n",
        "def quick_batch(pattern: str = \"*.txt\", limit: int = None):\n",
        "    \"\"\"\n",
        "    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å RAW_OCR_DIR ‡∏î‡πâ‡∏ß‡∏¢ pattern ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "    \"\"\"\n",
        "    proc = OCRProcessor()\n",
        "    summary = proc.process_batch(file_pattern=pattern, limit=limit)\n",
        "    # export training pairs (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
        "    out_pairs = proc.export_training_pairs()\n",
        "    print(\"\\nüßæ Usage summary (session):\")\n",
        "    print(json.dumps(proc.get_usage_summary(), ensure_ascii=False, indent=2))\n",
        "    return {'batch': summary, 'training_pairs': out_pairs}\n",
        "\n",
        "\n",
        "def quick_build_corpus(pattern: str = \"*.txt\", out_stem: str = None):\n",
        "    \"\"\"\n",
        "    ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô CLEANED_DIR ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏™ JSONL/CSV\n",
        "    \"\"\"\n",
        "    builder = CorpusBuilder()\n",
        "    return builder.build(pattern=pattern, out_stem=out_stem)\n",
        "\n",
        "\n",
        "def show_usage_log_summary():\n",
        "    \"\"\"\n",
        "    ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ usage ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå logs/usage.csv ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "    \"\"\"\n",
        "    logger = UsageLogger()\n",
        "    s = logger.get_summary()\n",
        "    print(\"\\nüìä Usage Log Summary (all runs)\")\n",
        "    print(json.dumps(s, ensure_ascii=False, indent=2))\n",
        "    return s\n",
        "\n",
        "print(\"‚úÖ Quick Start Functions ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpviaRvnugCB",
        "outputId": "99c3c39d-751d-4823-8077-2fa0b1bcb722"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Quick Start Functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 12: Main / CLI Runner (Colab-safe, with guards)\n",
        "# ============================================\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def _fallback_usage_summary():\n",
        "    \"\"\"‡∏™‡∏£‡∏∏‡∏õ usage ‡πÅ‡∏ö‡∏ö fallback ‡πÄ‡∏°‡∏∑‡πà‡∏≠ UsageLogger ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°\"\"\"\n",
        "    # ‡πÄ‡∏î‡∏≤ path ‡∏ó‡∏±‡πâ‡∏á 2 ‡πÇ‡∏´‡∏°‡∏î (Colab / Local)\n",
        "    candidates = [\n",
        "        Path(\"/content/drive/MyDrive/OCR/logs/usage.csv\"),\n",
        "        Path(\"./OCR/logs/usage.csv\"),\n",
        "    ]\n",
        "    log_file = next((p for p in candidates if p.exists()), None)\n",
        "\n",
        "    print(\"\\nüìí Usage Log Summary (All time) [fallback]\")\n",
        "    print(\"=\" * 50)\n",
        "    if not log_file:\n",
        "        print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå usage.csv ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå logs\")\n",
        "        return\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv(log_file, encoding=\"utf-8\")\n",
        "        if df.empty:\n",
        "            print(\"‚ÑπÔ∏è usage.csv ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤\")\n",
        "            return\n",
        "        total_files = len(df)\n",
        "        total_pages = df[\"pages_count\"].sum() if \"pages_count\" in df.columns else 0\n",
        "        total_tokens = df[\"total_tokens\"].sum() if \"total_tokens\" in df.columns else 0\n",
        "        total_cost_usd = df[\"cost_usd\"].sum() if \"cost_usd\" in df.columns else 0.0\n",
        "        total_cost_thb = df[\"cost_thb\"].sum() if \"cost_thb\" in df.columns else total_cost_usd * 35\n",
        "        avg_cost_per_page = (total_cost_usd / total_pages) if total_pages else 0\n",
        "\n",
        "        print(f\"Total files         : {total_files}\")\n",
        "        print(f\"Total pages         : {total_pages}\")\n",
        "        print(f\"Total tokens        : {total_tokens:,}\")\n",
        "        print(f\"Total cost (USD)    : ${total_cost_usd:.4f}\")\n",
        "        print(f\"Total cost (THB)    : ~{total_cost_thb:.2f}\")\n",
        "        print(f\"Avg cost per page   : ${avg_cost_per_page:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ‡∏≠‡πà‡∏≤‡∏ô usage.csv ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")\n",
        "\n",
        "\n",
        "def show_usage_log_summary():\n",
        "    \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ usage ‡πÇ‡∏î‡∏¢‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ UsageLogger ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô fallback\"\"\"\n",
        "    if \"UsageLogger\" in globals():\n",
        "        print(\"\\nüìí Usage Log Summary (All time)\")\n",
        "        print(\"=\" * 50)\n",
        "        ul = UsageLogger()\n",
        "        s = ul.get_summary()\n",
        "        print(f\"Total files         : {s.get('total_files',0)}\")\n",
        "        print(f\"Total pages         : {s.get('total_pages',0)}\")\n",
        "        print(f\"Total tokens        : {s.get('total_tokens',0):,}\")\n",
        "        print(f\"Input tokens        : {s.get('input_tokens',0):,}\")\n",
        "        print(f\"Output tokens       : {s.get('output_tokens',0):,}\")\n",
        "        print(f\"Total cost (USD)    : ${s.get('total_cost_usd',0):.4f}\")\n",
        "        print(f\"Total cost (THB)    : ~{s.get('total_cost_thb',0):.2f}\")\n",
        "        print(f\"Avg cost per page   : ${s.get('avg_cost_per_page',0):.4f}\")\n",
        "        print(f\"Most used model     : {s.get('most_used_model','N/A')}\")\n",
        "        print(f\"Validation stats    : {s.get('validation_stats',{})}\")\n",
        "    else:\n",
        "        _fallback_usage_summary()\n",
        "\n",
        "\n",
        "def quick_sample(sample_name: str):\n",
        "    print(\"\\n‚ñ∂Ô∏è Quick Sample Mode\")\n",
        "    print(\"=\" * 50)\n",
        "    if not sample_name:\n",
        "        print(\"‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ --sample ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ä‡πà‡∏ô --sample sample.txt\")\n",
        "        return\n",
        "    if \"Config\" not in globals() or \"OCRProcessor\" not in globals():\n",
        "        print(\"‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å 1‚Äì11 ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Config, OCRProcessor)\")\n",
        "        return\n",
        "    file_path = Path(Config.RAW_OCR_DIR) / sample_name\n",
        "    if not file_path.exists():\n",
        "        print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}\")\n",
        "        return\n",
        "    proc = OCRProcessor()\n",
        "    result = proc.process_file(file_path)\n",
        "    if result.get('success'):\n",
        "        print(f\"\\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {result['cleaned_path']}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {result.get('error','unknown error')}\")\n",
        "\n",
        "\n",
        "def quick_batch(limit: int = None):\n",
        "    print(\"\\nüöÄ Batch Mode\")\n",
        "    print(\"=\" * 50)\n",
        "    if \"Config\" not in globals() or \"OCRProcessor\" not in globals():\n",
        "        print(\"‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å 1‚Äì11 ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Config, OCRProcessor)\")\n",
        "        return\n",
        "    raw_dir = Path(Config.RAW_OCR_DIR)\n",
        "    files = sorted(raw_dir.glob(\"*.txt\"))\n",
        "    if not files:\n",
        "        print(f\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô {raw_dir}\")\n",
        "        return\n",
        "    if limit is not None:\n",
        "        files = files[:max(0, int(limit))]\n",
        "    print(f\"üìÅ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {len(files)} ‡πÑ‡∏ü‡∏•‡πå\")\n",
        "    proc = OCRProcessor()\n",
        "    for i, f in enumerate(files, 1):\n",
        "        print(f\"\\n[{i}/{len(files)}] {f.name}\")\n",
        "        _ = proc.process_file(f)\n",
        "    usage = proc.get_usage_summary()\n",
        "    print(\"\\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Session)\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Total tokens     : {usage['total_tokens']:,}\")\n",
        "    print(f\"Total cost (USD) : ${usage['total_cost_usd']:.4f}\")\n",
        "    print(f\"Total cost (THB) : ~{usage['total_cost_thb']:.2f}\")\n",
        "    print(f\"Files processed  : {usage['session_files']}\")\n",
        "    print(f\"Pages processed  : {usage['session_pages']}\")\n",
        "    print(f\"Avg cost/page    : ${usage['avg_cost_per_page']:.4f}\")\n",
        "\n",
        "\n",
        "def quick_build_corpus(merge_name: str = None):\n",
        "    print(\"\\nüß± Build Corpus\")\n",
        "    print(\"=\" * 50)\n",
        "    if \"Config\" not in globals():\n",
        "        print(\"‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å 1‚Äì11 ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Config)\")\n",
        "        return\n",
        "    cleaned_dir = Path(Config.CLEANED_DIR)\n",
        "    out_dir = Path(Config.CORPUS_DIR)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    files = sorted(cleaned_dir.glob(\"*.txt\"))\n",
        "    if not files:\n",
        "        print(f\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô {cleaned_dir}\")\n",
        "        return\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    corpus_name = merge_name.strip() if merge_name else f\"corpus_{ts}.txt\"\n",
        "    out_path = out_dir / corpus_name\n",
        "    print(f\"üìÅ ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å: {cleaned_dir}\")\n",
        "    print(f\"üìù ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á: {out_path}\")\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as w:\n",
        "        for f in files:\n",
        "            try:\n",
        "                txt = f.read_text(encoding=\"utf-8\")\n",
        "                w.write(f\"\\n\\n===== FILE: {f.name} =====\\n\\n\")\n",
        "                w.write(txt)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå {f.name}: {e}\")\n",
        "    print(f\"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {out_path}\")\n",
        "\n",
        "\n",
        "# ---------- CLI ----------\n",
        "parser = argparse.ArgumentParser(description=\"Enhanced Thai Novel OCR Processor - Main Runner\")\n",
        "parser.add_argument(\"--sample\", type=str, help=\"‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô RAW_OCR_DIR ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÄ‡∏ä‡πà‡∏ô sample.txt)\")\n",
        "parser.add_argument(\"--batch\", action=\"store_true\", help=\"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô RAW_OCR_DIR\")\n",
        "parser.add_argument(\"--limit\", type=int, default=None, help=\"‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô batch\")\n",
        "parser.add_argument(\"--build-corpus\", action=\"store_true\", help=\"‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CLEANED ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô corpus\")\n",
        "parser.add_argument(\"--corpus-name\", type=str, default=None, help=\"‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå corpus ‡πÄ‡∏≠‡∏á (‡πÄ‡∏ä‡πà‡∏ô my_corpus.txt)\")\n",
        "parser.add_argument(\"--summary\", action=\"store_true\", help=\"‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ usage log ‡∏™‡∏∞‡∏™‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
        "\n",
        "# ‡πÉ‡∏ä‡πâ parse_known_args ‡∏Å‡∏±‡∏ô -f kernel.json ‡πÉ‡∏ô Colab\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "def _main():\n",
        "    ran_any = False\n",
        "    if args.sample:\n",
        "        ran_any = True\n",
        "        quick_sample(args.sample)\n",
        "    if args.batch:\n",
        "        ran_any = True\n",
        "        print(\"\\nüì¶ ‡πÇ‡∏´‡∏°‡∏î Batch\")\n",
        "        quick_batch(limit=args.limit)\n",
        "    if args.build_corpus:\n",
        "        ran_any = True\n",
        "        print(\"\\nüìö ‡∏£‡∏ß‡∏° Corpus\")\n",
        "        quick_build_corpus(merge_name=args.corpus_name)\n",
        "    if args.summary or not ran_any:\n",
        "        if not ran_any:\n",
        "            print(\"\\n‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á argument ‡πÉ‡∏î ‡πÜ ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô\")\n",
        "        print(\"\\nüìà ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
        "        show_usage_log_summary()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    _main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnxJQ_S-wVUM",
        "outputId": "e78811aa-36c8-4e20-a0cb-7e3307caf86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á argument ‡πÉ‡∏î ‡πÜ ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡∏Å‡πà‡∏≠‡∏ô\n",
            "\n",
            "üìà ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
            "\n",
            "üìí Usage Log Summary (All time)\n",
            "==================================================\n",
            "Total files         : 0\n",
            "Total pages         : 0\n",
            "Total tokens        : 0\n",
            "Input tokens        : 0\n",
            "Output tokens       : 0\n",
            "Total cost (USD)    : $0.0000\n",
            "Total cost (THB)    : ~0.00\n",
            "Avg cost per page   : $0.0000\n",
            "Most used model     : N/A\n",
            "Validation stats    : {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Run me\n",
        "processor = OCRProcessor(analyze_metadata=True)  # True = ‡∏°‡∏µ metadata, False = ‡πÅ‡∏Ñ‡πà clean\n",
        "result = processor.process_file(Path(Config.RAW_OCR_DIR) / \"Namiya.txt\")\n",
        "\n",
        "print(\"\\nüìä Run Summary:\")\n",
        "print(\"Success:\", result.get('success'))\n",
        "if result.get('success'):\n",
        "    print(\"Output file:\", result.get('cleaned_path'))\n",
        "    print(\"Cost: $\", result.get('cost'))\n",
        "    print(\"Tokens used:\", result.get('tokens'))\n",
        "\n",
        "show_usage_log_summary()"
      ],
      "metadata": {
        "id": "BM3nfoOQYOtL",
        "outputId": "2dc4c6f1-9dc3-4f28-9af4-28663188970d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI client ready (Model: gpt-4o-mini)\n",
            "\n",
            "üìÑ Processing: Namiya.txt\n",
            "   üîç Metadata analysis: ENABLED\n",
            "   üìñ Multi-page file detected\n",
            "   üìä Found 5 pages\n",
            "   üìò Book: (‡∏õ‡∏≤‡∏è‡∏¥‡∏´‡∏≤‡∏£‡∏¢‡πå‡∏£‡πâ‡∏≤‡∏ô‡∏ä‡∏≥‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏≤‡∏°‡∏¥‡∏¢‡∏∞)\n",
            "   üßæ Chapter: (‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2 ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏´‡∏µ‡∏ö‡πÄ‡∏û‡∏•‡∏á‡∏õ‡∏≤‡∏Å‡πÉ‡∏ô‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏Å‡∏™‡∏á‡∏±‡∏î 2-8)\n",
            "   üìè Total content: 4,250 chars (~3,188 tokens)\n",
            "   üöÄ Processing all 5 pages in one batch...\n",
            "   ‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ 1\n",
            "   ‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "   ‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ 3\n",
            "   ‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ 4\n",
            "   ‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ 4\n",
            "   üîç Analyzing metadata for each page...\n",
            "      Analyzing page 1...\n",
            "      Analyzing page 2...\n",
            "      Analyzing page 3...\n",
            "      Analyzing page 4...\n",
            "      Analyzing page 4...\n",
            "   ‚úÖ Metadata analysis complete: 4,992 tokens, $0.0012\n",
            "   ‚úÖ Saved: Namiya_clean_20250829_134344.txt\n",
            "   üìÑ Pages: 5\n",
            "   üî§ Tokens: 9,301\n",
            "   ‚è± Time: 32.0s\n",
            "   üí∞ Cost: $0.0027 (~0.09 ‡∏ö‡∏≤‡∏ó)\n",
            "   üìã Validation: ‚úÖ0 ‚ö†Ô∏è5 ‚ùå0 (avg: 0.90)\n",
            "   üîç Metadata analyzed: 4 pages\n",
            "\n",
            "üìä Run Summary:\n",
            "Success: True\n",
            "Output file: /content/drive/MyDrive/OCR/cleaned/Namiya_clean_20250829_134344.txt\n",
            "Cost: $ 0.0026727\n",
            "Tokens used: 9301\n",
            "\n",
            "üìä Usage Log Summary (all runs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-462120127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokens used:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mshow_usage_log_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3178396450.py\u001b[0m in \u001b[0;36mshow_usage_log_summary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìä Usage Log Summary (all runs)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}