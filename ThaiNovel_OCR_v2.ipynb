{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/ThaiNovel_OCR_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell ใหม่: Mount Drive + Create Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# สร้าง folders ใน Drive\n",
        "import os\n",
        "BASE = '/content/drive/MyDrive/OCR'  # ← ชื่อ folder ของคุณ\n",
        "\n",
        "for folder in ['raw_ocr', 'batches', 'cleaned_gpt', 'cleaned_claude', 'final_corpus', 'reports', 'training_pairs']:\n",
        "    os.makedirs(f'{BASE}/{folder}', exist_ok=True)\n",
        "\n",
        "print(\"✅ Folders ready in Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm_K-lJ9hX8B",
        "outputId": "be421778-cc0f-41c7-80d9-6eb7da97e4b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Folders ready in Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check pyperclip\n",
        "try:\n",
        "    import pyperclip\n",
        "    CLIPBOARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIPBOARD_AVAILABLE = False\n",
        "    print(\"⚠️ pyperclip not installed - จะใช้ไฟล์แทน clipboard\")\n",
        "\n",
        "print(\"✅ Libraries loaded\")"
      ],
      "metadata": {
        "id": "6LyJqyOoc52Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d1f15c-822e-41cf-ea4e-f6e1cafe2b02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ============================================\n",
        "# 📌 Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import pyperclip\n",
        "    CLIPBOARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIPBOARD_AVAILABLE = False\n",
        "    print(\"⚠️ pyperclip not installed - จะใช้ไฟล์แทน clipboard\")\n",
        "\n",
        "print(\"✅ Libraries loaded\")\n",
        "\n",
        "# ============================================\n",
        "# 📌 Block 2: Enhanced Configuration\n",
        "# ============================================\n",
        "class Config:\n",
        "    \"\"\"Configuration สำหรับ OCR Processing - Thai Novel Optimized\"\"\"\n",
        "\n",
        "    # Paths - ชี้ไปที่ Drive\n",
        "    BASE = '/content/drive/MyDrive/OCR'\n",
        "\n",
        "    RAW_OCR_DIR = f'{BASE}/raw_ocr'\n",
        "    BATCHES_DIR = f'{BASE}/batches'\n",
        "    CLEANED_GPT_DIR = f'{BASE}/cleaned_gpt'\n",
        "    CLEANED_CLAUDE_DIR = f'{BASE}/cleaned_claude'\n",
        "    FINAL_DIR = f'{BASE}/final_corpus'\n",
        "    REPORTS_DIR = f'{BASE}/reports'\n",
        "    TRAINING_PAIRS_DIR = f'{BASE}/training_pairs'\n",
        "\n",
        "    # Processing parameters\n",
        "    MAX_PAGES_PER_BATCH = 20\n",
        "    MIN_LINE_LENGTH = 3\n",
        "\n",
        "    # Enhanced OCR replacements for Thai novels\n",
        "    OCR_REPLACEMENTS = {\n",
        "        # Common OCR errors\n",
        "        'เเ': 'แ',\n",
        "        'ํา': 'ำ',\n",
        "        'ํ า': 'ำ',\n",
        "        '  ': ' ',\n",
        "        '   ': ' ',\n",
        "        '\\t': ' ',\n",
        "\n",
        "        # Punctuation fixes\n",
        "        ' ๆ ': 'ๆ ',\n",
        "        'ๆ ': 'ๆ',\n",
        "        ' ๆ': 'ๆ',\n",
        "        ' \"': '\"',\n",
        "        '\" ': '\"',\n",
        "        ' ,': ',',\n",
        "        ' .': '.',\n",
        "\n",
        "        # Common Thai novel terms\n",
        "        'พวกเขๅ': 'พวกเขา',\n",
        "        'ทํา': 'ทำ',\n",
        "        'จๅก': 'จาก',\n",
        "        'ดู่': 'ดู',\n",
        "    }\n",
        "\n",
        "    SUSPICIOUS_PATTERNS = [\n",
        "        r'^[ก-ฮ]$',\n",
        "        r'^[a-zA-Z]$',\n",
        "        r'^.{1,2}$',\n",
        "        r'^\\d+$',\n",
        "    ]\n",
        "\n",
        "print(\"✅ Config loaded\")"
      ],
      "metadata": {
        "id": "UYQvoF4xncXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 3: Novel Text Analyzer\n",
        "# ============================================\n",
        "class NovelTextAnalyzer:\n",
        "    \"\"\"วิเคราะห์และแก้ปัญหา OCR สำหรับนิยายไทย\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def is_dialogue(text: str) -> bool:\n",
        "        dialogue_patterns = [\n",
        "            r'^\".*\"',\n",
        "            r'\".*\"$',\n",
        "            r'\".*\".*กล่าว',\n",
        "            r'\".*\".*พูด',\n",
        "            r'\".*\".*ตอบ',\n",
        "            r'\".*\".*ถาม',\n",
        "            r'\".*\".*ร้อง',\n",
        "            r'\".*\".*บ่น',\n",
        "        ]\n",
        "        for pattern in dialogue_patterns:\n",
        "            if re.search(pattern, text):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def is_incomplete_line(text: str) -> bool:\n",
        "        if len(text) < Config.MIN_LINE_LENGTH:\n",
        "            return True\n",
        "        for pattern in Config.SUSPICIOUS_PATTERNS:\n",
        "            if re.match(pattern, text.strip()):\n",
        "                return True\n",
        "        thai_vowels = 'ะาิีึืุูเแโใไ็่้๊๋ำ'\n",
        "        if not any(v in text for v in thai_vowels):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def should_merge_lines(prev_line: str, curr_line: str) -> bool:\n",
        "        if prev_line and not prev_line[-1] in '.!? ':\n",
        "            if not NovelTextAnalyzer.is_dialogue(curr_line):\n",
        "                if not curr_line.startswith(('  ', '\\t')):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_broken_words(text: str) -> str:\n",
        "        lines = text.split('\\n')\n",
        "        fixed_lines = []\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            curr_line = lines[i].strip()\n",
        "            if NovelTextAnalyzer.is_incomplete_line(curr_line):\n",
        "                if i > 0 and fixed_lines:\n",
        "                    prev = fixed_lines[-1]\n",
        "                    if not prev.endswith(('.', '!', '?', '\"')):\n",
        "                        fixed_lines[-1] = prev + curr_line\n",
        "                        i += 1\n",
        "                        continue\n",
        "                if i < len(lines) - 1:\n",
        "                    next_line = lines[i + 1].strip()\n",
        "                    if not NovelTextAnalyzer.is_dialogue(next_line):\n",
        "                        fixed_lines.append(curr_line + next_line)\n",
        "                        i += 2\n",
        "                        continue\n",
        "            if curr_line:\n",
        "                fixed_lines.append(curr_line)\n",
        "            i += 1\n",
        "        return '\\n'.join(fixed_lines)\n",
        "\n",
        "# ============================================\n",
        "# 📌 Block 4: Enhanced BatchPreparer\n",
        "# ============================================\n",
        "class EnhancedBatchPreparer:\n",
        "    \"\"\"Enhanced batch preparer สำหรับนิยายไทย\"\"\"\n",
        "\n",
        "    def __init__(self, input_folder=None, output_folder=None):\n",
        "        self.input_folder = Path(input_folder or Config.RAW_OCR_DIR)\n",
        "        self.output_folder = Path(output_folder or Config.BATCHES_DIR)\n",
        "        self.input_folder.mkdir(exist_ok=True)\n",
        "        self.output_folder.mkdir(exist_ok=True)\n",
        "        self.analyzer = NovelTextAnalyzer()\n",
        "\n",
        "    def pre_clean_text(self, text: str) -> str:\n",
        "        for old, new in Config.OCR_REPLACEMENTS.items():\n",
        "            text = text.replace(old, new)\n",
        "        text = self.analyzer.fix_broken_words(text)\n",
        "        text = self._smart_paragraph_split(text)\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "        text = re.sub(r' {2,}', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _smart_paragraph_split(self, text: str) -> str:\n",
        "        lines = text.split('\\n')\n",
        "        paragraphs = []\n",
        "        current_para = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current_para:\n",
        "                    paragraphs.append(' '.join(current_para))\n",
        "                    current_para = []\n",
        "                continue\n",
        "\n",
        "            if self.analyzer.is_dialogue(line):\n",
        "                if current_para and not self.analyzer.is_dialogue(current_para[-1]):\n",
        "                    paragraphs.append(' '.join(current_para))\n",
        "                    current_para = [line]\n",
        "                else:\n",
        "                    current_para.append(line)\n",
        "            else:\n",
        "                if i > 0 and current_para:\n",
        "                    if self.analyzer.should_merge_lines(current_para[-1], line):\n",
        "                        current_para.append(line)\n",
        "                    else:\n",
        "                        paragraphs.append(' '.join(current_para))\n",
        "                        current_para = [line]\n",
        "                else:\n",
        "                    current_para.append(line)\n",
        "\n",
        "        if current_para:\n",
        "            paragraphs.append(' '.join(current_para))\n",
        "        return '\\n'.join(paragraphs)\n",
        "\n",
        "    def create_enhanced_prompt(self, batch_text: str) -> str:\n",
        "        prompt = f\"\"\"กรุณาแก้ไขข้อความ OCR จากนิยายภาษาไทยต่อไปนี้\n",
        "\n",
        "กฎการแก้ไข:\n",
        "1. แก้คำผิด typo และการสะกดผิด\n",
        "2. แก้คำที่ขาดหาย/แตกหัก\n",
        "3. ลบตัวอักษรเดี่ยวๆ ที่ไม่มีความหมาย\n",
        "4. รักษารูปแบบบทสนทนา (คำพูดในเครื่องหมาย \"...\")\n",
        "5. จัด paragraph ให้เหมาะสม\n",
        "6. คงรูปแบบ markers [PAGE_XXX] และ [END_PAGE_XXX] ไว้ทุกตัว\n",
        "7. ห้ามเพิ่มเนื้อหาที่ไม่มีในต้นฉบับ\n",
        "\n",
        "ข้อความที่ต้องแก้:\n",
        "\n",
        "{batch_text}\n",
        "\n",
        "กรุณาแก้ไขแล้วคืนข้อความทั้งหมดพร้อม markers\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def create_batch(self, max_pages: int = None) -> Tuple[str, int]:\n",
        "        max_pages = max_pages or Config.MAX_PAGES_PER_BATCH\n",
        "        files = sorted(self.input_folder.glob(\"*.txt\"))[:max_pages]\n",
        "\n",
        "        if not files:\n",
        "            print(\"❌ ไม่พบไฟล์ใน folder raw_ocr/\")\n",
        "            return \"\", 0\n",
        "\n",
        "        batch_parts = [\"[START_BATCH]\"]\n",
        "        stats = {'total_lines': 0, 'suspicious_lines': 0, 'merged_lines': 0}\n",
        "\n",
        "        for i, file_path in enumerate(files, 1):\n",
        "            try:\n",
        "                text = file_path.read_text(encoding='utf-8')\n",
        "                original_lines = len(text.split('\\n'))\n",
        "                cleaned_text = self.pre_clean_text(text)\n",
        "                cleaned_lines = len(cleaned_text.split('\\n'))\n",
        "                stats['total_lines'] += original_lines\n",
        "                stats['merged_lines'] += (original_lines - cleaned_lines)\n",
        "\n",
        "                page_marker = f\"[PAGE_{i:03d}]\"\n",
        "                end_marker = f\"[END_PAGE_{i:03d}]\"\n",
        "                batch_parts.append(f\"\\n{page_marker}\\n{cleaned_text}\\n{end_marker}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error reading {file_path.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        batch_parts.append(\"\\n[END_BATCH]\")\n",
        "        batch_text = ''.join(batch_parts)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        batch_file = self.output_folder / f\"batch_{timestamp}.txt\"\n",
        "        batch_file.write_text(batch_text, encoding='utf-8')\n",
        "\n",
        "        print(f\"✅ สร้าง batch สำเร็จ: {batch_file.name}\")\n",
        "        print(f\"   📄 จำนวน: {len(files)} หน้า\")\n",
        "        print(f\"   💾 ขนาด: ~{len(batch_text.split())} คำ\")\n",
        "\n",
        "        return batch_text, len(files)\n",
        "\n",
        "    def prepare_and_copy(self, max_pages: int = None):\n",
        "        batch_text, page_count = self.create_batch(max_pages)\n",
        "        if page_count == 0:\n",
        "            return\n",
        "\n",
        "        prompt = self.create_enhanced_prompt(batch_text)\n",
        "        estimated_tokens = len(prompt) // 2\n",
        "\n",
        "        prompt_file = self.output_folder / \"latest_prompt.txt\"\n",
        "        prompt_file.write_text(prompt, encoding='utf-8')\n",
        "        print(f\"💾 บันทึก prompt ไว้ที่: {prompt_file}\")\n",
        "        print(f\"📊 ประมาณ {estimated_tokens:,} tokens\")\n",
        "        print(f\"\\n📝 ขั้นตอนต่อไป:\")\n",
        "        print(\"   1. เปิดไฟล์ prompt ใน Drive\")\n",
        "        print(\"   2. Copy ไปใส่ ChatGPT/Claude\")\n",
        "        print(\"   3. Copy ผลลัพธ์กลับมา\")\n",
        "        print(\"   4. Run menu option 2\")\n",
        "\n",
        "print(\"✅ Batch Preparer ready\")"
      ],
      "metadata": {
        "id": "zjkMXNW0nklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 5: Quality Validator\n",
        "# ============================================\n",
        "class QualityValidator:\n",
        "    @staticmethod\n",
        "    def validate_text(original: str, cleaned: str) -> Dict:\n",
        "        issues = []\n",
        "        len_ratio = len(cleaned) / len(original) if len(original) > 0 else 0\n",
        "        if len_ratio < 0.5:\n",
        "            issues.append(\"⚠️ ข้อความสั้นลงมาก\")\n",
        "        elif len_ratio > 1.5:\n",
        "            issues.append(\"⚠️ ข้อความยาวขึ้นมาก\")\n",
        "\n",
        "        orig_quotes = len(re.findall(r'\"[^\"]*\"', original))\n",
        "        clean_quotes = len(re.findall(r'\"[^\"]*\"', cleaned))\n",
        "        if abs(orig_quotes - clean_quotes) > 2:\n",
        "            issues.append(f\"⚠️ จำนวนบทสนทนาต่างกัน ({orig_quotes} -> {clean_quotes})\")\n",
        "\n",
        "        return {\n",
        "            'valid': len(issues) == 0,\n",
        "            'issues': issues,\n",
        "            'stats': {\n",
        "                'length_ratio': len_ratio,\n",
        "                'dialogue_count': clean_quotes,\n",
        "            }\n",
        "        }\n",
        "\n",
        "# ============================================\n",
        "# 📌 Block 6: Enhanced Result Parser\n",
        "# ============================================\n",
        "class EnhancedResultParser:\n",
        "    def __init__(self, output_folder=None, report_folder=None):\n",
        "        llm_type = input(\"ผลลัพธ์จาก [1] GPT หรือ [2] Claude? (1/2): \").strip()\n",
        "\n",
        "        if llm_type == '2':\n",
        "            self.output_folder = Path(output_folder or Config.CLEANED_CLAUDE_DIR)\n",
        "        else:\n",
        "            self.output_folder = Path(output_folder or Config.CLEANED_GPT_DIR)\n",
        "\n",
        "        self.report_folder = Path(report_folder or Config.REPORTS_DIR)\n",
        "        self.output_folder.mkdir(exist_ok=True)\n",
        "        self.report_folder.mkdir(exist_ok=True)\n",
        "        self.validator = QualityValidator()\n",
        "\n",
        "    def extract_pages(self, llm_output: str) -> Dict[int, str]:\n",
        "        pages = {}\n",
        "        pattern = r'\\[PAGE_(\\d{3})\\](.*?)\\[END_PAGE_\\1\\]'\n",
        "        matches = re.findall(pattern, llm_output, re.DOTALL)\n",
        "        for page_num, content in matches:\n",
        "            page_number = int(page_num)\n",
        "            pages[page_number] = content.strip()\n",
        "        return pages\n",
        "\n",
        "    def save_pages(self, pages: Dict[int, str]) -> int:\n",
        "        saved = 0\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        for page_num, content in pages.items():\n",
        "            filename = f\"page_{page_num:03d}_clean_{timestamp}.txt\"\n",
        "            filepath = self.output_folder / filename\n",
        "            try:\n",
        "                filepath.write_text(content, encoding='utf-8')\n",
        "                saved += 1\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error saving {filename}: {e}\")\n",
        "        return saved\n",
        "\n",
        "    def parse_from_clipboard(self):\n",
        "        llm_output = \"\"\n",
        "\n",
        "        # Try reading from file\n",
        "        print(\"📝 กรุณา paste ผลลัพธ์ในไฟล์ 'llm_output.txt'\")\n",
        "        output_file = Path(\"llm_output.txt\")\n",
        "        if output_file.exists():\n",
        "            llm_output = output_file.read_text(encoding='utf-8')\n",
        "        else:\n",
        "            print(\"❌ ไม่พบไฟล์ llm_output.txt\")\n",
        "            return\n",
        "\n",
        "        print(f\"📋 รับข้อความ {len(llm_output)} characters\")\n",
        "        pages = self.extract_pages(llm_output)\n",
        "\n",
        "        if not pages:\n",
        "            print(\"❌ ไม่พบ page markers\")\n",
        "            return\n",
        "\n",
        "        print(f\"✅ พบ {len(pages)} หน้า\")\n",
        "        saved = self.save_pages(pages)\n",
        "        print(f\"💾 บันทึก {saved} ไฟล์ไปที่ {self.output_folder}/\")\n",
        "\n",
        "print(\"✅ Parser ready\")"
      ],
      "metadata": {
        "id": "TtV9LKiLnqV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 7: Main Menu\n",
        "# ============================================\n",
        "def main_menu():\n",
        "    \"\"\"Interactive main menu\"\"\"\n",
        "\n",
        "    while True:\n",
        "        print(\"\"\"\n",
        "╔════════════════════════════════════════════╗\n",
        "║   ENHANCED OCR SCRIPTS - THAI NOVEL v2.0   ║\n",
        "╚════════════════════════════════════════════╝\n",
        "\n",
        "[1] 📦 Prepare Batch - รวมไฟล์พร้อม smart cleaning\n",
        "[2] 📋 Parse Results - แยกผลพร้อม quality check\n",
        "[3] 🔍 Compare Versions - เปรียบเทียบ GPT vs Claude\n",
        "[4] 📊 Show Statistics - ดูสถิติ corpus\n",
        "[5] 🔧 Test Analyzer - ทดสอบ text analyzer\n",
        "[6] ❌ Exit\n",
        "\n",
        "        \"\"\")\n",
        "\n",
        "        choice = input(\"Select (1-6): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            print(\"\\n🚀 Running Enhanced Batch Preparer...\")\n",
        "            print(\"-\" * 40)\n",
        "            preparer = EnhancedBatchPreparer()\n",
        "            preparer.prepare_and_copy()\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"\\n🚀 Running Enhanced Result Parser...\")\n",
        "            print(\"-\" * 40)\n",
        "            parser = EnhancedResultParser()\n",
        "            parser.parse_from_clipboard()\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"🔍 Compare feature - Coming soon!\")\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            print(\"\\n📊 Statistics:\")\n",
        "            raw = len(list(Path(Config.RAW_OCR_DIR).glob(\"*.txt\")))\n",
        "            print(f\"Raw OCR files: {raw}\")\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"🔧 Test Analyzer - Coming soon!\")\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '6':\n",
        "            print(\"\\n👋 Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Invalid choice\")\n",
        "\n",
        "print(\"✅ Main menu ready!\")\n",
        "print(\"\\n🎯 Run: main_menu() to start\")"
      ],
      "metadata": {
        "id": "2rW7GI64ntaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ยินดีต้อนรับสู่ Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}