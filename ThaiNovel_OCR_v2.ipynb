{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/ThaiNovel_OCR_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell à¹ƒà¸«à¸¡à¹ˆ: Mount Drive + Create Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡ folders à¹ƒà¸™ Drive\n",
        "import os\n",
        "BASE = '/content/drive/MyDrive/OCR'  # â† à¸Šà¸·à¹ˆà¸­ folder à¸‚à¸­à¸‡à¸„à¸¸à¸“\n",
        "\n",
        "for folder in ['raw_ocr', 'batches', 'cleaned_gpt', 'cleaned_claude', 'final_corpus', 'reports', 'training_pairs']:\n",
        "    os.makedirs(f'{BASE}/{folder}', exist_ok=True)\n",
        "\n",
        "print(\"âœ… Folders ready in Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm_K-lJ9hX8B",
        "outputId": "be421778-cc0f-41c7-80d9-6eb7da97e4b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Folders ready in Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check pyperclip\n",
        "try:\n",
        "    import pyperclip\n",
        "    CLIPBOARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIPBOARD_AVAILABLE = False\n",
        "    print(\"âš ï¸ pyperclip not installed - à¸ˆà¸°à¹ƒà¸Šà¹‰à¹„à¸Ÿà¸¥à¹Œà¹à¸—à¸™ clipboard\")\n",
        "\n",
        "print(\"âœ… Libraries loaded\")"
      ],
      "metadata": {
        "id": "6LyJqyOoc52Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d1f15c-822e-41cf-ea4e-f6e1cafe2b02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import pyperclip\n",
        "    CLIPBOARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIPBOARD_AVAILABLE = False\n",
        "    print(\"âš ï¸ pyperclip not installed - à¸ˆà¸°à¹ƒà¸Šà¹‰à¹„à¸Ÿà¸¥à¹Œà¹à¸—à¸™ clipboard\")\n",
        "\n",
        "print(\"âœ… Libraries loaded\")\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ Block 2: Enhanced Configuration\n",
        "# ============================================\n",
        "class Config:\n",
        "    \"\"\"Configuration à¸ªà¸³à¸«à¸£à¸±à¸š OCR Processing - Thai Novel Optimized\"\"\"\n",
        "\n",
        "    # Paths - à¸Šà¸µà¹‰à¹„à¸›à¸—à¸µà¹ˆ Drive\n",
        "    BASE = '/content/drive/MyDrive/OCR'\n",
        "\n",
        "    RAW_OCR_DIR = f'{BASE}/raw_ocr'\n",
        "    BATCHES_DIR = f'{BASE}/batches'\n",
        "    CLEANED_GPT_DIR = f'{BASE}/cleaned_gpt'\n",
        "    CLEANED_CLAUDE_DIR = f'{BASE}/cleaned_claude'\n",
        "    FINAL_DIR = f'{BASE}/final_corpus'\n",
        "    REPORTS_DIR = f'{BASE}/reports'\n",
        "    TRAINING_PAIRS_DIR = f'{BASE}/training_pairs'\n",
        "\n",
        "    # Processing parameters\n",
        "    MAX_PAGES_PER_BATCH = 20\n",
        "    MIN_LINE_LENGTH = 3\n",
        "\n",
        "    # Enhanced OCR replacements for Thai novels\n",
        "    OCR_REPLACEMENTS = {\n",
        "        # Common OCR errors\n",
        "        'à¹€à¹€': 'à¹',\n",
        "        'à¹à¸²': 'à¸³',\n",
        "        'à¹ à¸²': 'à¸³',\n",
        "        '  ': ' ',\n",
        "        '   ': ' ',\n",
        "        '\\t': ' ',\n",
        "\n",
        "        # Punctuation fixes\n",
        "        ' à¹† ': 'à¹† ',\n",
        "        'à¹† ': 'à¹†',\n",
        "        ' à¹†': 'à¹†',\n",
        "        ' \"': '\"',\n",
        "        '\" ': '\"',\n",
        "        ' ,': ',',\n",
        "        ' .': '.',\n",
        "\n",
        "        # Common Thai novel terms\n",
        "        'à¸à¸§à¸à¹€à¸‚à¹…': 'à¸à¸§à¸à¹€à¸‚à¸²',\n",
        "        'à¸—à¹à¸²': 'à¸—à¸³',\n",
        "        'à¸ˆà¹…à¸': 'à¸ˆà¸²à¸',\n",
        "        'à¸”à¸¹à¹ˆ': 'à¸”à¸¹',\n",
        "    }\n",
        "\n",
        "    SUSPICIOUS_PATTERNS = [\n",
        "        r'^[à¸-à¸®]$',\n",
        "        r'^[a-zA-Z]$',\n",
        "        r'^.{1,2}$',\n",
        "        r'^\\d+$',\n",
        "    ]\n",
        "\n",
        "print(\"âœ… Config loaded\")"
      ],
      "metadata": {
        "id": "UYQvoF4xncXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ Block 3: Novel Text Analyzer\n",
        "# ============================================\n",
        "class NovelTextAnalyzer:\n",
        "    \"\"\"à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸¥à¸°à¹à¸à¹‰à¸›à¸±à¸à¸«à¸² OCR à¸ªà¸³à¸«à¸£à¸±à¸šà¸™à¸´à¸¢à¸²à¸¢à¹„à¸—à¸¢\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def is_dialogue(text: str) -> bool:\n",
        "        dialogue_patterns = [\n",
        "            r'^\".*\"',\n",
        "            r'\".*\"$',\n",
        "            r'\".*\".*à¸à¸¥à¹ˆà¸²à¸§',\n",
        "            r'\".*\".*à¸à¸¹à¸”',\n",
        "            r'\".*\".*à¸•à¸­à¸š',\n",
        "            r'\".*\".*à¸–à¸²à¸¡',\n",
        "            r'\".*\".*à¸£à¹‰à¸­à¸‡',\n",
        "            r'\".*\".*à¸šà¹ˆà¸™',\n",
        "        ]\n",
        "        for pattern in dialogue_patterns:\n",
        "            if re.search(pattern, text):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def is_incomplete_line(text: str) -> bool:\n",
        "        if len(text) < Config.MIN_LINE_LENGTH:\n",
        "            return True\n",
        "        for pattern in Config.SUSPICIOUS_PATTERNS:\n",
        "            if re.match(pattern, text.strip()):\n",
        "                return True\n",
        "        thai_vowels = 'à¸°à¸²à¸´à¸µà¸¶à¸·à¸¸à¸¹à¹€à¹à¹‚à¹ƒà¹„à¹‡à¹ˆà¹‰à¹Šà¹‹à¸³'\n",
        "        if not any(v in text for v in thai_vowels):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def should_merge_lines(prev_line: str, curr_line: str) -> bool:\n",
        "        if prev_line and not prev_line[-1] in '.!? ':\n",
        "            if not NovelTextAnalyzer.is_dialogue(curr_line):\n",
        "                if not curr_line.startswith(('  ', '\\t')):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_broken_words(text: str) -> str:\n",
        "        lines = text.split('\\n')\n",
        "        fixed_lines = []\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            curr_line = lines[i].strip()\n",
        "            if NovelTextAnalyzer.is_incomplete_line(curr_line):\n",
        "                if i > 0 and fixed_lines:\n",
        "                    prev = fixed_lines[-1]\n",
        "                    if not prev.endswith(('.', '!', '?', '\"')):\n",
        "                        fixed_lines[-1] = prev + curr_line\n",
        "                        i += 1\n",
        "                        continue\n",
        "                if i < len(lines) - 1:\n",
        "                    next_line = lines[i + 1].strip()\n",
        "                    if not NovelTextAnalyzer.is_dialogue(next_line):\n",
        "                        fixed_lines.append(curr_line + next_line)\n",
        "                        i += 2\n",
        "                        continue\n",
        "            if curr_line:\n",
        "                fixed_lines.append(curr_line)\n",
        "            i += 1\n",
        "        return '\\n'.join(fixed_lines)\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ Block 4: Enhanced BatchPreparer\n",
        "# ============================================\n",
        "class EnhancedBatchPreparer:\n",
        "    \"\"\"Enhanced batch preparer à¸ªà¸³à¸«à¸£à¸±à¸šà¸™à¸´à¸¢à¸²à¸¢à¹„à¸—à¸¢\"\"\"\n",
        "\n",
        "    def __init__(self, input_folder=None, output_folder=None):\n",
        "        self.input_folder = Path(input_folder or Config.RAW_OCR_DIR)\n",
        "        self.output_folder = Path(output_folder or Config.BATCHES_DIR)\n",
        "        self.input_folder.mkdir(exist_ok=True)\n",
        "        self.output_folder.mkdir(exist_ok=True)\n",
        "        self.analyzer = NovelTextAnalyzer()\n",
        "\n",
        "    def pre_clean_text(self, text: str) -> str:\n",
        "        for old, new in Config.OCR_REPLACEMENTS.items():\n",
        "            text = text.replace(old, new)\n",
        "        text = self.analyzer.fix_broken_words(text)\n",
        "        text = self._smart_paragraph_split(text)\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "        text = re.sub(r' {2,}', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _smart_paragraph_split(self, text: str) -> str:\n",
        "        lines = text.split('\\n')\n",
        "        paragraphs = []\n",
        "        current_para = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current_para:\n",
        "                    paragraphs.append(' '.join(current_para))\n",
        "                    current_para = []\n",
        "                continue\n",
        "\n",
        "            if self.analyzer.is_dialogue(line):\n",
        "                if current_para and not self.analyzer.is_dialogue(current_para[-1]):\n",
        "                    paragraphs.append(' '.join(current_para))\n",
        "                    current_para = [line]\n",
        "                else:\n",
        "                    current_para.append(line)\n",
        "            else:\n",
        "                if i > 0 and current_para:\n",
        "                    if self.analyzer.should_merge_lines(current_para[-1], line):\n",
        "                        current_para.append(line)\n",
        "                    else:\n",
        "                        paragraphs.append(' '.join(current_para))\n",
        "                        current_para = [line]\n",
        "                else:\n",
        "                    current_para.append(line)\n",
        "\n",
        "        if current_para:\n",
        "            paragraphs.append(' '.join(current_para))\n",
        "        return '\\n'.join(paragraphs)\n",
        "\n",
        "    def create_enhanced_prompt(self, batch_text: str) -> str:\n",
        "        prompt = f\"\"\"à¸à¸£à¸¸à¸“à¸²à¹à¸à¹‰à¹„à¸‚à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ OCR à¸ˆà¸²à¸à¸™à¸´à¸¢à¸²à¸¢à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰\n",
        "\n",
        "à¸à¸à¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚:\n",
        "1. à¹à¸à¹‰à¸„à¸³à¸œà¸´à¸” typo à¹à¸¥à¸°à¸à¸²à¸£à¸ªà¸°à¸à¸”à¸œà¸´à¸”\n",
        "2. à¹à¸à¹‰à¸„à¸³à¸—à¸µà¹ˆà¸‚à¸²à¸”à¸«à¸²à¸¢/à¹à¸•à¸à¸«à¸±à¸\n",
        "3. à¸¥à¸šà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¹€à¸”à¸µà¹ˆà¸¢à¸§à¹† à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢\n",
        "4. à¸£à¸±à¸à¸©à¸²à¸£à¸¹à¸›à¹à¸šà¸šà¸šà¸—à¸ªà¸™à¸—à¸™à¸² (à¸„à¸³à¸à¸¹à¸”à¹ƒà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸«à¸¡à¸²à¸¢ \"...\")\n",
        "5. à¸ˆà¸±à¸” paragraph à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡\n",
        "6. à¸„à¸‡à¸£à¸¹à¸›à¹à¸šà¸š markers [PAGE_XXX] à¹à¸¥à¸° [END_PAGE_XXX] à¹„à¸§à¹‰à¸—à¸¸à¸à¸•à¸±à¸§\n",
        "7. à¸«à¹‰à¸²à¸¡à¹€à¸à¸´à¹ˆà¸¡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸™à¸•à¹‰à¸™à¸‰à¸šà¸±à¸š\n",
        "\n",
        "à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹à¸à¹‰:\n",
        "\n",
        "{batch_text}\n",
        "\n",
        "à¸à¸£à¸¸à¸“à¸²à¹à¸à¹‰à¹„à¸‚à¹à¸¥à¹‰à¸§à¸„à¸·à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¸£à¹‰à¸­à¸¡ markers\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def create_batch(self, max_pages: int = None) -> Tuple[str, int]:\n",
        "        max_pages = max_pages or Config.MAX_PAGES_PER_BATCH\n",
        "        files = sorted(self.input_folder.glob(\"*.txt\"))[:max_pages]\n",
        "\n",
        "        if not files:\n",
        "            print(\"âŒ à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œà¹ƒà¸™ folder raw_ocr/\")\n",
        "            return \"\", 0\n",
        "\n",
        "        batch_parts = [\"[START_BATCH]\"]\n",
        "        stats = {'total_lines': 0, 'suspicious_lines': 0, 'merged_lines': 0}\n",
        "\n",
        "        for i, file_path in enumerate(files, 1):\n",
        "            try:\n",
        "                text = file_path.read_text(encoding='utf-8')\n",
        "                original_lines = len(text.split('\\n'))\n",
        "                cleaned_text = self.pre_clean_text(text)\n",
        "                cleaned_lines = len(cleaned_text.split('\\n'))\n",
        "                stats['total_lines'] += original_lines\n",
        "                stats['merged_lines'] += (original_lines - cleaned_lines)\n",
        "\n",
        "                page_marker = f\"[PAGE_{i:03d}]\"\n",
        "                end_marker = f\"[END_PAGE_{i:03d}]\"\n",
        "                batch_parts.append(f\"\\n{page_marker}\\n{cleaned_text}\\n{end_marker}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error reading {file_path.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        batch_parts.append(\"\\n[END_BATCH]\")\n",
        "        batch_text = ''.join(batch_parts)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        batch_file = self.output_folder / f\"batch_{timestamp}.txt\"\n",
        "        batch_file.write_text(batch_text, encoding='utf-8')\n",
        "\n",
        "        print(f\"âœ… à¸ªà¸£à¹‰à¸²à¸‡ batch à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {batch_file.name}\")\n",
        "        print(f\"   ğŸ“„ à¸ˆà¸³à¸™à¸§à¸™: {len(files)} à¸«à¸™à¹‰à¸²\")\n",
        "        print(f\"   ğŸ’¾ à¸‚à¸™à¸²à¸”: ~{len(batch_text.split())} à¸„à¸³\")\n",
        "\n",
        "        return batch_text, len(files)\n",
        "\n",
        "    def prepare_and_copy(self, max_pages: int = None):\n",
        "        batch_text, page_count = self.create_batch(max_pages)\n",
        "        if page_count == 0:\n",
        "            return\n",
        "\n",
        "        prompt = self.create_enhanced_prompt(batch_text)\n",
        "        estimated_tokens = len(prompt) // 2\n",
        "\n",
        "        prompt_file = self.output_folder / \"latest_prompt.txt\"\n",
        "        prompt_file.write_text(prompt, encoding='utf-8')\n",
        "        print(f\"ğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸ prompt à¹„à¸§à¹‰à¸—à¸µà¹ˆ: {prompt_file}\")\n",
        "        print(f\"ğŸ“Š à¸›à¸£à¸°à¸¡à¸²à¸“ {estimated_tokens:,} tokens\")\n",
        "        print(f\"\\nğŸ“ à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¹ˆà¸­à¹„à¸›:\")\n",
        "        print(\"   1. à¹€à¸›à¸´à¸”à¹„à¸Ÿà¸¥à¹Œ prompt à¹ƒà¸™ Drive\")\n",
        "        print(\"   2. Copy à¹„à¸›à¹ƒà¸ªà¹ˆ ChatGPT/Claude\")\n",
        "        print(\"   3. Copy à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸à¸¥à¸±à¸šà¸¡à¸²\")\n",
        "        print(\"   4. Run menu option 2\")\n",
        "\n",
        "print(\"âœ… Batch Preparer ready\")"
      ],
      "metadata": {
        "id": "zjkMXNW0nklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ Block 5: Quality Validator\n",
        "# ============================================\n",
        "class QualityValidator:\n",
        "    @staticmethod\n",
        "    def validate_text(original: str, cleaned: str) -> Dict:\n",
        "        issues = []\n",
        "        len_ratio = len(cleaned) / len(original) if len(original) > 0 else 0\n",
        "        if len_ratio < 0.5:\n",
        "            issues.append(\"âš ï¸ à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ªà¸±à¹‰à¸™à¸¥à¸‡à¸¡à¸²à¸\")\n",
        "        elif len_ratio > 1.5:\n",
        "            issues.append(\"âš ï¸ à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¸‚à¸¶à¹‰à¸™à¸¡à¸²à¸\")\n",
        "\n",
        "        orig_quotes = len(re.findall(r'\"[^\"]*\"', original))\n",
        "        clean_quotes = len(re.findall(r'\"[^\"]*\"', cleaned))\n",
        "        if abs(orig_quotes - clean_quotes) > 2:\n",
        "            issues.append(f\"âš ï¸ à¸ˆà¸³à¸™à¸§à¸™à¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™ ({orig_quotes} -> {clean_quotes})\")\n",
        "\n",
        "        return {\n",
        "            'valid': len(issues) == 0,\n",
        "            'issues': issues,\n",
        "            'stats': {\n",
        "                'length_ratio': len_ratio,\n",
        "                'dialogue_count': clean_quotes,\n",
        "            }\n",
        "        }\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ Block 6: Enhanced Result Parser\n",
        "# ============================================\n",
        "class EnhancedResultParser:\n",
        "    def __init__(self, output_folder=None, report_folder=None):\n",
        "        llm_type = input(\"à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ˆà¸²à¸ [1] GPT à¸«à¸£à¸·à¸­ [2] Claude? (1/2): \").strip()\n",
        "\n",
        "        if llm_type == '2':\n",
        "            self.output_folder = Path(output_folder or Config.CLEANED_CLAUDE_DIR)\n",
        "        else:\n",
        "            self.output_folder = Path(output_folder or Config.CLEANED_GPT_DIR)\n",
        "\n",
        "        self.report_folder = Path(report_folder or Config.REPORTS_DIR)\n",
        "        self.output_folder.mkdir(exist_ok=True)\n",
        "        self.report_folder.mkdir(exist_ok=True)\n",
        "        self.validator = QualityValidator()\n",
        "\n",
        "    def extract_pages(self, llm_output: str) -> Dict[int, str]:\n",
        "        pages = {}\n",
        "        pattern = r'\\[PAGE_(\\d{3})\\](.*?)\\[END_PAGE_\\1\\]'\n",
        "        matches = re.findall(pattern, llm_output, re.DOTALL)\n",
        "        for page_num, content in matches:\n",
        "            page_number = int(page_num)\n",
        "            pages[page_number] = content.strip()\n",
        "        return pages\n",
        "\n",
        "    def save_pages(self, pages: Dict[int, str]) -> int:\n",
        "        saved = 0\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        for page_num, content in pages.items():\n",
        "            filename = f\"page_{page_num:03d}_clean_{timestamp}.txt\"\n",
        "            filepath = self.output_folder / filename\n",
        "            try:\n",
        "                filepath.write_text(content, encoding='utf-8')\n",
        "                saved += 1\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error saving {filename}: {e}\")\n",
        "        return saved\n",
        "\n",
        "    def parse_from_clipboard(self):\n",
        "        llm_output = \"\"\n",
        "\n",
        "        # Try reading from file\n",
        "        print(\"ğŸ“ à¸à¸£à¸¸à¸“à¸² paste à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ 'llm_output.txt'\")\n",
        "        output_file = Path(\"llm_output.txt\")\n",
        "        if output_file.exists():\n",
        "            llm_output = output_file.read_text(encoding='utf-8')\n",
        "        else:\n",
        "            print(\"âŒ à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ llm_output.txt\")\n",
        "            return\n",
        "\n",
        "        print(f\"ğŸ“‹ à¸£à¸±à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ {len(llm_output)} characters\")\n",
        "        pages = self.extract_pages(llm_output)\n",
        "\n",
        "        if not pages:\n",
        "            print(\"âŒ à¹„à¸¡à¹ˆà¸à¸š page markers\")\n",
        "            return\n",
        "\n",
        "        print(f\"âœ… à¸à¸š {len(pages)} à¸«à¸™à¹‰à¸²\")\n",
        "        saved = self.save_pages(pages)\n",
        "        print(f\"ğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸ {saved} à¹„à¸Ÿà¸¥à¹Œà¹„à¸›à¸—à¸µà¹ˆ {self.output_folder}/\")\n",
        "\n",
        "print(\"âœ… Parser ready\")"
      ],
      "metadata": {
        "id": "TtV9LKiLnqV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ Block 7: Main Menu\n",
        "# ============================================\n",
        "def main_menu():\n",
        "    \"\"\"Interactive main menu\"\"\"\n",
        "\n",
        "    while True:\n",
        "        print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘   ENHANCED OCR SCRIPTS - THAI NOVEL v2.0   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "[1] ğŸ“¦ Prepare Batch - à¸£à¸§à¸¡à¹„à¸Ÿà¸¥à¹Œà¸à¸£à¹‰à¸­à¸¡ smart cleaning\n",
        "[2] ğŸ“‹ Parse Results - à¹à¸¢à¸à¸œà¸¥à¸à¸£à¹‰à¸­à¸¡ quality check\n",
        "[3] ğŸ” Compare Versions - à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š GPT vs Claude\n",
        "[4] ğŸ“Š Show Statistics - à¸”à¸¹à¸ªà¸–à¸´à¸•à¸´ corpus\n",
        "[5] ğŸ”§ Test Analyzer - à¸—à¸”à¸ªà¸­à¸š text analyzer\n",
        "[6] âŒ Exit\n",
        "\n",
        "        \"\"\")\n",
        "\n",
        "        choice = input(\"Select (1-6): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            print(\"\\nğŸš€ Running Enhanced Batch Preparer...\")\n",
        "            print(\"-\" * 40)\n",
        "            preparer = EnhancedBatchPreparer()\n",
        "            preparer.prepare_and_copy()\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"\\nğŸš€ Running Enhanced Result Parser...\")\n",
        "            print(\"-\" * 40)\n",
        "            parser = EnhancedResultParser()\n",
        "            parser.parse_from_clipboard()\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"ğŸ” Compare feature - Coming soon!\")\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            print(\"\\nğŸ“Š Statistics:\")\n",
        "            raw = len(list(Path(Config.RAW_OCR_DIR).glob(\"*.txt\")))\n",
        "            print(f\"Raw OCR files: {raw}\")\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"ğŸ”§ Test Analyzer - Coming soon!\")\n",
        "            input(\"\\nPress Enter to continue...\")\n",
        "\n",
        "        elif choice == '6':\n",
        "            print(\"\\nğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"âŒ Invalid choice\")\n",
        "\n",
        "print(\"âœ… Main menu ready!\")\n",
        "print(\"\\nğŸ¯ Run: main_menu() to start\")"
      ],
      "metadata": {
        "id": "2rW7GI64ntaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "à¸¢à¸´à¸™à¸”à¸µà¸•à¹‰à¸­à¸™à¸£à¸±à¸šà¸ªà¸¹à¹ˆ Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}