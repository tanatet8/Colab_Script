{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/ThaiNovel_OCR_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell ‡πÉ‡∏´‡∏°‡πà: Mount Drive + Create Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á folders ‡πÉ‡∏ô Drive\n",
        "import os\n",
        "BASE = '/content/drive/MyDrive/OCR'  # ‚Üê ‡∏ä‡∏∑‡πà‡∏≠ folder ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "\n",
        "for folder in ['raw_ocr', 'batches', 'cleaned_gpt', 'cleaned_claude', 'final_corpus', 'reports', 'training_pairs']:\n",
        "    os.makedirs(f'{BASE}/{folder}', exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Folders ready in Drive!\")"
      ],
      "metadata": {
        "id": "Fm_K-lJ9hX8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 1: Setup & Import\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import difflib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check pyperclip\n",
        "try:\n",
        "    import pyperclip\n",
        "    CLIPBOARD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CLIPBOARD_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è pyperclip not installed - ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ó‡∏ô clipboard\")\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")"
      ],
      "metadata": {
        "id": "6LyJqyOoc52Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 2: Enhanced Configuration\n",
        "# ============================================\n",
        "class Config:\n",
        "    \"\"\"Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR Processing - Thai Novel Optimized\"\"\"\n",
        "\n",
        "# ‚ö†Ô∏è ‡πÅ‡∏Å‡πâ paths ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ\n",
        "    BASE = '/content/drive/MyDrive/OCR'  # ‚Üê folder ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô Drive\n",
        "\n",
        "    # Paths\n",
        "    RAW_OCR_DIR = 'raw_ocr'\n",
        "    BATCHES_DIR = 'batches'\n",
        "    CLEANED_GPT_DIR = 'cleaned_gpt'\n",
        "    CLEANED_CLAUDE_DIR = 'cleaned_claude'\n",
        "    FINAL_DIR = 'final_corpus'\n",
        "    REPORTS_DIR = 'reports'\n",
        "    TRAINING_PAIRS_DIR = 'training_pairs'\n",
        "\n",
        "    # Processing parameters\n",
        "    MAX_PAGES_PER_BATCH = 20\n",
        "    MIN_LINE_LENGTH = 3  # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ú‡∏¥‡∏î\n",
        "\n",
        "    # Enhanced OCR replacements for Thai novels\n",
        "    OCR_REPLACEMENTS = {\n",
        "        # Common OCR errors\n",
        "        '‡πÄ‡πÄ': '‡πÅ',\n",
        "        '‡πç‡∏≤': '‡∏≥',\n",
        "        '‡πç ‡∏≤': '‡∏≥',\n",
        "        '  ': ' ',\n",
        "        '   ': ' ',\n",
        "        '\\t': ' ',\n",
        "\n",
        "        # Punctuation fixes\n",
        "        ' ‡πÜ ': '‡πÜ ',\n",
        "        '‡πÜ ': '‡πÜ',\n",
        "        ' ‡πÜ': '‡πÜ',\n",
        "        ' \"': '\"',\n",
        "        '\" ': '\"',\n",
        "        ' ,': ',',\n",
        "        ' .': '.',\n",
        "\n",
        "        # Common Thai novel terms\n",
        "        '‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡πÖ': '‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤',\n",
        "        '‡∏ó‡πç‡∏≤': '‡∏ó‡∏≥',\n",
        "        '‡∏à‡πÖ‡∏Å': '‡∏à‡∏≤‡∏Å',\n",
        "        '‡∏î‡∏π‡πà': '‡∏î‡∏π',\n",
        "    }\n",
        "\n",
        "    # Suspicious patterns (‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô OCR error)\n",
        "    SUSPICIOUS_PATTERNS = [\n",
        "        r'^[‡∏Å-‡∏Æ]$',  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß\n",
        "        r'^[a-zA-Z]$',  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß\n",
        "        r'^.{1,2}$',  # ‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å (1-2 ‡∏ï‡∏±‡∏ß)\n",
        "        r'^\\d+$',  # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "    ]\n",
        "\n",
        "print(\"‚úÖ Enhanced Config loaded\")"
      ],
      "metadata": {
        "id": "pGT5oNZgc_Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 3: Novel Text Analyzer\n",
        "# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢\n",
        "# ============================================\n",
        "class NovelTextAnalyzer:\n",
        "    \"\"\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ OCR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def is_dialogue(text: str) -> bool:\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\"\"\"\n",
        "        dialogue_patterns = [\n",
        "            r'^\".*\"',  # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ \"\n",
        "            r'\".*\"$',  # ‡∏°‡∏µ quote\n",
        "            r'\".*\".*‡∏Å‡∏•‡πà‡∏≤‡∏ß',  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏Å‡∏•‡πà‡∏≤‡∏ß\n",
        "            r'\".*\".*‡∏û‡∏π‡∏î',  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏û‡∏π‡∏î\n",
        "            r'\".*\".*‡∏ï‡∏≠‡∏ö',  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏ï‡∏≠‡∏ö\n",
        "            r'\".*\".*‡∏ñ‡∏≤‡∏°',  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏ñ‡∏≤‡∏°\n",
        "            r'\".*\".*‡∏£‡πâ‡∏≠‡∏á',  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏£‡πâ‡∏≠‡∏á\n",
        "            r'\".*\".*‡∏ö‡πà‡∏ô',  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏ö‡πà‡∏ô\n",
        "        ]\n",
        "\n",
        "        for pattern in dialogue_patterns:\n",
        "            if re.search(pattern, text):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def is_incomplete_line(text: str) -> bool:\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\"\"\"\n",
        "        # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\n",
        "        if len(text) < Config.MIN_LINE_LENGTH:\n",
        "            return True\n",
        "\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö patterns ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢\n",
        "        for pattern in Config.SUSPICIOUS_PATTERNS:\n",
        "            if re.match(pattern, text.strip()):\n",
        "                return True\n",
        "\n",
        "        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏£‡∏∞‡πÄ‡∏•‡∏¢ ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ú‡∏¥‡∏î\n",
        "        thai_vowels = '‡∏∞‡∏≤‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πá‡πà‡πâ‡πä‡πã‡∏≥'\n",
        "        if not any(v in text for v in thai_vowels):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def should_merge_lines(prev_line: str, curr_line: str) -> bool:\n",
        "        \"\"\"‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏£‡∏ß‡∏° 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\"\"\"\n",
        "        # ‡∏ñ‡πâ‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ\n",
        "        if prev_line and not prev_line[-1] in '.!? ':\n",
        "            # ‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà\n",
        "            if not NovelTextAnalyzer.is_dialogue(curr_line):\n",
        "                # ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà paragraph ‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ)\n",
        "                if not curr_line.startswith(('  ', '\\t')):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_broken_words(text: str) -> str:\n",
        "        \"\"\"‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å\"\"\"\n",
        "        # Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÅ‡∏ï‡∏Å\n",
        "        lines = text.split('\\n')\n",
        "        fixed_lines = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            curr_line = lines[i].strip()\n",
        "\n",
        "            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢\n",
        "            if NovelTextAnalyzer.is_incomplete_line(curr_line):\n",
        "                # ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏´‡∏°\n",
        "                if i > 0 and fixed_lines:\n",
        "                    # ‡∏•‡∏≠‡∏á‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
        "                    prev = fixed_lines[-1]\n",
        "                    if not prev.endswith(('.', '!', '?', '\"')):\n",
        "                        fixed_lines[-1] = prev + curr_line\n",
        "                        i += 1\n",
        "                        continue\n",
        "\n",
        "                if i < len(lines) - 1:\n",
        "                    # ‡∏•‡∏≠‡∏á‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
        "                    next_line = lines[i + 1].strip()\n",
        "                    if not NovelTextAnalyzer.is_dialogue(next_line):\n",
        "                        fixed_lines.append(curr_line + next_line)\n",
        "                        i += 2\n",
        "                        continue\n",
        "\n",
        "            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏° ‡∏Å‡πá‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏Å‡∏ï‡∏¥\n",
        "            if curr_line:  # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á\n",
        "                fixed_lines.append(curr_line)\n",
        "            i += 1\n",
        "\n",
        "        return '\\n'.join(fixed_lines)\n",
        "\n",
        "print(\"‚úÖ NovelTextAnalyzer ready\")"
      ],
      "metadata": {
        "id": "aqOnSp4IdFUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 4: Enhanced BatchPreparer\n",
        "# ============================================\n",
        "class EnhancedBatchPreparer:\n",
        "    \"\"\"Enhanced batch preparer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢\"\"\"\n",
        "\n",
        "    def __init__(self, input_folder=None, output_folder=None):\n",
        "        self.input_folder = Path(input_folder or Config.RAW_OCR_DIR)\n",
        "        self.output_folder = Path(output_folder or Config.BATCHES_DIR)\n",
        "        self.input_folder.mkdir(exist_ok=True)\n",
        "        self.output_folder.mkdir(exist_ok=True)\n",
        "        self.analyzer = NovelTextAnalyzer()\n",
        "\n",
        "    def pre_clean_text(self, text: str) -> str:\n",
        "        \"\"\"‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î OCR text ‡πÅ‡∏ö‡∏ö enhanced\"\"\"\n",
        "\n",
        "        # Step 1: Basic replacements\n",
        "        for old, new in Config.OCR_REPLACEMENTS.items():\n",
        "            text = text.replace(old, new)\n",
        "\n",
        "        # Step 2: Fix broken words\n",
        "        text = self.analyzer.fix_broken_words(text)\n",
        "\n",
        "        # Step 3: Smart paragraph detection\n",
        "        text = self._smart_paragraph_split(text)\n",
        "\n",
        "        # Step 4: Clean up spacing\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)  # ‡∏•‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô\n",
        "        text = re.sub(r' {2,}', ' ', text)  # ‡∏•‡∏î space ‡∏ã‡πâ‡∏≥\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _smart_paragraph_split(self, text: str) -> str:\n",
        "        \"\"\"‡πÅ‡∏ö‡πà‡∏á paragraph ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î\"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        paragraphs = []\n",
        "        current_para = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "\n",
        "            if not line:\n",
        "                # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á = ‡∏à‡∏ö paragraph\n",
        "                if current_para:\n",
        "                    paragraphs.append(' '.join(current_para))\n",
        "                    current_para = []\n",
        "                continue\n",
        "\n",
        "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "            if self.analyzer.is_dialogue(line):\n",
        "                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ paragraph ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡∏à‡∏ö‡∏°‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô\n",
        "                if current_para and not self.analyzer.is_dialogue(current_para[-1]):\n",
        "                    paragraphs.append(' '.join(current_para))\n",
        "                    current_para = [line]\n",
        "                else:\n",
        "                    current_para.append(line)\n",
        "            else:\n",
        "                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "                if i > 0 and current_para:\n",
        "                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏´‡∏°\n",
        "                    if self.analyzer.should_merge_lines(current_para[-1], line):\n",
        "                        current_para.append(line)\n",
        "                    else:\n",
        "                        # ‡πÄ‡∏£‡∏¥‡πà‡∏° paragraph ‡πÉ‡∏´‡∏°‡πà\n",
        "                        paragraphs.append(' '.join(current_para))\n",
        "                        current_para = [line]\n",
        "                else:\n",
        "                    current_para.append(line)\n",
        "\n",
        "        # ‡πÄ‡∏û‡∏¥‡πà‡∏° paragraph ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
        "        if current_para:\n",
        "            paragraphs.append(' '.join(current_para))\n",
        "\n",
        "        return '\\n'.join(paragraphs)\n",
        "\n",
        "    def create_enhanced_prompt(self, batch_text: str) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢\"\"\"\n",
        "        prompt = f\"\"\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° OCR ‡∏à‡∏≤‡∏Å‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n",
        "\n",
        "‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:\n",
        "1. ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î typo ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î\n",
        "2. ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢/‡πÅ‡∏ï‡∏Å‡∏´‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô \"‡∏à‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á\" \"‡∏õ‡∏£‡∏∞‡∏ï‡∏π\" \"‡∏≤‡∏ô‡∏ä‡∏≥\" ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)\n",
        "3. ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Å, ‡∏õ, ‡∏á, T)\n",
        "4. ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ \"...\")\n",
        "5. ‡∏à‡∏±‡∏î paragraph ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° - ‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏¢‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î, ‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô paragraph\n",
        "6. ‡∏Ñ‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö markers [PAGE_XXX] ‡πÅ‡∏•‡∏∞ [END_PAGE_XXX] ‡πÑ‡∏ß‡πâ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß\n",
        "7. ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö\n",
        "\n",
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ:\n",
        "‚ùå OCR ‡∏ú‡∏¥‡∏î:\n",
        "\"‡∏à‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "‡∏õ‡∏£‡∏∞‡∏ï‡∏π\n",
        "‡∏≤‡∏ô‡∏ä‡∏≥\n",
        "‡∏Å\n",
        "‡πÑ‡∏´‡∏ô\"\n",
        "\n",
        "‚úÖ ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô:\n",
        "\"[‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó]\"\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ:\n",
        "\n",
        "{batch_text}\n",
        "\n",
        "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏° markers\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def create_batch(self, max_pages: int = None) -> Tuple[str, int]:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á batch ‡∏û‡∏£‡πâ‡∏≠‡∏° pre-cleaning ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á\"\"\"\n",
        "        max_pages = max_pages or Config.MAX_PAGES_PER_BATCH\n",
        "        files = sorted(self.input_folder.glob(\"*.txt\"))[:max_pages]\n",
        "\n",
        "        if not files:\n",
        "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô folder raw_ocr/\")\n",
        "            return \"\", 0\n",
        "\n",
        "        batch_parts = [\"[START_BATCH]\"]\n",
        "        stats = {'total_lines': 0, 'suspicious_lines': 0, 'merged_lines': 0}\n",
        "\n",
        "        for i, file_path in enumerate(files, 1):\n",
        "            try:\n",
        "                text = file_path.read_text(encoding='utf-8')\n",
        "\n",
        "                # ‡∏ô‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô clean\n",
        "                original_lines = len(text.split('\\n'))\n",
        "\n",
        "                # Clean text\n",
        "                cleaned_text = self.pre_clean_text(text)\n",
        "\n",
        "                # ‡∏ô‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á clean\n",
        "                cleaned_lines = len(cleaned_text.split('\\n'))\n",
        "                stats['total_lines'] += original_lines\n",
        "                stats['merged_lines'] += (original_lines - cleaned_lines)\n",
        "\n",
        "                # ‡πÄ‡∏û‡∏¥‡πà‡∏° markers\n",
        "                page_marker = f\"[PAGE_{i:03d}]\"\n",
        "                end_marker = f\"[END_PAGE_{i:03d}]\"\n",
        "                batch_parts.append(f\"\\n{page_marker}\\n{cleaned_text}\\n{end_marker}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error reading {file_path.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        batch_parts.append(\"\\n[END_BATCH]\")\n",
        "        batch_text = ''.join(batch_parts)\n",
        "\n",
        "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å batch\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        batch_file = self.output_folder / f\"batch_{timestamp}.txt\"\n",
        "        batch_file.write_text(batch_text, encoding='utf-8')\n",
        "\n",
        "        print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á batch ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {batch_file.name}\")\n",
        "        print(f\"   üìÑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(files)} ‡∏´‡∏ô‡πâ‡∏≤\")\n",
        "        print(f\"   üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:\")\n",
        "        print(f\"      - ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {stats['total_lines']}\")\n",
        "        print(f\"      - ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°: {stats['merged_lines']}\")\n",
        "        print(f\"   üíæ ‡∏Ç‡∏ô‡∏≤‡∏î: ~{len(batch_text.split())} ‡∏Ñ‡∏≥\")\n",
        "\n",
        "        return batch_text, len(files)\n",
        "\n",
        "    def prepare_and_copy(self, max_pages: int = None):\n",
        "        \"\"\"‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° batch ‡πÅ‡∏•‡∏∞ copy/save\"\"\"\n",
        "        batch_text, page_count = self.create_batch(max_pages)\n",
        "\n",
        "        if page_count == 0:\n",
        "            return\n",
        "\n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á enhanced prompt\n",
        "        prompt = self.create_enhanced_prompt(batch_text)\n",
        "\n",
        "        # Estimate tokens\n",
        "        estimated_tokens = len(prompt) // 2\n",
        "\n",
        "        # Save or copy\n",
        "        if CLIPBOARD_AVAILABLE:\n",
        "            try:\n",
        "                pyperclip.copy(prompt)\n",
        "                print(f\"\\n‚úÖ Copied to clipboard!\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Cannot copy: {e}\")\n",
        "                self._save_prompt_to_file(prompt)\n",
        "        else:\n",
        "            self._save_prompt_to_file(prompt)\n",
        "\n",
        "        print(f\"üìä ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì {estimated_tokens:,} tokens\")\n",
        "        print(f\"\\nüìù ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:\")\n",
        "        print(\"   1. ‡πÄ‡∏õ‡∏¥‡∏î ChatGPT/Claude\")\n",
        "        print(\"   2. Paste prompt\")\n",
        "        print(\"   3. ‡∏£‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\")\n",
        "        print(\"   4. Copy ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\")\n",
        "        print(\"   5. Run parse_results\")\n",
        "\n",
        "    def _save_prompt_to_file(self, prompt: str):\n",
        "        prompt_file = self.output_folder / \"latest_prompt.txt\"\n",
        "        prompt_file.write_text(prompt, encoding='utf-8')\n",
        "        print(f\"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å prompt ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {prompt_file}\")\n",
        "\n",
        "print(\"‚úÖ EnhancedBatchPreparer ready\")"
      ],
      "metadata": {
        "id": "nwe95a5xdYw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Block 5: Quality Validator\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏á LLM ‡πÅ‡∏Å‡πâ\n",
        "# ============================================\n",
        "class QualityValidator:\n",
        "    \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô LLM ‡πÅ‡∏•‡πâ‡∏ß\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_text(original: str, cleaned: str) -> Dict:\n",
        "        \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç\"\"\"\n",
        "        issues = []\n",
        "\n",
        "        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß\n",
        "        len_ratio = len(cleaned) / len(original) if len(original) > 0 else 0\n",
        "        if len_ratio < 0.5:\n",
        "            issues.append(\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏°‡∏≤‡∏Å (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)\")\n",
        "        elif len_ratio > 1.5:\n",
        "            issues.append(\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)\")\n",
        "\n",
        "        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "        orig_quotes = len(re.findall(r'\"[^\"]*\"', original))\n",
        "        clean_quotes = len(re.findall(r'\"[^\"]*\"', cleaned))\n",
        "        if abs(orig_quotes - clean_quotes) > 2:\n",
        "            issues.append(f\"‚ö†Ô∏è ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å ({orig_quotes} -> {clean_quotes})\")\n",
        "\n",
        "        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ (‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡πÉ‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö)\n",
        "        character_names = re.findall(r'(‡πÇ‡∏Ñ‡πÄ‡∏Æ|‡πÇ‡∏ä‡∏ï‡∏∞|‡∏≠‡∏±‡∏ï‡∏™‡∏∂‡∏¢‡∏∞)', original)\n",
        "        for name in set(character_names):\n",
        "            orig_count = original.count(name)\n",
        "            clean_count = cleaned.count(name)\n",
        "            if clean_count < orig_count * 0.8:\n",
        "                issues.append(f\"‚ö†Ô∏è ‡∏ä‡∏∑‡πà‡∏≠ '{name}' ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ ({orig_count} -> {clean_count})\")\n",
        "\n",
        "        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö paragraph structure\n",
        "        orig_paragraphs = len([p for p in original.split('\\n\\n') if p.strip()])\n",
        "        clean_paragraphs = len([p for p in cleaned.split('\\n') if p.strip()])\n",
        "\n",
        "        return {\n",
        "            'valid': len(issues) == 0,\n",
        "            'issues': issues,\n",
        "            'stats': {\n",
        "                'length_ratio': len_ratio,\n",
        "                'dialogue_count': clean_quotes,\n",
        "                'paragraph_count': clean_paragraphs,\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_quality_report(validations: List[Dict]) -> str:\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á quality report\"\"\"\n",
        "        report = \"üìä Quality Validation Report\\n\"\n",
        "        report += \"=\" * 50 + \"\\n\\n\"\n",
        "\n",
        "        total = len(validations)\n",
        "        valid = sum(1 for v in validations if v['valid'])\n",
        "\n",
        "        report += f\"‚úÖ Valid: {valid}/{total} ({valid/total*100:.1f}%)\\n\"\n",
        "        report += f\"‚ö†Ô∏è Issues found: {total - valid}\\n\\n\"\n",
        "\n",
        "        if total - valid > 0:\n",
        "            report += \"Issues detail:\\n\"\n",
        "            for i, val in enumerate(validations):\n",
        "                if not val['valid']:\n",
        "                    report += f\"\\nPage {i+1}:\\n\"\n",
        "                    for issue in val['issues']:\n",
        "                        report += f\"  {issue}\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "print(\"‚úÖ QualityValidator ready\")"
      ],
      "metadata": {
        "id": "cLi5tUxxdghh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}