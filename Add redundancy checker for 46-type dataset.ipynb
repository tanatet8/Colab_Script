{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/Add%20redundancy%20checker%20for%2046-type%20dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# REDUNDANCY CHECKER FIXED VERSION\n",
        "# แก้ปัญหา tier ที่เป็น '13B' และ error อื่นๆ\n",
        "# ============================================\n",
        "\n",
        "# ============================================\n",
        "# 📌 Block 1: Setup & Mount\n",
        "# ติดตั้งไลบรารี + Mount Google Drive\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q pandas numpy scikit-learn matplotlib seaborn openpyxl tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Libraries loaded\")"
      ],
      "metadata": {
        "id": "El9NnEM8WJZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 2: Configuration\n",
        "# ตั้งค่าพารามิเตอร์หลัก เช่น path, batch size, threshold\n",
        "# ============================================\n",
        "class Config:\n",
        "    # Paths - แก้ตรงนี้ให้เป็น path dataset และที่บันทึกผล\n",
        "    DATASET_DIR = '/content/drive/MyDrive/Dataset_Curation'\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/Dataset_Curation/redundancy_reports'\n",
        "\n",
        "    # Processing\n",
        "    BATCH_SIZE = 500  # จำนวน prompt ที่ประมวลผลต่อ batch\n",
        "    MAX_PROMPTS = None  # None = ใช้ทั้งหมด หรือใส่ตัวเลขเพื่อจำกัด\n",
        "\n",
        "    # Redundancy Thresholds by Type & Tier\n",
        "    THRESHOLDS = {\n",
        "        # Type: [Tier1-2, Tier3-4, Tier5-6]\n",
        "        'causal_reasoning': [0.75, 0.60, 0.40],\n",
        "        'symbolic_reasoning': [0.70, 0.55, 0.35],\n",
        "        'meta_reasoning': [0.60, 0.45, 0.30],\n",
        "        'moral_ambiguity_tradeoff': [0.50, 0.35, 0.25],\n",
        "        'philosophical_logic': [0.45, 0.30, 0.20],\n",
        "        # Default สำหรับ type อื่น ๆ\n",
        "        'default': [0.65, 0.50, 0.35]\n",
        "    }\n",
        "\n",
        "print(\"✅ Config loaded\")"
      ],
      "metadata": {
        "id": "Ap0AljqXWPpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 3: Data Extraction\n",
        "# โหลดไฟล์ markdown, แยกข้อมูล metadata และ prompt\n",
        "# ============================================\n",
        "class DataExtractor:\n",
        "    @staticmethod\n",
        "    def parse_block(block_text):\n",
        "        \"\"\"Extract data จาก 1 block ของ prompt\"\"\"\n",
        "        data = {}\n",
        "\n",
        "        # Metadata section\n",
        "        meta_match = re.search(r'###\\s*Metadata\\s*\\n(.*?)(?=\\n###|\\n##|$)',\n",
        "                              block_text, re.DOTALL)\n",
        "        if meta_match:\n",
        "            for line in meta_match.group(1).split('\\n'):\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    data[key.strip()] = value.strip()\n",
        "\n",
        "        # Prompts (TH, EN, ZH)\n",
        "        for lang in ['TH', 'EN', 'ZH']:\n",
        "            pattern = rf'###?\\s*Prompt\\s*\\({lang}\\)\\s*\\n(.*?)(?=\\n###|\\n##|$)'\n",
        "            match = re.search(pattern, block_text, re.DOTALL)\n",
        "            if match:\n",
        "                data[f'prompt_{lang.lower()}'] = match.group(1).strip()\n",
        "\n",
        "        # Reasoning\n",
        "        reason_match = re.search(r'###\\s*Reasoning\\s*\\n(.*?)(?=\\n###|$)',\n",
        "                                block_text, re.DOTALL)\n",
        "        if reason_match:\n",
        "            data['reasoning'] = reason_match.group(1).strip()\n",
        "\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def load_all_files(dataset_dir, max_prompts=None):\n",
        "        \"\"\"โหลดไฟล์ MD ทั้งหมด\"\"\"\n",
        "        all_prompts = []\n",
        "        md_files = sorted(Path(dataset_dir).glob('*_batch_*.md'))\n",
        "\n",
        "        print(f\"📂 พบ {len(md_files)} batch files\")\n",
        "\n",
        "        for file_path in tqdm(md_files, desc=\"Loading files\"):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # แบ่งโดย ## Prompt N\n",
        "            blocks = re.split(r'##\\s*Prompt\\s+\\d+', content)[1:]\n",
        "\n",
        "            for i, block in enumerate(blocks):\n",
        "                if max_prompts and len(all_prompts) >= max_prompts:\n",
        "                    break\n",
        "\n",
        "                prompt_data = DataExtractor.parse_block(block)\n",
        "                prompt_data['file'] = file_path.name\n",
        "                prompt_data['prompt_id'] = f\"{file_path.stem}_p{i+1:03d}\"\n",
        "\n",
        "                # Clean tier value (handle '13B' case)\n",
        "                if 'tier' in prompt_data:\n",
        "                    tier_val = prompt_data['tier']\n",
        "                    # ถ้า tier เป็น '13B' หรือค่าที่มี 'B' ให้ใช้ default tier\n",
        "                    if 'B' in str(tier_val).upper():\n",
        "                        prompt_data['model_size'] = tier_val\n",
        "                        prompt_data['tier'] = '2'  # Default tier\n",
        "                    elif not str(tier_val).isdigit():\n",
        "                        prompt_data['tier'] = '2'  # Default if not numeric\n",
        "                    else:\n",
        "                        # Convert to int to validate\n",
        "                        try:\n",
        "                            tier_int = int(tier_val)\n",
        "                            if tier_int > 6:\n",
        "                                prompt_data['model_size'] = f\"{tier_int}B\"\n",
        "                                prompt_data['tier'] = '2'\n",
        "                            else:\n",
        "                                prompt_data['tier'] = str(tier_int)\n",
        "                        except:\n",
        "                            prompt_data['tier'] = '2'\n",
        "\n",
        "                all_prompts.append(prompt_data)\n",
        "\n",
        "            if max_prompts and len(all_prompts) >= max_prompts:\n",
        "                break\n",
        "\n",
        "        return pd.DataFrame(all_prompts)\n",
        "\n",
        "print(\"✅ Data Extractor ready\")"
      ],
      "metadata": {
        "id": "NHb6UEBVWTYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 4: Redundancy Analysis - FIXED\n",
        "# วิเคราะห์ความซ้ำซ้อนของ prompt ด้วย TF-IDF + cosine similarity\n",
        "# ============================================\n",
        "class RedundancyAnalyzer:\n",
        "    def __init__(self, df, config):\n",
        "        self.df = df\n",
        "        self.config = config\n",
        "        self.similarity_matrices = {}\n",
        "\n",
        "        # Clean tier column\n",
        "        if 'tier' in self.df.columns:\n",
        "            self.df['tier'] = self.df['tier'].apply(self.clean_tier_value)\n",
        "\n",
        "    def clean_tier_value(self, tier_val):\n",
        "        \"\"\"Clean tier value to ensure it's numeric 1-6\"\"\"\n",
        "        if pd.isna(tier_val):\n",
        "            return 2\n",
        "\n",
        "        tier_str = str(tier_val)\n",
        "\n",
        "        # Handle '13B' or similar cases\n",
        "        if 'B' in tier_str.upper():\n",
        "            return 2  # Default tier for model sizes\n",
        "\n",
        "        # Try to extract numeric value\n",
        "        try:\n",
        "            tier_num = int(re.search(r'\\d+', tier_str).group())\n",
        "            if 1 <= tier_num <= 6:\n",
        "                return tier_num\n",
        "            else:\n",
        "                return 2  # Default if out of range\n",
        "        except:\n",
        "            return 2  # Default if can't parse\n",
        "\n",
        "    def get_threshold(self, reasoning_type, tier):\n",
        "        \"\"\"ดึง threshold ตาม reasoning_type และ tier\"\"\"\n",
        "        # Ensure tier is int\n",
        "        try:\n",
        "            tier = int(tier)\n",
        "        except:\n",
        "            tier = 2\n",
        "\n",
        "        tier_idx = min((tier-1)//2, 2)\n",
        "\n",
        "        if reasoning_type in self.config.THRESHOLDS:\n",
        "            thresholds = self.config.THRESHOLDS[reasoning_type]\n",
        "        else:\n",
        "            thresholds = self.config.THRESHOLDS['default']\n",
        "        return thresholds[tier_idx]\n",
        "\n",
        "    def calculate_similarity_batch(self, df_batch, text_col='prompt_en'):\n",
        "        \"\"\"คำนวณ similarity สำหรับ batch\"\"\"\n",
        "        # Use prompt_th as fallback if prompt_en doesn't exist\n",
        "        if text_col not in df_batch.columns or df_batch[text_col].isna().all():\n",
        "            text_col = 'prompt_th'\n",
        "\n",
        "        valid_df = df_batch[df_batch[text_col].notna()].reset_index(drop=True)\n",
        "        if len(valid_df) < 2:\n",
        "            return None, []\n",
        "\n",
        "        # TF-IDF\n",
        "        try:\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=500,\n",
        "                ngram_range=(1, 3),\n",
        "                min_df=1,  # Changed from 2 to 1 for small datasets\n",
        "                max_df=0.95\n",
        "            )\n",
        "            tfidf_matrix = vectorizer.fit_transform(valid_df[text_col])\n",
        "            sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error in TF-IDF: {e}\")\n",
        "            return None, []\n",
        "\n",
        "        # หา pair ที่เกิน threshold\n",
        "        pairs = []\n",
        "        for i in range(len(sim_matrix)):\n",
        "            for j in range(i+1, len(sim_matrix)):\n",
        "                row_i = valid_df.iloc[i]\n",
        "                row_j = valid_df.iloc[j]\n",
        "\n",
        "                tier_i = int(row_i.get('tier', 2))\n",
        "                tier_j = int(row_j.get('tier', 2))\n",
        "                type_i = row_i.get('reasoning_type', 'default')\n",
        "\n",
        "                threshold = self.get_threshold(type_i, max(tier_i, tier_j))\n",
        "\n",
        "                if sim_matrix[i][j] >= threshold:\n",
        "                    pairs.append({\n",
        "                        'idx1': i,\n",
        "                        'idx2': j,\n",
        "                        'prompt1_id': row_i['prompt_id'],\n",
        "                        'prompt2_id': row_j['prompt_id'],\n",
        "                        'similarity': sim_matrix[i][j],\n",
        "                        'threshold_used': threshold,\n",
        "                        'type1': type_i,\n",
        "                        'type2': row_j.get('reasoning_type', 'default'),\n",
        "                        'tier1': tier_i,\n",
        "                        'tier2': tier_j,\n",
        "                        'prompt1': row_i[text_col][:100] if text_col in row_i.index else 'N/A',\n",
        "                        'prompt2': row_j[text_col][:100] if text_col in row_j.index else 'N/A'\n",
        "                    })\n",
        "        return sim_matrix, pairs\n",
        "\n",
        "    def analyze_all(self):\n",
        "        \"\"\"วิเคราะห์ทุก prompt ใน batch\"\"\"\n",
        "        all_pairs = []\n",
        "        batch_size = self.config.BATCH_SIZE\n",
        "        n_batches = (len(self.df) + batch_size - 1) // batch_size\n",
        "\n",
        "        for batch_idx in tqdm(range(n_batches), desc=\"Analyzing batches\"):\n",
        "            start = batch_idx * batch_size\n",
        "            end = min(start + batch_size, len(self.df))\n",
        "            df_batch = self.df.iloc[start:end]\n",
        "            _, pairs = self.calculate_similarity_batch(df_batch)\n",
        "            all_pairs.extend(pairs)\n",
        "\n",
        "        return all_pairs\n",
        "\n",
        "    def get_distribution_stats(self):\n",
        "        \"\"\"คำนวณสถิติการกระจายข้อมูล\"\"\"\n",
        "        stats = {}\n",
        "        categories = ['reasoning_type', 'sub_type', 'domain_context', 'difficulty', 'tier']\n",
        "\n",
        "        for cat in categories:\n",
        "            if cat in self.df.columns:\n",
        "                value_counts = self.df[cat].value_counts()\n",
        "                stats[cat] = {\n",
        "                    'distribution': value_counts.to_dict(),\n",
        "                    'unique': len(value_counts),\n",
        "                    'max': value_counts.max(),\n",
        "                    'min': value_counts.min(),\n",
        "                    'std': value_counts.std(),\n",
        "                    'imbalance_ratio': value_counts.max() / value_counts.min() if value_counts.min() > 0 else float('inf')\n",
        "                }\n",
        "        return stats\n",
        "\n",
        "print(\"✅ Redundancy Analyzer ready\")"
      ],
      "metadata": {
        "id": "0BzPoBtXWYOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 5: Fix Suggestions\n",
        "# สร้างข้อเสนอแนะแก้ไขสำหรับคู่ prompt ที่ซ้ำ\n",
        "# ============================================\n",
        "class RedundancyFixer:\n",
        "    @staticmethod\n",
        "    def suggest_fixes(similar_pairs):\n",
        "        \"\"\"Suggest fixes for redundant pairs\"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        for pair in similar_pairs:\n",
        "            fix_options = []\n",
        "\n",
        "            # Domain shift suggestion\n",
        "            if pair['similarity'] > 0.9:\n",
        "                fix_options.append({\n",
        "                    'method': 'domain_shift',\n",
        "                    'priority': 'high',\n",
        "                    'description': 'Change domain context completely'\n",
        "                })\n",
        "\n",
        "            # Complexity change\n",
        "            if pair['tier1'] == pair['tier2']:\n",
        "                fix_options.append({\n",
        "                    'method': 'complexity_change',\n",
        "                    'priority': 'medium',\n",
        "                    'description': f'Adjust complexity (current tier: {pair[\"tier1\"]})'\n",
        "                })\n",
        "\n",
        "            # Angle change\n",
        "            if 0.7 < pair['similarity'] <= 0.9:\n",
        "                fix_options.append({\n",
        "                    'method': 'angle_change',\n",
        "                    'priority': 'medium',\n",
        "                    'description': 'Change questioning angle or framing'\n",
        "                })\n",
        "\n",
        "            suggestions.append({\n",
        "                'pair': pair,\n",
        "                'fixes': fix_options\n",
        "            })\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "print(\"✅ Fixer ready\")"
      ],
      "metadata": {
        "id": "HkteEe2BWa9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 6: Report Generation\n",
        "# สร้างรายงาน Excel ครบชุด + หา coverage gaps\n",
        "# ============================================\n",
        "class ReportGenerator:\n",
        "    @staticmethod\n",
        "    def create_excel_report(df, similar_pairs, stats, suggestions, output_path):\n",
        "        \"\"\"Generate comprehensive Excel report\"\"\"\n",
        "\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "            # Sheet 1: Overview\n",
        "            overview_data = {\n",
        "                'Metric': ['Total Prompts', 'Redundant Pairs', 'Avg Similarity',\n",
        "                          'Types Count', 'Tiers Range'],\n",
        "                'Value': [\n",
        "                    len(df),\n",
        "                    len(similar_pairs),\n",
        "                    np.mean([p['similarity'] for p in similar_pairs]) if similar_pairs else 0,\n",
        "                    df['reasoning_type'].nunique() if 'reasoning_type' in df.columns else 0,\n",
        "                    f\"{df['tier'].min()}-{df['tier'].max()}\" if 'tier' in df.columns else 'N/A'\n",
        "                ]\n",
        "            }\n",
        "            pd.DataFrame(overview_data).to_excel(writer, sheet_name='Overview', index=False)\n",
        "\n",
        "            # Sheet 2: Similar Pairs\n",
        "            if similar_pairs:\n",
        "                df_pairs = pd.DataFrame(similar_pairs)\n",
        "                df_pairs = df_pairs.sort_values('similarity', ascending=False)\n",
        "                df_pairs.to_excel(writer, sheet_name='Similar_Pairs', index=False)\n",
        "\n",
        "            # Sheet 3: Distribution\n",
        "            dist_data = []\n",
        "            for cat, cat_stats in stats.items():\n",
        "                for value, count in cat_stats['distribution'].items():\n",
        "                    dist_data.append({\n",
        "                        'Category': cat,\n",
        "                        'Value': value,\n",
        "                        'Count': count,\n",
        "                        'Percentage': count / len(df) * 100\n",
        "                    })\n",
        "            pd.DataFrame(dist_data).to_excel(writer, sheet_name='Distribution', index=False)\n",
        "\n",
        "            # Sheet 4: Fix Suggestions\n",
        "            fix_data = []\n",
        "            for sug in suggestions[:100]:  # Top 100\n",
        "                pair = sug['pair']\n",
        "                for fix in sug['fixes']:\n",
        "                    fix_data.append({\n",
        "                        'Prompt1_ID': pair['prompt1_id'],\n",
        "                        'Prompt2_ID': pair['prompt2_id'],\n",
        "                        'Similarity': pair['similarity'],\n",
        "                        'Fix_Method': fix['method'],\n",
        "                        'Priority': fix['priority'],\n",
        "                        'Description': fix['description']\n",
        "                    })\n",
        "            if fix_data:\n",
        "                pd.DataFrame(fix_data).to_excel(writer, sheet_name='Fix_Suggestions', index=False)\n",
        "\n",
        "            # Sheet 5: Gaps Analysis\n",
        "            gaps = ReportGenerator.find_gaps(df)\n",
        "            if gaps:\n",
        "                pd.DataFrame(gaps).to_excel(writer, sheet_name='Gaps', index=False)\n",
        "\n",
        "        print(f\"✅ Report saved to: {output_path}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def find_gaps(df):\n",
        "        \"\"\"Find coverage gaps\"\"\"\n",
        "        gaps = []\n",
        "\n",
        "        if 'reasoning_type' in df.columns and 'difficulty' in df.columns:\n",
        "            # Check all combinations\n",
        "            types = df['reasoning_type'].unique()\n",
        "            difficulties = ['easy', 'medium', 'hard']\n",
        "\n",
        "            for t in types:\n",
        "                for d in difficulties:\n",
        "                    count = len(df[(df['reasoning_type'] == t) &\n",
        "                                  (df['difficulty'] == d)])\n",
        "                    if count < 10:  # Threshold\n",
        "                        gaps.append({\n",
        "                            'Type': t,\n",
        "                            'Difficulty': d,\n",
        "                            'Current_Count': count,\n",
        "                            'Target': 10,\n",
        "                            'Gap': 10 - count\n",
        "                        })\n",
        "\n",
        "        return gaps\n",
        "\n",
        "print(\"✅ Report Generator ready\")"
      ],
      "metadata": {
        "id": "y1EScp2LWep7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 7: Visualization\n",
        "# ทำกราฟสรุปการกระจายประเภท, tier, difficulty, domain และการกระจาย similarity\n",
        "# ============================================\n",
        "def create_visualizations(df, similar_pairs, stats):\n",
        "    \"\"\"Create analysis visualizations\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # 1. Type distribution\n",
        "    if 'reasoning_type' in df.columns:\n",
        "        type_counts = df['reasoning_type'].value_counts().head(15)\n",
        "        type_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
        "        axes[0,0].set_title('Top 15 Reasoning Types')\n",
        "        axes[0,0].set_xlabel('Type')\n",
        "        axes[0,0].set_ylabel('Count')\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # 2. Tier distribution\n",
        "    if 'tier' in df.columns:\n",
        "        # Ensure tier values are clean\n",
        "        tier_counts = df['tier'].value_counts().sort_index()\n",
        "        tier_counts.plot(kind='bar', ax=axes[0,1], color='lightgreen')\n",
        "        axes[0,1].set_title('Tier Distribution')\n",
        "        axes[0,1].set_xlabel('Tier')\n",
        "        axes[0,1].set_ylabel('Count')\n",
        "\n",
        "    # 3. Similarity distribution\n",
        "    if similar_pairs:\n",
        "        similarities = [p['similarity'] for p in similar_pairs]\n",
        "        axes[0,2].hist(similarities, bins=30, color='coral', edgecolor='black')\n",
        "        axes[0,2].set_title('Similarity Score Distribution')\n",
        "        axes[0,2].set_xlabel('Similarity')\n",
        "        axes[0,2].set_ylabel('Frequency')\n",
        "        axes[0,2].axvline(x=0.7, color='r', linestyle='--', label='Threshold')\n",
        "        axes[0,2].legend()\n",
        "\n",
        "    # 4. Difficulty distribution\n",
        "    if 'difficulty' in df.columns:\n",
        "        diff_counts = df['difficulty'].value_counts()\n",
        "        diff_counts.plot(kind='pie', ax=axes[1,0], autopct='%1.1f%%')\n",
        "        axes[1,0].set_title('Difficulty Distribution')\n",
        "\n",
        "    # 5. Domain distribution\n",
        "    if 'domain_context' in df.columns:\n",
        "        domain_counts = df['domain_context'].value_counts().head(10)\n",
        "        domain_counts.plot(kind='barh', ax=axes[1,1], color='plum')\n",
        "        axes[1,1].set_title('Top 10 Domains')\n",
        "        axes[1,1].set_xlabel('Count')\n",
        "\n",
        "    # 6. Redundancy by Type\n",
        "    if similar_pairs and 'reasoning_type' in df.columns:\n",
        "        redundancy_by_type = defaultdict(int)\n",
        "        for pair in similar_pairs:\n",
        "            redundancy_by_type[pair['type1']] += 1\n",
        "\n",
        "        if redundancy_by_type:\n",
        "            top_redundant = dict(sorted(redundancy_by_type.items(),\n",
        "                                       key=lambda x: x[1], reverse=True)[:10])\n",
        "            axes[1,2].bar(range(len(top_redundant)), list(top_redundant.values()),\n",
        "                         color='salmon')\n",
        "            axes[1,2].set_xticks(range(len(top_redundant)))\n",
        "            axes[1,2].set_xticklabels(list(top_redundant.keys()), rotation=45, ha='right')\n",
        "            axes[1,2].set_title('Top 10 Types with Redundancy')\n",
        "            axes[1,2].set_ylabel('Redundant Pairs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    output_path = '/content/redundancy_analysis.png'\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"📊 Visualization saved to: {output_path}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"✅ Visualization ready\")"
      ],
      "metadata": {
        "id": "7M750KYdWhp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 8: Main Pipeline\n",
        "# รันทั้งกระบวนการ: โหลด, วิเคราะห์, สถิติ, ข้อเสนอแนะ, รายงาน, กราฟ, สรุป\n",
        "# ============================================\n",
        "def main():\n",
        "    \"\"\"Main analysis pipeline\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"🔍 REDUNDANCY CHECKER - 46 TYPE DATASET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create output directory\n",
        "    Path(Config.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n📊 Loading data...\")\n",
        "    df = DataExtractor.load_all_files(Config.DATASET_DIR, Config.MAX_PROMPTS)\n",
        "    print(f\"✅ Loaded {len(df)} prompts\")\n",
        "\n",
        "    # Basic info\n",
        "    print(\"\\n📋 Dataset Info:\")\n",
        "    print(f\"  Files: {df['file'].nunique()}\")\n",
        "    print(f\"  Types: {df['reasoning_type'].nunique() if 'reasoning_type' in df.columns else 'N/A'}\")\n",
        "\n",
        "    if 'tier' in df.columns:\n",
        "        unique_tiers = df['tier'].unique().tolist()\n",
        "        print(f\"  Tiers: {sorted([t for t in unique_tiers if str(t).isdigit()])}\")\n",
        "\n",
        "    # Check for model_size column\n",
        "    if 'model_size' in df.columns:\n",
        "        print(f\"  Model sizes: {df['model_size'].dropna().unique().tolist()}\")\n",
        "\n",
        "    # Analyze redundancy\n",
        "    print(\"\\n🔍 Analyzing redundancy...\")\n",
        "    analyzer = RedundancyAnalyzer(df, Config)\n",
        "    similar_pairs = analyzer.analyze_all()\n",
        "    print(f\"✅ Found {len(similar_pairs)} redundant pairs\")\n",
        "\n",
        "    # Get statistics\n",
        "    print(\"\\n📈 Calculating statistics...\")\n",
        "    stats = analyzer.get_distribution_stats()\n",
        "\n",
        "    # Generate suggestions\n",
        "    print(\"\\n💡 Generating fix suggestions...\")\n",
        "    fixer = RedundancyFixer()\n",
        "    suggestions = fixer.suggest_fixes(similar_pairs)\n",
        "\n",
        "    # Create report\n",
        "    print(\"\\n📝 Creating Excel report...\")\n",
        "    report_path = Path(Config.OUTPUT_DIR) / 'redundancy_report.xlsx'\n",
        "    ReportGenerator.create_excel_report(df, similar_pairs, stats, suggestions, report_path)\n",
        "\n",
        "    # Visualizations\n",
        "    print(\"\\n📊 Creating visualizations...\")\n",
        "    create_visualizations(df, similar_pairs, stats)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ ANALYSIS COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n📊 Summary:\")\n",
        "    print(f\"  Total Prompts: {len(df)}\")\n",
        "    print(f\"  Redundant Pairs: {len(similar_pairs)}\")\n",
        "\n",
        "    if len(df) > 1:\n",
        "        max_pairs = len(df) * (len(df) - 1) / 2\n",
        "        redundancy_rate = len(similar_pairs) / max_pairs * 100\n",
        "        print(f\"  Redundancy Rate: {redundancy_rate:.2f}%\")\n",
        "\n",
        "    if stats.get('reasoning_type'):\n",
        "        print(f\"  Type Imbalance: {stats['reasoning_type']['imbalance_ratio']:.2f}x\")\n",
        "\n",
        "    print(f\"\\n📁 Files Generated:\")\n",
        "    print(f\"  - {report_path}\")\n",
        "    print(f\"  - /content/redundancy_analysis.png\")\n",
        "\n",
        "    return df, similar_pairs, stats\n",
        "\n",
        "print(\"✅ All functions ready\")"
      ],
      "metadata": {
        "id": "pSAH50qLWk58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 📌 Block 9: Run\n",
        "# จุดเริ่มรันโปรแกรมหลักใน Colab\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    df, pairs, stats = main()\n",
        "    print(\"\\n🎉 Done! Check the output files in your Drive.\")"
      ],
      "metadata": {
        "id": "3mzlfXWdWpLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ยินดีต้อนรับสู่ Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}