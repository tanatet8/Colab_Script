{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanatet8/Colab_Script/blob/main/metadata/Metadata_Extractor_To_Excel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# METADATA EXTRACTOR TO EXCEL\n",
        "# Extract metadata ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å prompt ‚Üí Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM & Analysis\n",
        "# ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 300-500K prompts\n",
        "# ============================================\n",
        "\n",
        "# ============================================\n",
        "# Block 1: Setup & Import\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")"
      ],
      "metadata": {
        "id": "D-SOKdA6Vt2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 2: Configuration\n",
        "# ============================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    DATASET_DIR = '/content/drive/MyDrive/Dataset_Curation'\n",
        "    OUTPUT_EXCEL = '/content/drive/MyDrive/Dataset_Curation/metadata_master.xlsx'\n",
        "\n",
        "    # File patterns\n",
        "    FILE_PATTERN = '*_FIXED_COMPLETE.md'  # ‡∏´‡∏£‡∏∑‡∏≠ '*_batch_*.md'\n",
        "\n",
        "    # Processing\n",
        "    MAX_PROMPTS = None  # None = all, or set limit\n",
        "\n",
        "    # Metadata fields to extract (20+ fields for scalability)\n",
        "    METADATA_FIELDS = [\n",
        "        'prompt_id', 'batch_id', 'reasoning_type', 'sub_type',\n",
        "        'difficulty', 'tier', 'model_size', 'language', 'domain_context',\n",
        "        'contains_statistics', 'has_numerical_estimate', 'requires_visualization',\n",
        "        'symbolic_risk', 'contains_fallacy_risk', 'confidence_level_expected',\n",
        "        'is_behavior_driven', 'concept_tags', 'fallacy', 'fallacy_type',\n",
        "        'chain_depth', 'tone_style', 'self_critique', 'belief_tracking',\n",
        "        'eval_standard', 'reasoning_path_trace'\n",
        "    ]\n",
        "\n",
        "    # Additional tracking fields\n",
        "    TRACKING_FIELDS = [\n",
        "        'prompt_hash', 'file_source', 'extraction_date',\n",
        "        'prompt_length_th', 'prompt_length_en', 'has_reasoning',\n",
        "        'has_rejected', 'has_explanation', 'quality_score',\n",
        "        'combination_key', 'saturation_level'\n",
        "    ]\n",
        "\n",
        "print(f\"üìÅ Dataset: {Config.DATASET_DIR}\")\n",
        "print(f\"üìä Output: {Config.OUTPUT_EXCEL}\")"
      ],
      "metadata": {
        "id": "juATTSVtVyAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 3: Metadata Extractor\n",
        "# ============================================\n",
        "class MetadataExtractor:\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_hash(text):\n",
        "        \"\"\"Generate unique hash for prompt\"\"\"\n",
        "        return hashlib.md5(text.encode()).hexdigest()[:12]\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_block(block_text, file_name, prompt_num):\n",
        "        \"\"\"Extract all metadata from a prompt block\"\"\"\n",
        "        data = {\n",
        "            'file_source': file_name,\n",
        "            'extraction_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "            'prompt_num': prompt_num\n",
        "        }\n",
        "\n",
        "        # Extract Metadata section\n",
        "        meta_match = re.search(r'###\\s*Metadata\\s*\\n(.*?)(?=\\n###|\\n##|$)',\n",
        "                              block_text, re.DOTALL)\n",
        "        if meta_match:\n",
        "            metadata_text = meta_match.group(1)\n",
        "            # Parse each metadata field\n",
        "            for line in metadata_text.split('\\n'):\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    key = key.strip()\n",
        "                    value = value.strip()\n",
        "\n",
        "                    # Special handling for different types\n",
        "                    if key == 'concept_tags':\n",
        "                        # Clean concept tags format\n",
        "                        value = value.strip('[]')\n",
        "                        data[key] = value\n",
        "                        # Also create individual tag columns\n",
        "                        tags = [t.strip(' \"\\'') for t in value.split(',')]\n",
        "                        data['tag_count'] = len(tags)\n",
        "                        data['tag_1'] = tags[0] if len(tags) > 0 else ''\n",
        "                        data['tag_2'] = tags[1] if len(tags) > 1 else ''\n",
        "                        data['tag_3'] = tags[2] if len(tags) > 2 else ''\n",
        "                    else:\n",
        "                        data[key] = value\n",
        "\n",
        "        # Extract prompts content\n",
        "        for lang in ['TH', 'EN', 'ZH']:\n",
        "            pattern = rf'###?\\s*Prompt\\s*\\({lang}\\)\\s*\\n(.*?)(?=\\n###|\\n##|$)'\n",
        "            match = re.search(pattern, block_text, re.DOTALL)\n",
        "            if match:\n",
        "                prompt_text = match.group(1).strip()\n",
        "                data[f'prompt_{lang.lower()}'] = prompt_text[:100]  # First 100 chars for preview\n",
        "                data[f'prompt_length_{lang.lower()}'] = len(prompt_text)\n",
        "\n",
        "                # Generate hash from first available prompt\n",
        "                if 'prompt_hash' not in data and prompt_text:\n",
        "                    data['prompt_hash'] = MetadataExtractor.generate_hash(prompt_text)\n",
        "\n",
        "        # Check for other sections\n",
        "        data['has_reasoning'] = 'Y' if '### Reasoning' in block_text else 'N'\n",
        "        data['has_rejected'] = 'Y' if '### Rejected Reasoning' in block_text else 'N'\n",
        "        data['has_explanation'] = 'Y' if '### Explanation' in block_text else 'N'\n",
        "\n",
        "        # Create combination key for tracking\n",
        "        reasoning_type = data.get('reasoning_type', 'unknown')\n",
        "        sub_type = data.get('sub_type', 'unknown')\n",
        "        domain = data.get('domain_context', 'unknown')\n",
        "        data['combination_key'] = f\"{reasoning_type}|{sub_type}|{domain}\"\n",
        "\n",
        "        # Default quality score (can be updated manually later)\n",
        "        data['quality_score'] = 'pending'\n",
        "\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def load_all_files(dataset_dir, file_pattern, max_prompts=None):\n",
        "        \"\"\"Load and extract metadata from all files\"\"\"\n",
        "        all_metadata = []\n",
        "        md_files = sorted(Path(dataset_dir).glob(file_pattern))\n",
        "\n",
        "        print(f\"üìÇ Found {len(md_files)} files matching pattern: {file_pattern}\")\n",
        "\n",
        "        total_prompts = 0\n",
        "        for file_path in md_files:\n",
        "            print(f\"  üìÑ Processing: {file_path.name}\")\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Split by prompt blocks\n",
        "            blocks = re.split(r'##\\s*Prompt\\s+(\\d+)', content)[1:]\n",
        "\n",
        "            # Process pairs of (number, content)\n",
        "            for i in range(0, len(blocks), 2):\n",
        "                if i+1 < len(blocks):\n",
        "                    prompt_num = blocks[i]\n",
        "                    block_content = blocks[i+1]\n",
        "\n",
        "                    if max_prompts and total_prompts >= max_prompts:\n",
        "                        break\n",
        "\n",
        "                    metadata = MetadataExtractor.parse_block(\n",
        "                        f\"## Prompt {prompt_num}\\n{block_content}\",\n",
        "                        file_path.name,\n",
        "                        int(prompt_num)\n",
        "                    )\n",
        "                    all_metadata.append(metadata)\n",
        "                    total_prompts += 1\n",
        "\n",
        "            if max_prompts and total_prompts >= max_prompts:\n",
        "                print(f\"  ‚ö†Ô∏è Reached max prompts limit: {max_prompts}\")\n",
        "                break\n",
        "\n",
        "        print(f\"‚úÖ Extracted metadata from {total_prompts} prompts\")\n",
        "        return pd.DataFrame(all_metadata)"
      ],
      "metadata": {
        "id": "cbpjjiXgV5IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 4: Analysis & Statistics\n",
        "# ============================================\n",
        "class MetadataAnalyzer:\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_saturation(df):\n",
        "        \"\"\"Calculate saturation level for each combination\"\"\"\n",
        "        # Count combinations\n",
        "        combo_counts = df['combination_key'].value_counts()\n",
        "\n",
        "        # Define saturation levels\n",
        "        def get_saturation(count):\n",
        "            if count >= 10:\n",
        "                return 'HIGH'\n",
        "            elif count >= 5:\n",
        "                return 'MEDIUM'\n",
        "            else:\n",
        "                return 'LOW'\n",
        "\n",
        "        # Apply saturation level\n",
        "        df['saturation_level'] = df['combination_key'].apply(\n",
        "            lambda x: get_saturation(combo_counts.get(x, 0))\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_statistics(df):\n",
        "        \"\"\"Generate comprehensive statistics\"\"\"\n",
        "        stats = {\n",
        "            'Total Prompts': len(df),\n",
        "            'Unique Files': df['file_source'].nunique() if 'file_source' in df.columns else 0,\n",
        "            'Reasoning Types': df['reasoning_type'].nunique() if 'reasoning_type' in df.columns else 0,\n",
        "            'Sub Types': df['sub_type'].nunique() if 'sub_type' in df.columns else 0,\n",
        "            'Domains': df['domain_context'].nunique() if 'domain_context' in df.columns else 0,\n",
        "            'Languages': df['language'].nunique() if 'language' in df.columns else 0,\n",
        "            'Avg Chain Depth': df['chain_depth'].apply(lambda x: int(x) if pd.notna(x) and str(x).isdigit() else 0).mean()\n",
        "        }\n",
        "\n",
        "        # Difficulty distribution\n",
        "        if 'difficulty' in df.columns:\n",
        "            diff_dist = df['difficulty'].value_counts()\n",
        "            stats['Easy %'] = diff_dist.get('easy', 0) / len(df) * 100\n",
        "            stats['Medium %'] = diff_dist.get('medium', 0) / len(df) * 100\n",
        "            stats['Hard %'] = diff_dist.get('hard', 0) / len(df) * 100\n",
        "\n",
        "        # Tier distribution\n",
        "        if 'tier' in df.columns:\n",
        "            tier_dist = df['tier'].value_counts()\n",
        "            for tier in range(1, 7):\n",
        "                stats[f'Tier {tier} Count'] = tier_dist.get(str(tier), 0)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    @staticmethod\n",
        "    def find_gaps(df):\n",
        "        \"\"\"Find gaps in coverage\"\"\"\n",
        "        gaps = []\n",
        "\n",
        "        # Check reasoning_type √ó difficulty\n",
        "        if 'reasoning_type' in df.columns and 'difficulty' in df.columns:\n",
        "            for rtype in df['reasoning_type'].unique():\n",
        "                for diff in ['easy', 'medium', 'hard']:\n",
        "                    count = len(df[(df['reasoning_type'] == rtype) &\n",
        "                                  (df['difficulty'] == diff)])\n",
        "                    if count < 5:  # Threshold\n",
        "                        gaps.append({\n",
        "                            'Gap Type': 'Type√óDifficulty',\n",
        "                            'Combination': f\"{rtype} + {diff}\",\n",
        "                            'Current Count': count,\n",
        "                            'Recommended': 5,\n",
        "                            'Priority': 'High' if count == 0 else 'Medium'\n",
        "                        })\n",
        "\n",
        "        # Check domain √ó sub_type\n",
        "        if 'domain_context' in df.columns and 'sub_type' in df.columns:\n",
        "            for domain in df['domain_context'].unique():\n",
        "                for subtype in df['sub_type'].unique():\n",
        "                    count = len(df[(df['domain_context'] == domain) &\n",
        "                                  (df['sub_type'] == subtype)])\n",
        "                    if count == 0:  # Only show complete gaps\n",
        "                        gaps.append({\n",
        "                            'Gap Type': 'Domain√óSubType',\n",
        "                            'Combination': f\"{domain} + {subtype}\",\n",
        "                            'Current Count': 0,\n",
        "                            'Recommended': 3,\n",
        "                            'Priority': 'Low'\n",
        "                        })\n",
        "\n",
        "        return pd.DataFrame(gaps[:50])  # Top 50 gaps"
      ],
      "metadata": {
        "id": "O-RBfDaNV_Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 5: LLM Instructions Generator\n",
        "# ============================================\n",
        "class LLMInstructions:\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_instructions(df, stats_dict):\n",
        "        \"\"\"Generate instructions for LLM to avoid duplicates\"\"\"\n",
        "\n",
        "        instructions = []\n",
        "\n",
        "        # Header\n",
        "        instructions.append(\"=\"*60)\n",
        "        instructions.append(\"LLM DATASET CREATION GUIDELINES\")\n",
        "        instructions.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "        instructions.append(\"=\"*60)\n",
        "\n",
        "        # Current Status\n",
        "        instructions.append(\"\\nüìä CURRENT DATASET STATUS:\")\n",
        "        instructions.append(f\"‚Ä¢ Total Prompts: {stats_dict['Total Prompts']}\")\n",
        "        instructions.append(f\"‚Ä¢ Reasoning Types: {stats_dict['Reasoning Types']}\")\n",
        "        instructions.append(f\"‚Ä¢ Domains: {stats_dict['Domains']}\")\n",
        "        instructions.append(f\"‚Ä¢ Difficulty: Easy {stats_dict.get('Easy %', 0):.1f}%, \"\n",
        "                          f\"Medium {stats_dict.get('Medium %', 0):.1f}%, \"\n",
        "                          f\"Hard {stats_dict.get('Hard %', 0):.1f}%\")\n",
        "\n",
        "        # Oversaturated Combinations\n",
        "        instructions.append(\"\\nüö´ AVOID CREATING (Oversaturated):\")\n",
        "        combo_counts = df['combination_key'].value_counts()\n",
        "        for combo, count in combo_counts.head(10).items():\n",
        "            if count >= 10:\n",
        "                parts = combo.split('|')\n",
        "                instructions.append(f\"  ‚ùå {parts[0]} + {parts[1]} + {parts[2]} (has {count} already)\")\n",
        "\n",
        "        # Recommended to Create\n",
        "        instructions.append(\"\\n‚úÖ PRIORITIZE CREATING (Gaps):\")\n",
        "\n",
        "        # Find missing combinations\n",
        "        if 'reasoning_type' in df.columns and 'domain_context' in df.columns:\n",
        "            existing_combos = set(df['combination_key'].unique())\n",
        "\n",
        "            # Sample of what's missing\n",
        "            priority_domains = ['technology', 'economics', 'social_science', 'philosophy']\n",
        "            priority_subtypes = ['reverse_causality', 'feedback_loop', 'hidden_variable', 'temporal_lag']\n",
        "\n",
        "            for domain in priority_domains:\n",
        "                for subtype in priority_subtypes:\n",
        "                    test_combo = f\"causal_reasoning|{subtype}|{domain}\"\n",
        "                    if test_combo not in existing_combos:\n",
        "                        instructions.append(f\"  ‚úÖ causal_reasoning + {subtype} + {domain}\")\n",
        "\n",
        "        # Balance Recommendations\n",
        "        instructions.append(\"\\n‚öñÔ∏è BALANCE RECOMMENDATIONS:\")\n",
        "        instructions.append(\"‚Ä¢ Target Difficulty: Easy 25%, Medium 50%, Hard 25%\")\n",
        "        instructions.append(\"‚Ä¢ Target Tiers: Tier 1-2 (40%), Tier 3-4 (35%), Tier 5-6 (25%)\")\n",
        "        instructions.append(\"‚Ä¢ Each sub_type should have 5-10 prompts\")\n",
        "        instructions.append(\"‚Ä¢ Each domain should have 10-20 prompts\")\n",
        "\n",
        "        # Concept Tags to Explore\n",
        "        instructions.append(\"\\nüè∑Ô∏è UNDERUSED CONCEPT TAGS TO EXPLORE:\")\n",
        "        if 'concept_tags' in df.columns:\n",
        "            all_tags = []\n",
        "            for tags in df['concept_tags'].dropna():\n",
        "                all_tags.extend([t.strip(' \"\\'[]') for t in str(tags).split(',')])\n",
        "            tag_counts = Counter(all_tags)\n",
        "\n",
        "            # Find underused tags\n",
        "            underused = [tag for tag, count in tag_counts.items() if count < 3]\n",
        "            if underused:\n",
        "                instructions.append(f\"  ‚Ä¢ Consider using: {', '.join(underused[:10])}\")\n",
        "\n",
        "        # Quality Guidelines\n",
        "        instructions.append(\"\\nüìù QUALITY GUIDELINES:\")\n",
        "        instructions.append(\"‚Ä¢ Include (TH), (EN), (ZH) versions for all prompts\")\n",
        "        instructions.append(\"‚Ä¢ Provide reasoning with clear logical steps\")\n",
        "        instructions.append(\"‚Ä¢ Include rejected reasoning with fallacy identification\")\n",
        "        instructions.append(\"‚Ä¢ Add explanation comparing correct vs incorrect\")\n",
        "        instructions.append(\"‚Ä¢ Maintain chain_depth of 2-4 for complexity\")\n",
        "\n",
        "        return '\\n'.join(instructions)"
      ],
      "metadata": {
        "id": "z1V3sMUFWCPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 6: Excel Export\n",
        "# ============================================\n",
        "class ExcelExporter:\n",
        "\n",
        "    @staticmethod\n",
        "    def export_to_excel(df, stats_dict, gaps_df, llm_instructions, output_path):\n",
        "        \"\"\"Export everything to Excel with multiple sheets\"\"\"\n",
        "\n",
        "        # Calculate saturation levels\n",
        "        df = MetadataAnalyzer.calculate_saturation(df)\n",
        "\n",
        "        # Create Excel writer\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "\n",
        "            # Sheet 1: All Metadata\n",
        "            df.to_excel(writer, sheet_name='All_Metadata', index=False)\n",
        "\n",
        "            # Sheet 2: Summary Statistics\n",
        "            stats_df = pd.DataFrame([stats_dict]).T.reset_index()\n",
        "            stats_df.columns = ['Metric', 'Value']\n",
        "            stats_df.to_excel(writer, sheet_name='Summary_Stats', index=False)\n",
        "\n",
        "            # Sheet 3: Distribution Analysis\n",
        "            dist_data = []\n",
        "            for col in ['reasoning_type', 'sub_type', 'domain_context', 'difficulty', 'tier']:\n",
        "                if col in df.columns:\n",
        "                    value_counts = df[col].value_counts()\n",
        "                    for val, count in value_counts.items():\n",
        "                        dist_data.append({\n",
        "                            'Category': col,\n",
        "                            'Value': val,\n",
        "                            'Count': count,\n",
        "                            'Percentage': count / len(df) * 100,\n",
        "                            'Status': 'High' if count > 20 else 'OK' if count > 5 else 'Low'\n",
        "                        })\n",
        "            pd.DataFrame(dist_data).to_excel(writer, sheet_name='Distribution', index=False)\n",
        "\n",
        "            # Sheet 4: Combination Matrix\n",
        "            if 'reasoning_type' in df.columns and 'sub_type' in df.columns:\n",
        "                pivot_table = pd.crosstab(df['reasoning_type'], df['sub_type'])\n",
        "                pivot_table.to_excel(writer, sheet_name='Type_x_SubType_Matrix')\n",
        "\n",
        "            # Sheet 5: Domain Coverage\n",
        "            if 'domain_context' in df.columns and 'sub_type' in df.columns:\n",
        "                domain_pivot = pd.crosstab(df['domain_context'], df['sub_type'])\n",
        "                domain_pivot.to_excel(writer, sheet_name='Domain_x_SubType_Matrix')\n",
        "\n",
        "            # Sheet 6: Gaps Analysis\n",
        "            if not gaps_df.empty:\n",
        "                gaps_df.to_excel(writer, sheet_name='Coverage_Gaps', index=False)\n",
        "\n",
        "            # Sheet 7: LLM Instructions\n",
        "            instructions_df = pd.DataFrame({'Instructions': llm_instructions.split('\\n')})\n",
        "            instructions_df.to_excel(writer, sheet_name='LLM_Instructions', index=False)\n",
        "\n",
        "            # Sheet 8: High Saturation List\n",
        "            high_sat = df[df['saturation_level'] == 'HIGH'][\n",
        "                ['prompt_id', 'combination_key', 'saturation_level']\n",
        "            ].drop_duplicates()\n",
        "            high_sat.to_excel(writer, sheet_name='High_Saturation', index=False)\n",
        "\n",
        "            # Sheet 9: Tag Analysis\n",
        "            if 'concept_tags' in df.columns:\n",
        "                all_tags = []\n",
        "                for tags in df['concept_tags'].dropna():\n",
        "                    for tag in str(tags).split(','):\n",
        "                        all_tags.append(tag.strip(' \"\\'[]'))\n",
        "\n",
        "                tag_counts = Counter(all_tags)\n",
        "                tag_df = pd.DataFrame(tag_counts.items(), columns=['Tag', 'Count'])\n",
        "                tag_df = tag_df.sort_values('Count', ascending=False)\n",
        "                tag_df['Status'] = tag_df['Count'].apply(\n",
        "                    lambda x: 'Overused' if x > 10 else 'OK' if x > 3 else 'Underused'\n",
        "                )\n",
        "                tag_df.to_excel(writer, sheet_name='Tag_Analysis', index=False)\n",
        "\n",
        "        print(f\"‚úÖ Excel exported to: {output_path}\")"
      ],
      "metadata": {
        "id": "GPuuC0WNWPSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 7: Main Pipeline\n",
        "# ============================================\n",
        "def main():\n",
        "    \"\"\"Main extraction and analysis pipeline\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìä METADATA EXTRACTOR TO EXCEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Extract metadata\n",
        "    print(\"\\nüìÇ Step 1: Extracting metadata from files...\")\n",
        "    df = MetadataExtractor.load_all_files(\n",
        "        Config.DATASET_DIR,\n",
        "        Config.FILE_PATTERN,\n",
        "        Config.MAX_PROMPTS\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ùå No data extracted!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"‚úÖ Extracted {len(df)} prompts with metadata\")\n",
        "\n",
        "    # Step 2: Generate statistics\n",
        "    print(\"\\nüìà Step 2: Generating statistics...\")\n",
        "    stats_dict = MetadataAnalyzer.generate_statistics(df)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nüìä Summary:\")\n",
        "    for key, value in stats_dict.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  ‚Ä¢ {key}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "    # Step 3: Find gaps\n",
        "    print(\"\\nüîç Step 3: Finding coverage gaps...\")\n",
        "    gaps_df = MetadataAnalyzer.find_gaps(df)\n",
        "    print(f\"  Found {len(gaps_df)} gaps in coverage\")\n",
        "\n",
        "    # Step 4: Generate LLM instructions\n",
        "    print(\"\\nüìù Step 4: Generating LLM instructions...\")\n",
        "    llm_instructions = LLMInstructions.generate_instructions(df, stats_dict)\n",
        "\n",
        "    # Step 5: Export to Excel\n",
        "    print(\"\\nüíæ Step 5: Exporting to Excel...\")\n",
        "    ExcelExporter.export_to_excel(\n",
        "        df, stats_dict, gaps_df, llm_instructions,\n",
        "        Config.OUTPUT_EXCEL\n",
        "    )\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüìÅ Output file: {Config.OUTPUT_EXCEL}\")\n",
        "    print(\"\\nüìä Excel contains:\")\n",
        "    print(\"  1. All_Metadata - Full dataset with 20+ fields\")\n",
        "    print(\"  2. Summary_Stats - Key metrics\")\n",
        "    print(\"  3. Distribution - Count analysis\")\n",
        "    print(\"  4. Type_x_SubType_Matrix - Coverage matrix\")\n",
        "    print(\"  5. Domain_x_SubType_Matrix - Domain coverage\")\n",
        "    print(\"  6. Coverage_Gaps - Missing combinations\")\n",
        "    print(\"  7. LLM_Instructions - Guidelines for new prompts\")\n",
        "    print(\"  8. High_Saturation - Oversaturated combinations\")\n",
        "    print(\"  9. Tag_Analysis - Concept tag usage\")\n",
        "\n",
        "    print(\"\\nüí° Next steps:\")\n",
        "    print(\"  1. Open Excel file\")\n",
        "    print(\"  2. Copy data to your template\")\n",
        "    print(\"  3. Share with LLM for prompt generation\")\n",
        "    print(\"  4. Use pivot tables for deeper analysis\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "tIwFD2OkWT45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 8: Run Pipeline\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    df_metadata = main()\n",
        "\n",
        "    # Optional: Quick preview\n",
        "    if df_metadata is not None and not df_metadata.empty:\n",
        "        print(\"\\nüëÄ Preview first 5 rows:\")\n",
        "        print(df_metadata[['prompt_id', 'reasoning_type', 'sub_type',\n",
        "                          'domain_context', 'difficulty']].head())"
      ],
      "metadata": {
        "id": "GJ_vN1oBWYTb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}